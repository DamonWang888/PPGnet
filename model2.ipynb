{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "try:\n",
    "    import ipdb\n",
    "except:\n",
    "    import pdb as ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self,ninp,fmaps,kwidth,stride):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv2d(ninp,fmaps,kwidth,stride)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "        self.act=nn.ELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=self.pool(x)\n",
    "        x=self.act(x)\n",
    "        return x\n",
    "    \n",
    "class Conv1dBlock(nn.Module):\n",
    "    def __init__(self,ninp,noutp,kernel_size,stride=None,padding=0,Handle=True):\n",
    "        super(Conv1dBlock,self).__init__()\n",
    "        self.conv=nn.Conv1d(ninp,noutp,kernel_size,stride=stride,padding=padding)\n",
    "        self.Handle=Handle\n",
    "        self.norm=nn.BatchNorm1d(noutp)\n",
    "        self.pool=nn.MaxPool1d(5,stride=1,padding=2)\n",
    "        self.act=nn.ReLU()\n",
    "        self.dropout=nn.Dropout()\n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        if self.Handle:\n",
    "            x=self.norm(x)\n",
    "            x=self.pool(x)\n",
    "            x=self.act(x)\n",
    "            x=self.dropout(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    网络结构定义\n",
    "    \"\"\"\n",
    "    '''\n",
    "    def __init__(self,ntr,nc):\n",
    "        super(PPGNet,self).__init__()\n",
    "#       ngf=opt.ngf # \n",
    "        self.ntr=ntr #每一段信号追踪的分片数\n",
    "        self.nc=nc #中间卷积层的个数\n",
    "        self.ConvBlocks=nn.ModuleList()\n",
    "        self.conv1=nn.Conv2d(4,8,1)\n",
    "        self.conv2=nn.Conv2d(8,16,(ntr,3))\n",
    "        self.act2=nn.MaxPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "        for i in np.arange(nc):\n",
    "            ConvBlock=Conv2dBlock(2**(i+4),2**(i+5),(1,3),(1,1))\n",
    "            self.ConvBlocks.append(ConvBlock)\n",
    "        self.conv3=nn.Conv2d(2**(nc+4),32,(1,1),(1,1))\n",
    "        self.act3=nn.ELU()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(64,512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512,1),\n",
    "            nn.ELU()\n",
    "        )\n",
    "    '''\n",
    "    def __init__(self,fmaps,input_dim1,input_dim2,hidden_dim):\n",
    "            super(PPGNet,self).__init__()\n",
    "\n",
    "            # fisrt CNN feature extractor\n",
    "            self.hidden_dim=hidden_dim\n",
    "            self.ParallelBlocks=nn.ModuleList()\n",
    "            for idx,kernel in enumerate(fmaps,start=1):\n",
    "                ConvBlock=Conv1dBlock(1,1,kernel,stride=2,Handle=False)\n",
    "                self.ParallelBlocks.append(ConvBlock)\n",
    "            # lstm feature extractor      \n",
    "            self.lstm1=nn.LSTM(input_dim1,hidden_dim,2) #   \n",
    "            self.lstm2=nn.LSTM(input_dim2,hidden_dim,2)# \n",
    "        \n",
    "           # middle sequential blocks \n",
    "            self.ConvBlocks=nn.ModuleList()\n",
    "            ConvBlock1=Conv1dBlock(1,32,40,stride=1)\n",
    "            self.ConvBlocks.append(ConvBlock1)\n",
    "#             for jdx in np.arange(8):\n",
    "            ConvBlock2=Conv1dBlock(32,47,50,stride=1)\n",
    "            self.ConvBlocks.append(ConvBlock2)\n",
    "            \n",
    "            # linear\n",
    "            self.linear=nn.Sequential(\n",
    "                nn.Linear(384*125,1024),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "#                 nn.Linear(32768,16384),\n",
    "#                 nn.BatchNorm1d(16384),\n",
    "#                 nn.ReLU(),\n",
    "                \n",
    "#                 nn.Linear(32768,8192),\n",
    "#                 nn.BatchNorm1d(8192),\n",
    "#                 nn.ReLU(),\n",
    "                \n",
    "#                 nn.Linear(32768,1024),\n",
    "#                 nn.BatchNorm1d(1024),\n",
    "#                 nn.ReLU(),\n",
    "                \n",
    "                nn.Linear(1024,1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "                \n",
    "#             self.conv2=nn.Conv2d(8,16,(ntr,3))\n",
    "#             self.act2=nn.MaxPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "#             for i in np.arange(nc):\n",
    "#                 ConvBlock=Conv2dBlock(2**(i+4),2**(i+5),(1,3),(1,1))\n",
    "#                 self.ConvBlocks.append(ConvBlock)\n",
    "#             self.conv3=nn.Conv2d(2**(nc+4),32,(1,1),(1,1))\n",
    "#             self.act3=nn.ELU()\n",
    "#             self.fc=nn.Sequential(\n",
    "#                 nn.Linear(64,512),\n",
    "#                 nn.ELU(),\n",
    "#                 nn.Linear(512,1),\n",
    "#                 nn.ELU()\n",
    "#             )\n",
    "\n",
    "    def forward(self,x):\n",
    "#         channel=x.size(1)\n",
    "#         ipdb.set_trace()\n",
    "#         print('1:',x.shape)\n",
    "#         length=len(self.ParallelBlocks)\n",
    "        ParallelOut=[]\n",
    "        channelout=[]#每一个通道平行卷积后处理\n",
    "        serialout=[]\n",
    "        for chidx in np.arange(x.size(1)):\n",
    "            for i in np.arange(len(self.ParallelBlocks)):\n",
    "                Block=self.ParallelBlocks[i]\n",
    "                InitOut=Block(x[:,chidx,:].unsqueeze(1))\n",
    "                ParallelOut.append(InitOut)\n",
    "\n",
    "            channelout=torch.cat(ParallelOut,2)\n",
    "            ParallelOut=[]\n",
    "            # sequential block\n",
    "#             ipdb.set_trace()\n",
    "\n",
    "            for chidy in np.arange(len(self.ConvBlocks)):\n",
    "                SecondBlock=self.ConvBlocks[chidy]\n",
    "                if chidy == 0:\n",
    "                    SecondOut=SecondBlock(channelout)\n",
    "                else:\n",
    "                    SecondOut=SecondBlock(SecondOut)\n",
    "\n",
    "                if chidy+1==len(self.ConvBlocks):\n",
    "                    serialout.append(SecondOut)\n",
    "        # 5 channel concat\n",
    "    #   SecondIn=torch.cat(ParallelOut,1) # n*40*__\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "#         X=t.cat(ParallelOut,1)\n",
    "#         X=self.conv1(X)\n",
    "            \n",
    "#         ipdb.set_trace()\n",
    "#         for j in np.arange(len(self.ConvBlocks)):\n",
    "#             Block=self.ConvBlocks[j]\n",
    "#             X=Block(X)\n",
    "#         ipdb.set_trace()\n",
    "        hidden=(torch.zeros([2, x.size(0), self.hidden_dim],dtype=torch.double),\n",
    "                torch.zeros([2, x.size(0), self.hidden_dim],dtype=torch.double))\n",
    "        LstmOut,self.hidden=self.lstm1(x.view(x.size(1),-1,x.size(2)),hidden)\n",
    "        \n",
    "        concat1=torch.cat(serialout,1)\n",
    "        LstmOut1=LstmOut.view(LstmOut.size(1),-1,LstmOut.size(2))\n",
    "        concat=torch.cat((LstmOut1,concat1),1)\n",
    "        \n",
    "        hidden=(torch.zeros([2, concat.size(0), self.hidden_dim],dtype=torch.double),\n",
    "                torch.zeros([2, concat.size(0), self.hidden_dim],dtype=torch.double))\n",
    "        LstmOutp,self.hidden=self.lstm1(concat.view(concat.size(1),-1,concat.size(2)),hidden)\n",
    "#         ipdb.set_trace()\n",
    "        out=self.linear(LstmOutp.view(LstmOutp.size(1),-1))\n",
    "        return out\n",
    "#         x1=self.conv1(x)\n",
    "#         print('2:',x.shape)\n",
    "#         x=self.conv2(x)\n",
    "#         print('3:',x.shape)\n",
    "#         x=self.act2(x)\n",
    "#         print('4:',x.shape)\n",
    "#         pdb.set_trace()\n",
    "#         for i in np.arange(self.nc):\n",
    "#             ConvBlock=self.ConvBlocks[i]\n",
    "#             x=ConvBlock(x)\n",
    "#             print('{}:'.format(i+5),x.shape)\n",
    "#         pdb.set_trace()\n",
    "#         x=self.conv3(x)\n",
    "#         x=self.act3(x)\n",
    "#         x=x.view(x.size(0),1,-1)\n",
    "#         out=self.fc(x)\n",
    "#         return out.squeeze()\n",
    "    \n",
    "    def get_n_params(self):\n",
    "        pp=0\n",
    "        for p in list(self.parameters()):\n",
    "            nn=1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn*s\n",
    "            pp += nn\n",
    "        return pp \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2dBlock') != -1:\n",
    "        print('Initializing weights of convresblock to 0.0, 0.02')\n",
    "        for k, p in m.named_parameters():\n",
    "            if 'weight' in k and 'conv' in k:\n",
    "                p.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('Conv2d') != -1:\n",
    "        print('Initialzing weight to 0.0, 0.02 for module: ', m)\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            print('bias to 0 for module: ', m)\n",
    "            m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        print('Initializing FC weight to xavier uniform')\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "def build_norm_layer(norm_type, param=None, num_feats=None):\n",
    "    if norm_type == 'bnorm':\n",
    "        return nn.BatchNorm2d(num_feats)\n",
    "    elif norm_type == 'snorm':\n",
    "        spectral_norm(param)\n",
    "        return None\n",
    "    elif norm_type is None:\n",
    "        return None\n",
    "    else:\n",
    "        raise TypeError('Unrecognized norm type: ', norm_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters:  49737034\n"
     ]
    }
   ],
   "source": [
    "kernels=[5,20,40,60,80]\n",
    "model=PPGNet(kernels,125,125,125)\n",
    "print('Total model parameters: ',model.get_n_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import os\n",
    "import scipy.io as sio\n",
    "class PPGData(data.Dataset):\n",
    "    def __init__(self,root):\n",
    "        datas=os.listdir(root)\n",
    "        self.totaldata=[os.path.join(root,data) for data in datas]\n",
    "    def __getitem__(self,index):\n",
    "#       timedata=np.zeros((4,7,1025))\n",
    "#         ipdb.set_trace()\n",
    "        ppgpath=self.totaldata[index]\n",
    "        time=sio.loadmat(ppgpath)\n",
    "        time=time['ppg']\n",
    "        time=time.reshape(8,125)\n",
    "        listslice=ppgpath.split('-')\n",
    "        label=listslice[2][:-4]\n",
    "        return time,label\n",
    "    def __len__(self):\n",
    "        return len(self.totaldata)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda True\n",
      "1622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "filepath='/home/wcj/ReferenceProject/PPGnet/ppghr'\n",
    "dataloader=PPGData(filepath)\n",
    "# pdb.set_trace()\n",
    "device = 'cpu'\n",
    "if t.cuda.is_available:\n",
    "#     device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "    device = 'cuda'\n",
    "CUDA = (device == 'cuda')\n",
    "print(device,CUDA)\n",
    "# pdb.set_trace()\n",
    "dloador=DataLoader(dataloader,batch_size=10,shuffle=True,num_workers=8,pin_memory=CUDA)\n",
    "\n",
    "print(len(dataloader))\n",
    "dataiter=iter(dloador)\n",
    "sample,label=dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   10] loss 114.826447\n",
      "[1,   20] loss 63.512405\n",
      "[1,   30] loss 39.749833\n",
      "[1,   40] loss 41.679797\n",
      "[1,   50] loss 31.475236\n",
      "[1,   60] loss 27.918379\n",
      "[1,   70] loss 26.658218\n",
      "[1,   80] loss 31.830644\n",
      "[1,   90] loss 30.356957\n",
      "[1,  100] loss 24.866935\n",
      "[1,  110] loss 24.057739\n",
      "[1,  120] loss 28.117739\n",
      "[1,  130] loss 23.935385\n",
      "[1,  140] loss 25.033024\n",
      "[1,  150] loss 28.318848\n",
      "[1,  160] loss 22.006725\n",
      "[2,   10] loss 24.693608\n",
      "[2,   20] loss 21.320766\n",
      "[2,   30] loss 19.278396\n",
      "[2,   40] loss 19.730962\n",
      "[2,   50] loss 18.081757\n",
      "[2,   60] loss 19.896156\n",
      "[2,   70] loss 26.025699\n",
      "[2,   80] loss 26.202573\n",
      "[2,   90] loss 20.298536\n",
      "[2,  100] loss 21.959055\n",
      "[2,  110] loss 19.594975\n",
      "[2,  120] loss 17.566269\n",
      "[2,  130] loss 17.684633\n",
      "[2,  140] loss 16.103818\n",
      "[2,  150] loss 20.078954\n",
      "[2,  160] loss 20.343543\n",
      "[3,   10] loss 20.398725\n",
      "[3,   20] loss 16.114635\n",
      "[3,   30] loss 19.313070\n",
      "[3,   40] loss 18.034370\n",
      "[3,   50] loss 18.534032\n",
      "[3,   60] loss 17.725432\n",
      "[3,   70] loss 17.384991\n",
      "[3,   80] loss 17.535531\n",
      "[3,   90] loss 17.860626\n",
      "[3,  100] loss 18.697858\n",
      "[3,  110] loss 18.031396\n",
      "[3,  120] loss 18.406034\n",
      "[3,  130] loss 16.186281\n",
      "[3,  140] loss 18.506644\n",
      "[3,  150] loss 16.733103\n",
      "[3,  160] loss 17.072106\n",
      "[4,   10] loss 17.055496\n",
      "[4,   20] loss 19.442621\n",
      "[4,   30] loss 18.999308\n",
      "[4,   40] loss 17.735560\n",
      "[4,   50] loss 19.256949\n",
      "[4,   60] loss 19.495084\n",
      "[4,   70] loss 19.042092\n",
      "[4,   80] loss 16.195238\n",
      "[4,   90] loss 18.705275\n",
      "[4,  100] loss 17.050740\n",
      "[4,  110] loss 17.835079\n",
      "[4,  120] loss 18.428050\n",
      "[4,  130] loss 16.424203\n",
      "[4,  140] loss 15.693088\n",
      "[4,  150] loss 16.000770\n",
      "[4,  160] loss 14.705125\n",
      "[5,   10] loss 12.779542\n",
      "[5,   20] loss 17.719947\n",
      "[5,   30] loss 18.140343\n",
      "[5,   40] loss 16.631203\n",
      "[5,   50] loss 13.017914\n",
      "[5,   60] loss 15.109674\n",
      "[5,   70] loss 16.299163\n",
      "[5,   80] loss 14.083785\n",
      "[5,   90] loss 15.413127\n",
      "[5,  100] loss 16.917825\n",
      "[5,  110] loss 14.542033\n",
      "[5,  120] loss 16.628967\n",
      "[5,  130] loss 16.033971\n",
      "[5,  140] loss 16.740877\n",
      "[5,  150] loss 17.602860\n",
      "[5,  160] loss 17.272930\n",
      "[6,   10] loss 12.589971\n",
      "[6,   20] loss 15.449583\n",
      "[6,   30] loss 16.321821\n",
      "[6,   40] loss 16.030805\n",
      "[6,   50] loss 16.216641\n",
      "[6,   60] loss 14.187160\n",
      "[6,   70] loss 15.306901\n",
      "[6,   80] loss 14.144053\n",
      "[6,   90] loss 15.900706\n",
      "[6,  100] loss 15.316201\n",
      "[6,  110] loss 15.370437\n",
      "[6,  120] loss 16.324850\n",
      "[6,  130] loss 15.969833\n",
      "[6,  140] loss 16.883044\n",
      "[6,  150] loss 13.090948\n",
      "[6,  160] loss 16.487686\n",
      "[7,   10] loss 19.771462\n",
      "[7,   20] loss 18.841201\n",
      "[7,   30] loss 14.519161\n",
      "[7,   40] loss 13.928040\n",
      "[7,   50] loss 14.735555\n",
      "[7,   60] loss 15.336674\n",
      "[7,   70] loss 14.810165\n",
      "[7,   80] loss 16.473389\n",
      "[7,   90] loss 14.435925\n",
      "[7,  100] loss 15.586174\n",
      "[7,  110] loss 14.276281\n",
      "[7,  120] loss 14.057991\n",
      "[7,  130] loss 13.794399\n",
      "[7,  140] loss 14.045267\n",
      "[7,  150] loss 16.414057\n",
      "[7,  160] loss 16.094587\n",
      "[8,   10] loss 12.424613\n",
      "[8,   20] loss 17.100024\n",
      "[8,   30] loss 13.835577\n",
      "[8,   40] loss 14.442586\n",
      "[8,   50] loss 14.442943\n",
      "[8,   60] loss 14.978309\n",
      "[8,   70] loss 12.927451\n",
      "[8,   80] loss 14.227108\n",
      "[8,   90] loss 15.662456\n",
      "[8,  100] loss 16.018980\n",
      "[8,  110] loss 15.576861\n",
      "[8,  120] loss 11.866313\n",
      "[8,  130] loss 11.878302\n",
      "[8,  140] loss 14.738046\n",
      "[8,  150] loss 14.934075\n",
      "[8,  160] loss 15.616217\n",
      "[9,   10] loss 14.126686\n",
      "[9,   20] loss 14.183319\n",
      "[9,   30] loss 12.387731\n",
      "[9,   40] loss 16.748293\n",
      "[9,   50] loss 12.744266\n",
      "[9,   60] loss 15.167097\n",
      "[9,   70] loss 13.300150\n",
      "[9,   80] loss 14.934722\n",
      "[9,   90] loss 15.204175\n",
      "[9,  100] loss 13.910389\n",
      "[9,  110] loss 14.318176\n",
      "[9,  120] loss 13.694684\n",
      "[9,  130] loss 12.649929\n",
      "[9,  140] loss 14.699714\n",
      "[9,  150] loss 13.455688\n",
      "[9,  160] loss 14.894063\n",
      "[10,   10] loss 14.718859\n",
      "[10,   20] loss 13.149802\n",
      "[10,   30] loss 13.785292\n",
      "[10,   40] loss 13.785737\n",
      "[10,   50] loss 13.692864\n",
      "[10,   60] loss 11.451892\n",
      "[10,   70] loss 15.170309\n",
      "[10,   80] loss 15.020611\n",
      "[10,   90] loss 14.546120\n",
      "[10,  100] loss 14.219480\n",
      "[10,  110] loss 11.741832\n",
      "[10,  120] loss 13.561246\n",
      "[10,  130] loss 15.549154\n",
      "[10,  140] loss 13.345184\n",
      "[10,  150] loss 15.148279\n",
      "[10,  160] loss 14.428795\n",
      "[11,   10] loss 13.328597\n",
      "[11,   20] loss 12.894050\n",
      "[11,   30] loss 14.160815\n",
      "[11,   40] loss 13.537022\n",
      "[11,   50] loss 16.390005\n",
      "[11,   60] loss 14.158789\n",
      "[11,   70] loss 11.240194\n",
      "[11,   80] loss 10.023110\n",
      "[11,   90] loss 10.436407\n",
      "[11,  100] loss 11.775865\n",
      "[11,  110] loss 16.723978\n",
      "[11,  120] loss 12.376026\n",
      "[11,  130] loss 13.416422\n",
      "[11,  140] loss 12.093427\n",
      "[11,  150] loss 15.728145\n",
      "[11,  160] loss 17.144384\n",
      "[12,   10] loss 13.828773\n",
      "[12,   20] loss 15.447905\n",
      "[12,   30] loss 12.923641\n",
      "[12,   40] loss 12.170091\n",
      "[12,   50] loss 13.173335\n",
      "[12,   60] loss 11.716133\n",
      "[12,   70] loss 15.319171\n",
      "[12,   80] loss 13.585276\n",
      "[12,   90] loss 14.211998\n",
      "[12,  100] loss 12.533379\n",
      "[12,  110] loss 13.370475\n",
      "[12,  120] loss 13.345094\n",
      "[12,  130] loss 11.563451\n",
      "[12,  140] loss 12.163329\n",
      "[12,  150] loss 11.975356\n",
      "[12,  160] loss 13.108134\n",
      "[13,   10] loss 12.727239\n",
      "[13,   20] loss 15.227203\n",
      "[13,   30] loss 12.710042\n",
      "[13,   40] loss 13.815426\n",
      "[13,   50] loss 13.584104\n",
      "[13,   60] loss 11.696790\n",
      "[13,   70] loss 11.349285\n",
      "[13,   80] loss 12.148377\n",
      "[13,   90] loss 11.123649\n",
      "[13,  100] loss 11.924495\n",
      "[13,  110] loss 11.836907\n",
      "[13,  120] loss 11.350961\n",
      "[13,  130] loss 12.403293\n",
      "[13,  140] loss 11.822180\n",
      "[13,  150] loss 14.702521\n",
      "[13,  160] loss 10.730185\n",
      "[14,   10] loss 13.316666\n",
      "[14,   20] loss 11.977221\n",
      "[14,   30] loss 11.990819\n",
      "[14,   40] loss 11.695139\n",
      "[14,   50] loss 12.619728\n",
      "[14,   60] loss 11.125950\n",
      "[14,   70] loss 12.304531\n",
      "[14,   80] loss 15.108269\n",
      "[14,   90] loss 13.051394\n",
      "[14,  100] loss 10.477212\n",
      "[14,  110] loss 13.669978\n",
      "[14,  120] loss 12.682757\n",
      "[14,  130] loss 10.195412\n",
      "[14,  140] loss 12.337580\n",
      "[14,  150] loss 11.324604\n",
      "[14,  160] loss 14.563949\n",
      "[15,   10] loss 16.141281\n",
      "[15,   20] loss 13.124671\n",
      "[15,   30] loss 14.223565\n",
      "[15,   40] loss 12.444424\n",
      "[15,   50] loss 13.224124\n",
      "[15,   60] loss 12.120322\n",
      "[15,   70] loss 12.131187\n",
      "[15,   80] loss 15.429373\n",
      "[15,   90] loss 11.943953\n",
      "[15,  100] loss 12.686053\n",
      "[15,  110] loss 10.411885\n",
      "[15,  120] loss 10.342531\n",
      "[15,  130] loss 12.119122\n",
      "[15,  140] loss 11.364642\n",
      "[15,  150] loss 10.947719\n",
      "[15,  160] loss 11.844395\n",
      "[16,   10] loss 10.670481\n",
      "[16,   20] loss 11.539174\n",
      "[16,   30] loss 13.357639\n",
      "[16,   40] loss 13.098002\n",
      "[16,   50] loss 9.745971\n",
      "[16,   60] loss 12.229950\n",
      "[16,   70] loss 11.998162\n",
      "[16,   80] loss 12.711961\n",
      "[16,   90] loss 11.198855\n",
      "[16,  100] loss 10.542930\n",
      "[16,  110] loss 11.005564\n",
      "[16,  120] loss 12.161688\n",
      "[16,  130] loss 12.221227\n",
      "[16,  140] loss 14.262852\n",
      "[16,  150] loss 10.981627\n",
      "[16,  160] loss 10.708703\n",
      "[17,   10] loss 17.255719\n",
      "[17,   20] loss 13.763161\n",
      "[17,   30] loss 15.868443\n",
      "[17,   40] loss 12.435460\n",
      "[17,   50] loss 15.872512\n",
      "[17,   60] loss 12.720764\n",
      "[17,   70] loss 12.660014\n",
      "[17,   80] loss 14.650318\n",
      "[17,   90] loss 12.397736\n",
      "[17,  100] loss 16.579057\n",
      "[17,  110] loss 10.560891\n",
      "[17,  120] loss 12.120087\n",
      "[17,  130] loss 10.098668\n",
      "[17,  140] loss 12.037385\n",
      "[17,  150] loss 11.806917\n",
      "[17,  160] loss 14.336106\n",
      "[18,   10] loss 11.406605\n",
      "[18,   20] loss 11.611849\n",
      "[18,   30] loss 12.597992\n",
      "[18,   40] loss 11.771967\n",
      "[18,   50] loss 12.874444\n",
      "[18,   60] loss 13.539598\n",
      "[18,   70] loss 13.493271\n",
      "[18,   80] loss 11.295330\n",
      "[18,   90] loss 10.534368\n",
      "[18,  100] loss 11.638721\n",
      "[18,  110] loss 10.636566\n",
      "[18,  120] loss 12.379707\n",
      "[18,  130] loss 13.182200\n",
      "[18,  140] loss 11.841927\n",
      "[18,  150] loss 11.456377\n",
      "[18,  160] loss 10.669345\n",
      "[19,   10] loss 11.782975\n",
      "[19,   20] loss 12.883875\n",
      "[19,   30] loss 11.667177\n",
      "[19,   40] loss 12.150243\n",
      "[19,   50] loss 11.837434\n",
      "[19,   60] loss 11.260779\n",
      "[19,   70] loss 12.884667\n",
      "[19,   80] loss 13.436493\n",
      "[19,   90] loss 12.176730\n",
      "[19,  100] loss 10.865453\n",
      "[19,  110] loss 12.033516\n",
      "[19,  120] loss 13.414404\n",
      "[19,  130] loss 11.585458\n",
      "[19,  140] loss 10.225731\n",
      "[19,  150] loss 10.316513\n",
      "[19,  160] loss 11.210921\n",
      "[20,   10] loss 13.450838\n",
      "[20,   20] loss 11.860892\n",
      "[20,   30] loss 10.328612\n",
      "[20,   40] loss 12.813033\n",
      "[20,   50] loss 10.794976\n",
      "[20,   60] loss 10.850357\n",
      "[20,   70] loss 12.706350\n",
      "[20,   80] loss 11.225776\n",
      "[20,   90] loss 11.832627\n",
      "[20,  100] loss 11.747685\n",
      "[20,  110] loss 10.831888\n",
      "[20,  120] loss 11.134036\n",
      "[20,  130] loss 12.685713\n",
      "[20,  140] loss 13.546445\n",
      "[20,  150] loss 9.287009\n",
      "[20,  160] loss 11.297048\n",
      "[21,   10] loss 12.652098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21,   20] loss 11.733287\n",
      "[21,   30] loss 12.170260\n",
      "[21,   40] loss 12.847095\n",
      "[21,   50] loss 11.080617\n",
      "[21,   60] loss 9.764896\n",
      "[21,   70] loss 10.937823\n",
      "[21,   80] loss 9.249292\n",
      "[21,   90] loss 13.049461\n",
      "[21,  100] loss 11.415427\n",
      "[21,  110] loss 13.379997\n",
      "[21,  120] loss 10.604961\n",
      "[21,  130] loss 9.090721\n",
      "[21,  140] loss 8.264062\n",
      "[21,  150] loss 12.846546\n",
      "[21,  160] loss 9.792360\n",
      "[22,   10] loss 12.689251\n",
      "[22,   20] loss 12.162139\n",
      "[22,   30] loss 10.490704\n",
      "[22,   40] loss 10.454704\n",
      "[22,   50] loss 11.918427\n",
      "[22,   60] loss 12.610358\n",
      "[22,   70] loss 12.856504\n",
      "[22,   80] loss 10.877302\n",
      "[22,   90] loss 10.100235\n",
      "[22,  100] loss 12.851627\n",
      "[22,  110] loss 11.239151\n",
      "[22,  120] loss 11.871637\n",
      "[22,  130] loss 13.507391\n",
      "[22,  140] loss 14.503515\n",
      "[22,  150] loss 13.117711\n",
      "[22,  160] loss 12.354125\n",
      "[23,   10] loss 11.102847\n",
      "[23,   20] loss 11.501174\n",
      "[23,   30] loss 11.849333\n",
      "[23,   40] loss 12.656423\n",
      "[23,   50] loss 10.818761\n",
      "[23,   60] loss 10.301899\n",
      "[23,   70] loss 10.272170\n",
      "[23,   80] loss 10.480386\n",
      "[23,   90] loss 11.498884\n",
      "[23,  100] loss 11.098630\n",
      "[23,  110] loss 11.308996\n",
      "[23,  120] loss 9.716395\n",
      "[23,  130] loss 11.758074\n",
      "[23,  140] loss 10.865234\n",
      "[23,  150] loss 10.054136\n",
      "[23,  160] loss 12.179401\n",
      "[24,   10] loss 10.703623\n",
      "[24,   20] loss 11.572665\n",
      "[24,   30] loss 9.557578\n",
      "[24,   40] loss 10.618269\n",
      "[24,   50] loss 10.484700\n",
      "[24,   60] loss 11.628288\n",
      "[24,   70] loss 13.224154\n",
      "[24,   80] loss 12.658224\n",
      "[24,   90] loss 10.494683\n",
      "[24,  100] loss 11.287211\n",
      "[24,  110] loss 14.914060\n",
      "[24,  120] loss 10.802979\n",
      "[24,  130] loss 10.319346\n",
      "[24,  140] loss 11.256062\n",
      "[24,  150] loss 11.111115\n",
      "[24,  160] loss 11.972602\n",
      "[25,   10] loss 13.735495\n",
      "[25,   20] loss 16.245669\n",
      "[25,   30] loss 11.839876\n",
      "[25,   40] loss 14.396897\n",
      "[25,   50] loss 13.627619\n",
      "[25,   60] loss 10.336090\n",
      "[25,   70] loss 13.312032\n",
      "[25,   80] loss 11.367902\n",
      "[25,   90] loss 12.831201\n",
      "[25,  100] loss 13.832385\n",
      "[25,  110] loss 11.284730\n",
      "[25,  120] loss 10.561865\n",
      "[25,  130] loss 12.214719\n",
      "[25,  140] loss 10.274549\n",
      "[25,  150] loss 10.875591\n",
      "[25,  160] loss 10.393332\n",
      "[26,   10] loss 11.375233\n",
      "[26,   20] loss 12.581948\n",
      "[26,   30] loss 9.965322\n",
      "[26,   40] loss 11.799395\n",
      "[26,   50] loss 9.793331\n",
      "[26,   60] loss 11.457954\n",
      "[26,   70] loss 10.594566\n",
      "[26,   80] loss 11.060423\n",
      "[26,   90] loss 11.733504\n",
      "[26,  100] loss 10.342579\n",
      "[26,  110] loss 10.719748\n",
      "[26,  120] loss 11.960182\n",
      "[26,  130] loss 12.206980\n",
      "[26,  140] loss 9.909877\n",
      "[26,  150] loss 13.305273\n",
      "[26,  160] loss 11.180261\n",
      "[27,   10] loss 11.727420\n",
      "[27,   20] loss 10.574412\n",
      "[27,   30] loss 10.382103\n",
      "[27,   40] loss 9.925693\n",
      "[27,   50] loss 10.221221\n",
      "[27,   60] loss 10.564239\n",
      "[27,   70] loss 11.645136\n",
      "[27,   80] loss 10.644421\n",
      "[27,   90] loss 11.171113\n",
      "[27,  100] loss 12.241946\n",
      "[27,  110] loss 11.630326\n",
      "[27,  120] loss 11.492874\n",
      "[27,  130] loss 12.029478\n",
      "[27,  140] loss 12.068488\n",
      "[27,  150] loss 10.415023\n",
      "[27,  160] loss 9.106452\n",
      "[28,   10] loss 11.570152\n",
      "[28,   20] loss 11.011684\n",
      "[28,   30] loss 10.296399\n",
      "[28,   40] loss 11.983949\n",
      "[28,   50] loss 11.974660\n",
      "[28,   60] loss 11.087339\n",
      "[28,   70] loss 12.738317\n",
      "[28,   80] loss 9.679450\n",
      "[28,   90] loss 9.460399\n",
      "[28,  100] loss 11.175328\n",
      "[28,  110] loss 13.740526\n",
      "[28,  120] loss 10.783071\n",
      "[28,  130] loss 9.790586\n",
      "[28,  140] loss 10.415046\n",
      "[28,  150] loss 10.644018\n",
      "[28,  160] loss 7.971742\n",
      "[29,   10] loss 9.816188\n",
      "[29,   20] loss 10.684209\n",
      "[29,   30] loss 10.102810\n",
      "[29,   40] loss 10.256559\n",
      "[29,   50] loss 10.427096\n",
      "[29,   60] loss 9.974581\n",
      "[29,   70] loss 10.887139\n",
      "[29,   80] loss 14.246754\n",
      "[29,   90] loss 13.010043\n",
      "[29,  100] loss 14.093484\n",
      "[29,  110] loss 12.330730\n",
      "[29,  120] loss 9.427651\n",
      "[29,  130] loss 12.081126\n",
      "[29,  140] loss 12.195505\n",
      "[29,  150] loss 8.620817\n",
      "[29,  160] loss 12.150868\n",
      "[30,   10] loss 9.862487\n",
      "[30,   20] loss 11.971337\n",
      "[30,   30] loss 9.428560\n",
      "[30,   40] loss 10.304899\n",
      "[30,   50] loss 9.915637\n",
      "[30,   60] loss 11.102469\n",
      "[30,   70] loss 9.594210\n",
      "[30,   80] loss 8.721104\n",
      "[30,   90] loss 10.025396\n",
      "[30,  100] loss 11.009141\n",
      "[30,  110] loss 11.590947\n",
      "[30,  120] loss 11.532859\n",
      "[30,  130] loss 11.601434\n",
      "[30,  140] loss 9.848197\n",
      "[30,  150] loss 8.472018\n",
      "[30,  160] loss 12.212961\n",
      "[31,   10] loss 8.418436\n",
      "[31,   20] loss 11.329976\n",
      "[31,   30] loss 10.212974\n",
      "[31,   40] loss 10.049015\n",
      "[31,   50] loss 12.471789\n",
      "[31,   60] loss 10.950721\n",
      "[31,   70] loss 8.166058\n",
      "[31,   80] loss 11.735305\n",
      "[31,   90] loss 8.888457\n",
      "[31,  100] loss 9.395273\n",
      "[31,  110] loss 10.041857\n",
      "[31,  120] loss 13.357451\n",
      "[31,  130] loss 9.152517\n",
      "[31,  140] loss 11.726265\n",
      "[31,  150] loss 9.515705\n",
      "[31,  160] loss 10.812645\n",
      "[32,   10] loss 9.025237\n",
      "[32,   20] loss 10.794141\n",
      "[32,   30] loss 10.273329\n",
      "[32,   40] loss 10.090354\n",
      "[32,   50] loss 10.887898\n",
      "[32,   60] loss 9.140138\n",
      "[32,   70] loss 10.751045\n",
      "[32,   80] loss 9.269179\n",
      "[32,   90] loss 9.334701\n",
      "[32,  100] loss 10.272231\n",
      "[32,  110] loss 10.197494\n",
      "[32,  120] loss 10.117200\n",
      "[32,  130] loss 9.805136\n",
      "[32,  140] loss 9.503502\n",
      "[32,  150] loss 9.650675\n",
      "[32,  160] loss 11.827047\n",
      "[33,   10] loss 10.975506\n",
      "[33,   20] loss 9.981257\n",
      "[33,   30] loss 9.734859\n",
      "[33,   40] loss 10.337205\n",
      "[33,   50] loss 8.476047\n",
      "[33,   60] loss 8.485265\n",
      "[33,   70] loss 10.157317\n",
      "[33,   80] loss 8.409579\n",
      "[33,   90] loss 8.561577\n",
      "[33,  100] loss 9.612324\n",
      "[33,  110] loss 9.830673\n",
      "[33,  120] loss 8.967790\n",
      "[33,  130] loss 11.882565\n",
      "[33,  140] loss 10.449403\n",
      "[33,  150] loss 13.042855\n",
      "[33,  160] loss 10.406534\n",
      "[34,   10] loss 8.660426\n",
      "[34,   20] loss 9.816738\n",
      "[34,   30] loss 10.455484\n",
      "[34,   40] loss 11.729857\n",
      "[34,   50] loss 11.615403\n",
      "[34,   60] loss 10.159394\n",
      "[34,   70] loss 11.386223\n",
      "[34,   80] loss 10.135097\n",
      "[34,   90] loss 9.053196\n",
      "[34,  100] loss 11.659133\n",
      "[34,  110] loss 8.362306\n",
      "[34,  120] loss 9.157009\n",
      "[34,  130] loss 9.818875\n",
      "[34,  140] loss 9.456808\n",
      "[34,  150] loss 11.153567\n",
      "[34,  160] loss 12.015422\n",
      "[35,   10] loss 10.399975\n",
      "[35,   20] loss 9.914565\n",
      "[35,   30] loss 11.162441\n",
      "[35,   40] loss 11.294599\n",
      "[35,   50] loss 11.719771\n",
      "[35,   60] loss 12.446793\n",
      "[35,   70] loss 10.378784\n",
      "[35,   80] loss 9.394414\n",
      "[35,   90] loss 7.942024\n",
      "[35,  100] loss 8.354980\n",
      "[35,  110] loss 8.644215\n",
      "[35,  120] loss 11.586561\n",
      "[35,  130] loss 10.535031\n",
      "[35,  140] loss 11.851735\n",
      "[35,  150] loss 8.850732\n",
      "[35,  160] loss 9.181065\n",
      "[36,   10] loss 11.183009\n",
      "[36,   20] loss 11.085557\n",
      "[36,   30] loss 10.537169\n",
      "[36,   40] loss 10.173586\n",
      "[36,   50] loss 9.874804\n",
      "[36,   60] loss 7.075416\n",
      "[36,   70] loss 9.532332\n",
      "[36,   80] loss 9.432577\n",
      "[36,   90] loss 10.891350\n",
      "[36,  100] loss 9.951261\n",
      "[36,  110] loss 10.425571\n",
      "[36,  120] loss 9.806668\n",
      "[36,  130] loss 8.557173\n",
      "[36,  140] loss 10.466954\n",
      "[36,  150] loss 9.556536\n",
      "[36,  160] loss 11.293436\n",
      "[37,   10] loss 9.089985\n",
      "[37,   20] loss 9.944884\n",
      "[37,   30] loss 9.728778\n",
      "[37,   40] loss 10.917125\n",
      "[37,   50] loss 9.210204\n",
      "[37,   60] loss 11.184334\n",
      "[37,   70] loss 8.672903\n",
      "[37,   80] loss 8.751667\n",
      "[37,   90] loss 8.766293\n",
      "[37,  100] loss 9.868090\n",
      "[37,  110] loss 9.509769\n",
      "[37,  120] loss 9.648065\n",
      "[37,  130] loss 9.652308\n",
      "[37,  140] loss 10.973005\n",
      "[37,  150] loss 9.074624\n",
      "[37,  160] loss 7.925893\n",
      "[38,   10] loss 9.457789\n",
      "[38,   20] loss 10.261272\n",
      "[38,   30] loss 9.672659\n",
      "[38,   40] loss 10.584228\n",
      "[38,   50] loss 8.711669\n",
      "[38,   60] loss 9.169895\n",
      "[38,   70] loss 9.911222\n",
      "[38,   80] loss 10.265989\n",
      "[38,   90] loss 10.400255\n",
      "[38,  100] loss 8.912685\n",
      "[38,  110] loss 9.942365\n",
      "[38,  120] loss 8.826784\n",
      "[38,  130] loss 9.585502\n",
      "[38,  140] loss 8.689260\n",
      "[38,  150] loss 9.130868\n",
      "[38,  160] loss 8.819768\n",
      "[39,   10] loss 10.575947\n",
      "[39,   20] loss 8.513235\n",
      "[39,   30] loss 8.279239\n",
      "[39,   40] loss 9.284547\n",
      "[39,   50] loss 10.785649\n",
      "[39,   60] loss 9.264730\n",
      "[39,   70] loss 14.088054\n",
      "[39,   80] loss 10.418557\n",
      "[39,   90] loss 12.131835\n",
      "[39,  100] loss 11.797153\n",
      "[39,  110] loss 12.199252\n",
      "[39,  120] loss 8.680013\n",
      "[39,  130] loss 10.960030\n",
      "[39,  140] loss 11.803145\n",
      "[39,  150] loss 12.269851\n",
      "[39,  160] loss 10.558325\n",
      "[40,   10] loss 12.063775\n",
      "[40,   20] loss 8.896017\n",
      "[40,   30] loss 8.783180\n",
      "[40,   40] loss 11.460136\n",
      "[40,   50] loss 11.344529\n",
      "[40,   60] loss 9.011073\n",
      "[40,   70] loss 8.037317\n",
      "[40,   80] loss 9.494606\n",
      "[40,   90] loss 9.067686\n",
      "[40,  100] loss 10.744586\n",
      "[40,  110] loss 10.437908\n",
      "[40,  120] loss 10.383061\n",
      "[40,  130] loss 8.063568\n",
      "[40,  140] loss 10.405033\n",
      "[40,  150] loss 10.007436\n",
      "[40,  160] loss 10.421188\n",
      "[41,   10] loss 11.321901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41,   20] loss 11.827612\n",
      "[41,   30] loss 9.322524\n",
      "[41,   40] loss 7.999043\n",
      "[41,   50] loss 8.915833\n",
      "[41,   60] loss 10.429987\n",
      "[41,   70] loss 8.993097\n",
      "[41,   80] loss 10.504537\n",
      "[41,   90] loss 9.364591\n",
      "[41,  100] loss 7.901154\n",
      "[41,  110] loss 8.332145\n",
      "[41,  120] loss 9.548835\n",
      "[41,  130] loss 8.182590\n",
      "[41,  140] loss 7.695733\n",
      "[41,  150] loss 8.324579\n",
      "[41,  160] loss 11.176021\n",
      "[42,   10] loss 8.624420\n",
      "[42,   20] loss 10.301882\n",
      "[42,   30] loss 9.667352\n",
      "[42,   40] loss 9.285037\n",
      "[42,   50] loss 10.554078\n",
      "[42,   60] loss 8.168023\n",
      "[42,   70] loss 7.626632\n",
      "[42,   80] loss 7.395015\n",
      "[42,   90] loss 10.115675\n",
      "[42,  100] loss 8.852825\n",
      "[42,  110] loss 9.412097\n",
      "[42,  120] loss 10.790533\n",
      "[42,  130] loss 8.915978\n",
      "[42,  140] loss 10.413921\n",
      "[42,  150] loss 10.533427\n",
      "[42,  160] loss 10.216706\n",
      "[43,   10] loss 9.612207\n",
      "[43,   20] loss 8.025902\n",
      "[43,   30] loss 9.094998\n",
      "[43,   40] loss 10.104631\n",
      "[43,   50] loss 8.422684\n",
      "[43,   60] loss 8.793226\n",
      "[43,   70] loss 8.021216\n",
      "[43,   80] loss 8.732087\n",
      "[43,   90] loss 9.026922\n",
      "[43,  100] loss 8.898864\n",
      "[43,  110] loss 11.708068\n",
      "[43,  120] loss 7.321654\n",
      "[43,  130] loss 10.522492\n",
      "[43,  140] loss 8.876930\n",
      "[43,  150] loss 10.512031\n",
      "[43,  160] loss 9.225042\n",
      "[44,   10] loss 7.845848\n",
      "[44,   20] loss 9.866518\n",
      "[44,   30] loss 9.954261\n",
      "[44,   40] loss 6.969289\n",
      "[44,   50] loss 7.667144\n",
      "[44,   60] loss 9.914255\n",
      "[44,   70] loss 9.410271\n",
      "[44,   80] loss 10.972691\n",
      "[44,   90] loss 9.450594\n",
      "[44,  100] loss 8.787918\n",
      "[44,  110] loss 8.955139\n",
      "[44,  120] loss 10.355845\n",
      "[44,  130] loss 9.681362\n",
      "[44,  140] loss 10.205660\n",
      "[44,  150] loss 8.834869\n",
      "[44,  160] loss 10.559261\n",
      "[45,   10] loss 12.040575\n",
      "[45,   20] loss 8.376188\n",
      "[45,   30] loss 7.449401\n",
      "[45,   40] loss 9.141752\n",
      "[45,   50] loss 9.707545\n",
      "[45,   60] loss 9.183144\n",
      "[45,   70] loss 9.294962\n",
      "[45,   80] loss 8.492488\n",
      "[45,   90] loss 9.713108\n",
      "[45,  100] loss 8.141580\n",
      "[45,  110] loss 10.640567\n",
      "[45,  120] loss 9.105302\n",
      "[45,  130] loss 7.623846\n",
      "[45,  140] loss 10.723847\n",
      "[45,  150] loss 7.904505\n",
      "[45,  160] loss 10.432281\n",
      "[46,   10] loss 11.139997\n",
      "[46,   20] loss 9.695597\n",
      "[46,   30] loss 9.131244\n",
      "[46,   40] loss 12.790579\n",
      "[46,   50] loss 10.980490\n",
      "[46,   60] loss 8.816668\n",
      "[46,   70] loss 8.248030\n",
      "[46,   80] loss 8.609336\n",
      "[46,   90] loss 8.630542\n",
      "[46,  100] loss 9.456372\n",
      "[46,  110] loss 10.887355\n",
      "[46,  120] loss 8.560442\n",
      "[46,  130] loss 9.819848\n",
      "[46,  140] loss 10.126756\n",
      "[46,  150] loss 10.188696\n",
      "[46,  160] loss 9.015305\n",
      "[47,   10] loss 9.414876\n",
      "[47,   20] loss 9.090215\n",
      "[47,   30] loss 9.555916\n",
      "[47,   40] loss 9.664877\n",
      "[47,   50] loss 8.971706\n",
      "[47,   60] loss 9.574973\n",
      "[47,   70] loss 7.484707\n",
      "[47,   80] loss 9.157897\n",
      "[47,   90] loss 8.130965\n",
      "[47,  100] loss 9.250201\n",
      "[47,  110] loss 11.157556\n",
      "[47,  120] loss 9.808044\n",
      "[47,  130] loss 9.494232\n",
      "[47,  140] loss 8.945756\n",
      "[47,  150] loss 9.225111\n",
      "[47,  160] loss 9.226721\n",
      "[48,   10] loss 6.534774\n",
      "[48,   20] loss 9.181439\n",
      "[48,   30] loss 11.426252\n",
      "[48,   40] loss 8.473090\n",
      "[48,   50] loss 8.383716\n",
      "[48,   60] loss 12.018642\n",
      "[48,   70] loss 9.909769\n",
      "[48,   80] loss 8.295364\n",
      "[48,   90] loss 6.657623\n",
      "[48,  100] loss 9.781146\n",
      "[48,  110] loss 8.736589\n",
      "[48,  120] loss 9.306019\n",
      "[48,  130] loss 10.119384\n",
      "[48,  140] loss 9.113923\n",
      "[48,  150] loss 7.433911\n",
      "[48,  160] loss 9.027086\n",
      "[49,   10] loss 8.262386\n",
      "[49,   20] loss 8.295622\n",
      "[49,   30] loss 7.753203\n",
      "[49,   40] loss 8.082330\n",
      "[49,   50] loss 9.569159\n",
      "[49,   60] loss 9.012932\n",
      "[49,   70] loss 8.748997\n",
      "[49,   80] loss 9.439548\n",
      "[49,   90] loss 8.657053\n",
      "[49,  100] loss 10.284455\n",
      "[49,  110] loss 8.480153\n",
      "[49,  120] loss 8.441843\n",
      "[49,  130] loss 10.379482\n",
      "[49,  140] loss 8.286406\n",
      "[49,  150] loss 9.233542\n",
      "[49,  160] loss 10.458436\n",
      "[50,   10] loss 9.628156\n",
      "[50,   20] loss 8.040591\n",
      "[50,   30] loss 8.569378\n",
      "[50,   40] loss 7.912635\n",
      "[50,   50] loss 6.520432\n",
      "[50,   60] loss 8.950123\n",
      "[50,   70] loss 7.666361\n",
      "[50,   80] loss 9.588000\n",
      "[50,   90] loss 7.781783\n",
      "[50,  100] loss 7.461069\n",
      "[50,  110] loss 8.757017\n",
      "[50,  120] loss 8.863149\n",
      "[50,  130] loss 6.981465\n",
      "[50,  140] loss 8.141410\n",
      "[50,  150] loss 7.859144\n",
      "[50,  160] loss 9.677722\n",
      "[51,   10] loss 9.377267\n",
      "[51,   20] loss 9.004578\n",
      "[51,   30] loss 9.783437\n",
      "[51,   40] loss 8.189617\n",
      "[51,   50] loss 7.019156\n",
      "[51,   60] loss 7.853302\n",
      "[51,   70] loss 8.874691\n",
      "[51,   80] loss 7.277070\n",
      "[51,   90] loss 8.195677\n",
      "[51,  100] loss 9.178244\n",
      "[51,  110] loss 9.000890\n",
      "[51,  120] loss 8.980632\n",
      "[51,  130] loss 9.476078\n",
      "[51,  140] loss 9.683426\n",
      "[51,  150] loss 6.258344\n",
      "[51,  160] loss 8.696174\n",
      "[52,   10] loss 10.488193\n",
      "[52,   20] loss 10.485334\n",
      "[52,   30] loss 10.039461\n",
      "[52,   40] loss 10.094361\n",
      "[52,   50] loss 8.424210\n",
      "[52,   60] loss 11.640889\n",
      "[52,   70] loss 10.103078\n",
      "[52,   80] loss 9.791595\n",
      "[52,   90] loss 8.765140\n",
      "[52,  100] loss 6.949589\n",
      "[52,  110] loss 9.153236\n",
      "[52,  120] loss 9.048730\n",
      "[52,  130] loss 9.918907\n",
      "[52,  140] loss 8.686232\n",
      "[52,  150] loss 9.318456\n",
      "[52,  160] loss 11.804225\n",
      "[53,   10] loss 9.427354\n",
      "[53,   20] loss 8.303737\n",
      "[53,   30] loss 8.272650\n",
      "[53,   40] loss 8.544893\n",
      "[53,   50] loss 9.241176\n",
      "[53,   60] loss 9.178290\n",
      "[53,   70] loss 8.687543\n",
      "[53,   80] loss 8.098641\n",
      "[53,   90] loss 9.360778\n",
      "[53,  100] loss 7.969548\n",
      "[53,  110] loss 9.484846\n",
      "[53,  120] loss 9.293522\n",
      "[53,  130] loss 9.694298\n",
      "[53,  140] loss 8.793876\n",
      "[53,  150] loss 9.201058\n",
      "[53,  160] loss 9.852138\n",
      "[54,   10] loss 9.807616\n",
      "[54,   20] loss 10.625478\n",
      "[54,   30] loss 11.925639\n",
      "[54,   40] loss 8.656870\n",
      "[54,   50] loss 8.318034\n",
      "[54,   60] loss 7.714790\n",
      "[54,   70] loss 9.119484\n",
      "[54,   80] loss 9.169396\n",
      "[54,   90] loss 8.769571\n",
      "[54,  100] loss 7.225399\n",
      "[54,  110] loss 8.314649\n",
      "[54,  120] loss 9.174000\n",
      "[54,  130] loss 9.842953\n",
      "[54,  140] loss 8.546799\n",
      "[54,  150] loss 8.276735\n",
      "[54,  160] loss 8.512884\n",
      "[55,   10] loss 8.435333\n",
      "[55,   20] loss 8.611725\n",
      "[55,   30] loss 7.970017\n",
      "[55,   40] loss 10.645653\n",
      "[55,   50] loss 7.736434\n",
      "[55,   60] loss 7.866170\n",
      "[55,   70] loss 8.564982\n",
      "[55,   80] loss 10.329578\n",
      "[55,   90] loss 9.263096\n",
      "[55,  100] loss 7.187387\n",
      "[55,  110] loss 9.144620\n",
      "[55,  120] loss 7.764262\n",
      "[55,  130] loss 8.460652\n",
      "[55,  140] loss 8.855700\n",
      "[55,  150] loss 9.690648\n",
      "[55,  160] loss 8.027114\n",
      "[56,   10] loss 7.918654\n",
      "[56,   20] loss 8.235606\n",
      "[56,   30] loss 6.829335\n",
      "[56,   40] loss 6.600259\n",
      "[56,   50] loss 6.437746\n",
      "[56,   60] loss 9.487526\n",
      "[56,   70] loss 9.322686\n",
      "[56,   80] loss 9.306688\n",
      "[56,   90] loss 9.537956\n",
      "[56,  100] loss 7.176338\n",
      "[56,  110] loss 7.954359\n",
      "[56,  120] loss 9.375527\n",
      "[56,  130] loss 8.087792\n",
      "[56,  140] loss 6.917811\n",
      "[56,  150] loss 7.728202\n",
      "[56,  160] loss 8.473410\n",
      "[57,   10] loss 9.381211\n",
      "[57,   20] loss 9.054522\n",
      "[57,   30] loss 9.863205\n",
      "[57,   40] loss 9.201599\n",
      "[57,   50] loss 8.800403\n",
      "[57,   60] loss 7.548188\n",
      "[57,   70] loss 7.809853\n",
      "[57,   80] loss 11.987243\n",
      "[57,   90] loss 8.683422\n",
      "[57,  100] loss 8.864943\n",
      "[57,  110] loss 7.383000\n",
      "[57,  120] loss 7.967899\n",
      "[57,  130] loss 7.188204\n",
      "[57,  140] loss 8.148128\n",
      "[57,  150] loss 8.753782\n",
      "[57,  160] loss 8.861195\n",
      "[58,   10] loss 8.478627\n",
      "[58,   20] loss 8.071335\n",
      "[58,   30] loss 6.729312\n",
      "[58,   40] loss 8.326634\n",
      "[58,   50] loss 9.519250\n",
      "[58,   60] loss 9.316421\n",
      "[58,   70] loss 7.197019\n",
      "[58,   80] loss 8.293383\n",
      "[58,   90] loss 8.209948\n",
      "[58,  100] loss 8.951231\n",
      "[58,  110] loss 9.163031\n",
      "[58,  120] loss 15.627124\n",
      "[58,  130] loss 8.622707\n",
      "[58,  140] loss 8.904832\n",
      "[58,  150] loss 9.316718\n",
      "[58,  160] loss 8.028230\n",
      "[59,   10] loss 9.228639\n",
      "[59,   20] loss 7.050892\n",
      "[59,   30] loss 7.061025\n",
      "[59,   40] loss 8.209085\n",
      "[59,   50] loss 8.875519\n",
      "[59,   60] loss 8.866291\n",
      "[59,   70] loss 8.494694\n",
      "[59,   80] loss 9.564089\n",
      "[59,   90] loss 8.354878\n",
      "[59,  100] loss 7.976026\n",
      "[59,  110] loss 6.937910\n",
      "[59,  120] loss 9.298987\n",
      "[59,  130] loss 7.116218\n",
      "[59,  140] loss 8.771621\n",
      "[59,  150] loss 9.177801\n",
      "[59,  160] loss 8.583005\n",
      "[60,   10] loss 8.938804\n",
      "[60,   20] loss 9.075952\n",
      "[60,   30] loss 7.435799\n",
      "[60,   40] loss 8.717340\n",
      "[60,   50] loss 9.577442\n",
      "[60,   60] loss 8.972931\n",
      "[60,   70] loss 7.441316\n",
      "[60,   80] loss 9.291591\n",
      "[60,   90] loss 9.310049\n",
      "[60,  100] loss 7.388369\n",
      "[60,  110] loss 8.022717\n",
      "[60,  120] loss 6.286050\n",
      "[60,  130] loss 8.260588\n",
      "[60,  140] loss 8.441796\n",
      "[60,  150] loss 7.744746\n",
      "[60,  160] loss 7.130154\n",
      "[61,   10] loss 7.404204\n",
      "[61,   20] loss 10.296726\n",
      "[61,   30] loss 8.674008\n",
      "[61,   40] loss 8.762722\n",
      "[61,   50] loss 7.743199\n",
      "[61,   60] loss 9.001107\n",
      "[61,   70] loss 10.054789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61,   80] loss 5.983373\n",
      "[61,   90] loss 8.918439\n",
      "[61,  100] loss 7.920262\n",
      "[61,  110] loss 8.872116\n",
      "[61,  120] loss 7.592964\n",
      "[61,  130] loss 9.683468\n",
      "[61,  140] loss 8.218237\n",
      "[61,  150] loss 7.283930\n",
      "[61,  160] loss 8.221235\n",
      "[62,   10] loss 7.571388\n",
      "[62,   20] loss 7.731349\n",
      "[62,   30] loss 9.293556\n",
      "[62,   40] loss 8.405648\n",
      "[62,   50] loss 9.204209\n",
      "[62,   60] loss 7.628799\n",
      "[62,   70] loss 6.733740\n",
      "[62,   80] loss 8.011008\n",
      "[62,   90] loss 8.077671\n",
      "[62,  100] loss 7.277001\n",
      "[62,  110] loss 7.247722\n",
      "[62,  120] loss 7.785574\n",
      "[62,  130] loss 7.486943\n",
      "[62,  140] loss 7.786588\n",
      "[62,  150] loss 8.243304\n",
      "[62,  160] loss 9.291474\n",
      "[63,   10] loss 9.383692\n",
      "[63,   20] loss 9.224545\n",
      "[63,   30] loss 8.563586\n",
      "[63,   40] loss 7.547905\n",
      "[63,   50] loss 9.155745\n",
      "[63,   60] loss 8.912480\n",
      "[63,   70] loss 6.159879\n",
      "[63,   80] loss 9.056838\n",
      "[63,   90] loss 7.471764\n",
      "[63,  100] loss 8.249536\n",
      "[63,  110] loss 10.032697\n",
      "[63,  120] loss 8.941082\n",
      "[63,  130] loss 7.496727\n",
      "[63,  140] loss 7.194786\n",
      "[63,  150] loss 6.992298\n",
      "[63,  160] loss 8.126723\n",
      "[64,   10] loss 9.390540\n",
      "[64,   20] loss 8.657846\n",
      "[64,   30] loss 6.050846\n",
      "[64,   40] loss 8.212762\n",
      "[64,   50] loss 8.216762\n",
      "[64,   60] loss 7.770390\n",
      "[64,   70] loss 8.292150\n",
      "[64,   80] loss 8.700071\n",
      "[64,   90] loss 7.996321\n",
      "[64,  100] loss 6.394830\n",
      "[64,  110] loss 9.747423\n",
      "[64,  120] loss 8.765665\n",
      "[64,  130] loss 8.089141\n",
      "[64,  140] loss 7.617612\n",
      "[64,  150] loss 10.030253\n",
      "[64,  160] loss 7.461858\n",
      "[65,   10] loss 7.657085\n",
      "[65,   20] loss 6.456197\n",
      "[65,   30] loss 7.944982\n",
      "[65,   40] loss 7.740472\n",
      "[65,   50] loss 10.792649\n",
      "[65,   60] loss 8.253286\n",
      "[65,   70] loss 8.029455\n",
      "[65,   80] loss 7.750573\n",
      "[65,   90] loss 7.068629\n",
      "[65,  100] loss 6.631687\n",
      "[65,  110] loss 8.149410\n",
      "[65,  120] loss 8.072657\n",
      "[65,  130] loss 8.300811\n",
      "[65,  140] loss 7.085090\n",
      "[65,  150] loss 7.527947\n",
      "[65,  160] loss 7.358198\n",
      "[66,   10] loss 8.466018\n",
      "[66,   20] loss 7.012642\n",
      "[66,   30] loss 9.642353\n",
      "[66,   40] loss 7.308515\n",
      "[66,   50] loss 8.173435\n",
      "[66,   60] loss 7.450577\n",
      "[66,   70] loss 7.895671\n",
      "[66,   80] loss 7.528045\n",
      "[66,   90] loss 7.909370\n",
      "[66,  100] loss 8.620821\n",
      "[66,  110] loss 8.090886\n",
      "[66,  120] loss 7.625936\n",
      "[66,  130] loss 6.933072\n",
      "[66,  140] loss 8.521373\n",
      "[66,  150] loss 8.402777\n",
      "[66,  160] loss 7.841424\n",
      "[67,   10] loss 7.541484\n",
      "[67,   20] loss 7.765938\n",
      "[67,   30] loss 7.758275\n",
      "[67,   40] loss 6.462565\n",
      "[67,   50] loss 6.647928\n",
      "[67,   60] loss 7.842452\n",
      "[67,   70] loss 6.693768\n",
      "[67,   80] loss 7.803953\n",
      "[67,   90] loss 7.553200\n",
      "[67,  100] loss 6.252316\n",
      "[67,  110] loss 7.923432\n",
      "[67,  120] loss 6.127055\n",
      "[67,  130] loss 6.644608\n",
      "[67,  140] loss 8.105857\n",
      "[67,  150] loss 7.741418\n",
      "[67,  160] loss 8.486240\n",
      "[68,   10] loss 7.745386\n",
      "[68,   20] loss 7.442787\n",
      "[68,   30] loss 6.873254\n",
      "[68,   40] loss 9.408014\n",
      "[68,   50] loss 9.433510\n",
      "[68,   60] loss 8.848490\n",
      "[68,   70] loss 8.679324\n",
      "[68,   80] loss 8.177598\n",
      "[68,   90] loss 7.621375\n",
      "[68,  100] loss 6.442443\n",
      "[68,  110] loss 8.702390\n",
      "[68,  120] loss 8.474672\n",
      "[68,  130] loss 8.108455\n",
      "[68,  140] loss 10.044282\n",
      "[68,  150] loss 9.926376\n",
      "[68,  160] loss 9.264847\n",
      "[69,   10] loss 9.390760\n",
      "[69,   20] loss 9.507647\n",
      "[69,   30] loss 7.444506\n",
      "[69,   40] loss 6.982543\n",
      "[69,   50] loss 7.590237\n",
      "[69,   60] loss 8.460921\n",
      "[69,   70] loss 7.081231\n",
      "[69,   80] loss 7.887698\n",
      "[69,   90] loss 7.911055\n",
      "[69,  100] loss 8.096916\n",
      "[69,  110] loss 7.339187\n",
      "[69,  120] loss 8.210104\n",
      "[69,  130] loss 8.610692\n",
      "[69,  140] loss 7.133132\n",
      "[69,  150] loss 8.676685\n",
      "[69,  160] loss 7.029369\n",
      "[70,   10] loss 8.163839\n",
      "[70,   20] loss 8.147943\n",
      "[70,   30] loss 8.059123\n",
      "[70,   40] loss 9.226988\n",
      "[70,   50] loss 8.290316\n",
      "[70,   60] loss 7.530186\n",
      "[70,   70] loss 10.545861\n",
      "[70,   80] loss 7.916394\n",
      "[70,   90] loss 8.045741\n",
      "[70,  100] loss 8.846376\n",
      "[70,  110] loss 7.285719\n",
      "[70,  120] loss 7.489681\n",
      "[70,  130] loss 7.577298\n",
      "[70,  140] loss 8.075604\n",
      "[70,  150] loss 7.756075\n",
      "[70,  160] loss 8.968472\n",
      "[71,   10] loss 9.275903\n",
      "[71,   20] loss 9.161567\n",
      "[71,   30] loss 7.654058\n",
      "[71,   40] loss 7.968819\n",
      "[71,   50] loss 9.536535\n",
      "[71,   60] loss 7.452910\n",
      "[71,   70] loss 9.018945\n",
      "[71,   80] loss 7.541729\n",
      "[71,   90] loss 8.056582\n",
      "[71,  100] loss 7.439018\n",
      "[71,  110] loss 7.527550\n",
      "[71,  120] loss 6.940259\n",
      "[71,  130] loss 7.725770\n",
      "[71,  140] loss 7.744442\n",
      "[71,  150] loss 7.639714\n",
      "[71,  160] loss 7.766438\n",
      "[72,   10] loss 8.720119\n",
      "[72,   20] loss 9.724394\n",
      "[72,   30] loss 9.187902\n",
      "[72,   40] loss 8.223294\n",
      "[72,   50] loss 8.598520\n",
      "[72,   60] loss 9.165106\n",
      "[72,   70] loss 8.243587\n",
      "[72,   80] loss 7.938282\n",
      "[72,   90] loss 9.171076\n",
      "[72,  100] loss 8.167240\n",
      "[72,  110] loss 8.246325\n",
      "[72,  120] loss 8.552489\n",
      "[72,  130] loss 7.692735\n",
      "[72,  140] loss 6.266100\n",
      "[72,  150] loss 9.286193\n",
      "[72,  160] loss 9.202675\n",
      "[73,   10] loss 7.233838\n",
      "[73,   20] loss 6.583224\n",
      "[73,   30] loss 8.069515\n",
      "[73,   40] loss 8.897619\n",
      "[73,   50] loss 6.805141\n",
      "[73,   60] loss 9.104333\n",
      "[73,   70] loss 8.330407\n",
      "[73,   80] loss 9.404398\n",
      "[73,   90] loss 11.247248\n",
      "[73,  100] loss 7.948023\n",
      "[73,  110] loss 6.823322\n",
      "[73,  120] loss 8.391168\n",
      "[73,  130] loss 7.301460\n",
      "[73,  140] loss 9.629386\n",
      "[73,  150] loss 7.123692\n",
      "[73,  160] loss 9.481244\n",
      "[74,   10] loss 7.575003\n",
      "[74,   20] loss 8.235658\n",
      "[74,   30] loss 7.734170\n",
      "[74,   40] loss 9.006080\n",
      "[74,   50] loss 8.067158\n",
      "[74,   60] loss 7.916296\n",
      "[74,   70] loss 6.914711\n",
      "[74,   80] loss 6.961866\n",
      "[74,   90] loss 7.370551\n",
      "[74,  100] loss 7.459813\n",
      "[74,  110] loss 8.255090\n",
      "[74,  120] loss 7.113964\n",
      "[74,  130] loss 7.887901\n",
      "[74,  140] loss 7.914396\n",
      "[74,  150] loss 10.005356\n",
      "[74,  160] loss 8.628436\n",
      "[75,   10] loss 7.183332\n",
      "[75,   20] loss 8.700717\n",
      "[75,   30] loss 7.348464\n",
      "[75,   40] loss 7.814923\n",
      "[75,   50] loss 8.741574\n",
      "[75,   60] loss 7.802501\n",
      "[75,   70] loss 7.714919\n",
      "[75,   80] loss 7.893002\n",
      "[75,   90] loss 7.174985\n",
      "[75,  100] loss 7.519228\n",
      "[75,  110] loss 6.652357\n",
      "[75,  120] loss 6.926864\n",
      "[75,  130] loss 7.302986\n",
      "[75,  140] loss 8.816094\n",
      "[75,  150] loss 7.670940\n",
      "[75,  160] loss 8.538791\n",
      "[76,   10] loss 6.763785\n",
      "[76,   20] loss 6.677532\n",
      "[76,   30] loss 6.431260\n",
      "[76,   40] loss 6.732744\n",
      "[76,   50] loss 8.320942\n",
      "[76,   60] loss 8.164147\n",
      "[76,   70] loss 7.599115\n",
      "[76,   80] loss 8.829418\n",
      "[76,   90] loss 8.056806\n",
      "[76,  100] loss 6.225908\n",
      "[76,  110] loss 6.693379\n",
      "[76,  120] loss 5.688404\n",
      "[76,  130] loss 7.979550\n",
      "[76,  140] loss 9.359795\n",
      "[76,  150] loss 6.984678\n",
      "[76,  160] loss 10.528267\n",
      "[77,   10] loss 8.111867\n",
      "[77,   20] loss 8.103598\n",
      "[77,   30] loss 6.483179\n",
      "[77,   40] loss 7.317667\n",
      "[77,   50] loss 7.856013\n",
      "[77,   60] loss 7.350556\n",
      "[77,   70] loss 6.576387\n",
      "[77,   80] loss 8.633254\n",
      "[77,   90] loss 7.826707\n",
      "[77,  100] loss 6.553985\n",
      "[77,  110] loss 7.743376\n",
      "[77,  120] loss 7.636029\n",
      "[77,  130] loss 6.608037\n",
      "[77,  140] loss 9.066998\n",
      "[77,  150] loss 9.214504\n",
      "[77,  160] loss 8.803111\n",
      "[78,   10] loss 7.262502\n",
      "[78,   20] loss 9.752356\n",
      "[78,   30] loss 7.630836\n",
      "[78,   40] loss 7.413023\n",
      "[78,   50] loss 6.359795\n",
      "[78,   60] loss 5.741103\n",
      "[78,   70] loss 6.987866\n",
      "[78,   80] loss 7.064802\n",
      "[78,   90] loss 7.163136\n",
      "[78,  100] loss 6.778607\n",
      "[78,  110] loss 7.227916\n",
      "[78,  120] loss 8.188659\n",
      "[78,  130] loss 10.388187\n",
      "[78,  140] loss 9.834194\n",
      "[78,  150] loss 8.311735\n",
      "[78,  160] loss 6.991298\n",
      "[79,   10] loss 6.985453\n",
      "[79,   20] loss 5.443239\n",
      "[79,   30] loss 6.890334\n",
      "[79,   40] loss 7.732926\n",
      "[79,   50] loss 7.532091\n",
      "[79,   60] loss 6.157507\n",
      "[79,   70] loss 7.722126\n",
      "[79,   80] loss 8.149458\n",
      "[79,   90] loss 5.841069\n",
      "[79,  100] loss 6.720038\n",
      "[79,  110] loss 6.521438\n",
      "[79,  120] loss 8.335773\n",
      "[79,  130] loss 7.906162\n",
      "[79,  140] loss 6.988759\n",
      "[79,  150] loss 8.203925\n",
      "[79,  160] loss 8.750238\n",
      "[80,   10] loss 5.716976\n",
      "[80,   20] loss 7.678332\n",
      "[80,   30] loss 8.169761\n",
      "[80,   40] loss 8.045646\n",
      "[80,   50] loss 6.951429\n",
      "[80,   60] loss 7.468847\n",
      "[80,   70] loss 6.936614\n",
      "[80,   80] loss 7.802861\n",
      "[80,   90] loss 7.369727\n",
      "[80,  100] loss 8.200135\n",
      "[80,  110] loss 8.046151\n",
      "[80,  120] loss 8.795798\n",
      "[80,  130] loss 6.761084\n",
      "[80,  140] loss 6.680997\n",
      "[80,  150] loss 7.491461\n",
      "[80,  160] loss 6.722621\n",
      "[81,   10] loss 9.049400\n",
      "[81,   20] loss 7.485088\n",
      "[81,   30] loss 7.387036\n",
      "[81,   40] loss 7.937089\n",
      "[81,   50] loss 6.603830\n",
      "[81,   60] loss 7.245314\n",
      "[81,   70] loss 7.361630\n",
      "[81,   80] loss 6.966248\n",
      "[81,   90] loss 6.168979\n",
      "[81,  100] loss 7.589709\n",
      "[81,  110] loss 7.635020\n",
      "[81,  120] loss 6.583779\n",
      "[81,  130] loss 9.463815\n",
      "[81,  140] loss 6.852657\n",
      "[81,  150] loss 10.203678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81,  160] loss 7.816786\n",
      "[82,   10] loss 9.522141\n",
      "[82,   20] loss 9.185812\n",
      "[82,   30] loss 7.071410\n",
      "[82,   40] loss 6.287179\n",
      "[82,   50] loss 6.282547\n",
      "[82,   60] loss 6.825699\n",
      "[82,   70] loss 6.891238\n",
      "[82,   80] loss 7.957542\n",
      "[82,   90] loss 7.422582\n",
      "[82,  100] loss 6.876020\n",
      "[82,  110] loss 7.441336\n",
      "[82,  120] loss 7.191988\n",
      "[82,  130] loss 8.525136\n",
      "[82,  140] loss 6.637956\n",
      "[82,  150] loss 6.401080\n",
      "[82,  160] loss 8.098628\n",
      "[83,   10] loss 5.327407\n",
      "[83,   20] loss 7.414848\n",
      "[83,   30] loss 7.534813\n",
      "[83,   40] loss 7.282266\n",
      "[83,   50] loss 6.685845\n",
      "[83,   60] loss 8.969882\n",
      "[83,   70] loss 8.175414\n",
      "[83,   80] loss 10.020833\n",
      "[83,   90] loss 7.345167\n",
      "[83,  100] loss 7.058191\n",
      "[83,  110] loss 6.920401\n",
      "[83,  120] loss 7.687195\n",
      "[83,  130] loss 7.681737\n",
      "[83,  140] loss 6.936241\n",
      "[83,  150] loss 8.221530\n",
      "[83,  160] loss 7.327930\n",
      "[84,   10] loss 6.875762\n",
      "[84,   20] loss 8.245025\n",
      "[84,   30] loss 9.567143\n",
      "[84,   40] loss 7.622764\n",
      "[84,   50] loss 7.718247\n",
      "[84,   60] loss 9.059585\n",
      "[84,   70] loss 7.952795\n",
      "[84,   80] loss 7.009163\n",
      "[84,   90] loss 8.436394\n",
      "[84,  100] loss 8.277592\n",
      "[84,  110] loss 8.795784\n",
      "[84,  120] loss 7.627674\n",
      "[84,  130] loss 8.215843\n",
      "[84,  140] loss 8.470900\n",
      "[84,  150] loss 6.249850\n",
      "[84,  160] loss 6.858019\n",
      "[85,   10] loss 6.661396\n",
      "[85,   20] loss 7.732013\n",
      "[85,   30] loss 8.393048\n",
      "[85,   40] loss 5.742295\n",
      "[85,   50] loss 7.676433\n",
      "[85,   60] loss 7.848526\n",
      "[85,   70] loss 7.512972\n",
      "[85,   80] loss 8.171792\n",
      "[85,   90] loss 9.491652\n",
      "[85,  100] loss 8.819938\n",
      "[85,  110] loss 9.133053\n",
      "[85,  120] loss 8.788900\n",
      "[85,  130] loss 9.210911\n",
      "[85,  140] loss 6.418034\n",
      "[85,  150] loss 8.003126\n",
      "[85,  160] loss 7.972775\n",
      "[86,   10] loss 7.709565\n",
      "[86,   20] loss 7.759005\n",
      "[86,   30] loss 7.076518\n",
      "[86,   40] loss 7.295418\n",
      "[86,   50] loss 8.689680\n",
      "[86,   60] loss 8.880301\n",
      "[86,   70] loss 8.573614\n",
      "[86,   80] loss 5.961917\n",
      "[86,   90] loss 7.417910\n",
      "[86,  100] loss 6.741120\n",
      "[86,  110] loss 8.482913\n",
      "[86,  120] loss 8.847962\n",
      "[86,  130] loss 6.929392\n",
      "[86,  140] loss 7.530195\n",
      "[86,  150] loss 6.396568\n",
      "[86,  160] loss 7.608577\n",
      "[87,   10] loss 8.093584\n",
      "[87,   20] loss 7.702035\n",
      "[87,   30] loss 6.181660\n",
      "[87,   40] loss 8.279118\n",
      "[87,   50] loss 6.379667\n",
      "[87,   60] loss 9.144103\n",
      "[87,   70] loss 8.798120\n",
      "[87,   80] loss 8.938954\n",
      "[87,   90] loss 8.193436\n",
      "[87,  100] loss 8.952803\n",
      "[87,  110] loss 7.228519\n",
      "[87,  120] loss 9.447092\n",
      "[87,  130] loss 7.117371\n",
      "[87,  140] loss 7.471652\n",
      "[87,  150] loss 6.607017\n",
      "[87,  160] loss 5.866107\n",
      "[88,   10] loss 6.713405\n",
      "[88,   20] loss 6.612194\n",
      "[88,   30] loss 5.879367\n",
      "[88,   40] loss 8.595799\n",
      "[88,   50] loss 7.580913\n",
      "[88,   60] loss 6.318939\n",
      "[88,   70] loss 10.004150\n",
      "[88,   80] loss 8.220467\n",
      "[88,   90] loss 7.637005\n",
      "[88,  100] loss 8.241878\n",
      "[88,  110] loss 7.484159\n",
      "[88,  120] loss 7.892426\n",
      "[88,  130] loss 8.721686\n",
      "[88,  140] loss 7.129926\n",
      "[88,  150] loss 5.707836\n",
      "[88,  160] loss 5.801824\n",
      "[89,   10] loss 7.203007\n",
      "[89,   20] loss 9.767338\n",
      "[89,   30] loss 6.513276\n",
      "[89,   40] loss 8.775188\n",
      "[89,   50] loss 6.617744\n",
      "[89,   60] loss 6.222549\n",
      "[89,   70] loss 8.236312\n",
      "[89,   80] loss 9.503758\n",
      "[89,   90] loss 8.252615\n",
      "[89,  100] loss 6.656351\n",
      "[89,  110] loss 7.642590\n",
      "[89,  120] loss 8.558325\n",
      "[89,  130] loss 5.257499\n",
      "[89,  140] loss 7.756155\n",
      "[89,  150] loss 7.678630\n",
      "[89,  160] loss 6.705253\n",
      "[90,   10] loss 7.506712\n",
      "[90,   20] loss 7.283479\n",
      "[90,   30] loss 8.352544\n",
      "[90,   40] loss 7.639754\n",
      "[90,   50] loss 8.207771\n",
      "[90,   60] loss 6.427602\n",
      "[90,   70] loss 8.980487\n",
      "[90,   80] loss 6.891951\n",
      "[90,   90] loss 10.073144\n",
      "[90,  100] loss 8.561641\n",
      "[90,  110] loss 8.783661\n",
      "[90,  120] loss 7.966260\n",
      "[90,  130] loss 6.582018\n",
      "[90,  140] loss 6.608191\n",
      "[90,  150] loss 8.318336\n",
      "[90,  160] loss 6.847084\n",
      "[91,   10] loss 6.168825\n",
      "[91,   20] loss 8.351844\n",
      "[91,   30] loss 8.997448\n",
      "[91,   40] loss 9.143403\n",
      "[91,   50] loss 10.545149\n",
      "[91,   60] loss 6.779269\n",
      "[91,   70] loss 6.156394\n",
      "[91,   80] loss 7.026602\n",
      "[91,   90] loss 7.596243\n",
      "[91,  100] loss 8.933044\n",
      "[91,  110] loss 5.955246\n",
      "[91,  120] loss 6.591714\n",
      "[91,  130] loss 8.619793\n",
      "[91,  140] loss 7.028729\n",
      "[91,  150] loss 7.183556\n",
      "[91,  160] loss 6.757401\n",
      "[92,   10] loss 7.232603\n",
      "[92,   20] loss 9.097159\n",
      "[92,   30] loss 7.980353\n",
      "[92,   40] loss 7.238128\n",
      "[92,   50] loss 7.160028\n",
      "[92,   60] loss 8.157712\n",
      "[92,   70] loss 7.161166\n",
      "[92,   80] loss 7.425345\n",
      "[92,   90] loss 6.983180\n",
      "[92,  100] loss 6.580116\n",
      "[92,  110] loss 6.858321\n",
      "[92,  120] loss 8.347906\n",
      "[92,  130] loss 7.532781\n",
      "[92,  140] loss 5.939841\n",
      "[92,  150] loss 7.245116\n",
      "[92,  160] loss 6.513670\n",
      "[93,   10] loss 8.838831\n",
      "[93,   20] loss 8.697940\n",
      "[93,   30] loss 8.415258\n",
      "[93,   40] loss 8.279274\n",
      "[93,   50] loss 9.731475\n",
      "[93,   60] loss 6.948991\n",
      "[93,   70] loss 6.637216\n",
      "[93,   80] loss 6.356308\n",
      "[93,   90] loss 6.594974\n",
      "[93,  100] loss 6.053089\n",
      "[93,  110] loss 8.306334\n",
      "[93,  120] loss 7.405951\n",
      "[93,  130] loss 6.868011\n",
      "[93,  140] loss 7.588400\n",
      "[93,  150] loss 6.352293\n",
      "[93,  160] loss 8.881033\n",
      "[94,   10] loss 7.672632\n",
      "[94,   20] loss 6.258195\n",
      "[94,   30] loss 8.409792\n",
      "[94,   40] loss 6.780345\n",
      "[94,   50] loss 8.572839\n",
      "[94,   60] loss 6.694445\n",
      "[94,   70] loss 7.317270\n",
      "[94,   80] loss 8.714816\n",
      "[94,   90] loss 7.215552\n",
      "[94,  100] loss 6.622189\n",
      "[94,  110] loss 7.573232\n",
      "[94,  120] loss 6.974042\n",
      "[94,  130] loss 6.792496\n",
      "[94,  140] loss 6.904041\n",
      "[94,  150] loss 5.623255\n",
      "[94,  160] loss 5.678424\n",
      "[95,   10] loss 8.735363\n",
      "[95,   20] loss 7.109307\n",
      "[95,   30] loss 7.429658\n",
      "[95,   40] loss 7.168493\n",
      "[95,   50] loss 6.961866\n",
      "[95,   60] loss 8.558096\n",
      "[95,   70] loss 7.460380\n",
      "[95,   80] loss 6.928717\n",
      "[95,   90] loss 6.874221\n",
      "[95,  100] loss 7.377302\n",
      "[95,  110] loss 7.440124\n",
      "[95,  120] loss 8.429052\n",
      "[95,  130] loss 6.019109\n",
      "[95,  140] loss 7.421649\n",
      "[95,  150] loss 5.967930\n",
      "[95,  160] loss 7.334552\n",
      "[96,   10] loss 6.990660\n",
      "[96,   20] loss 8.445427\n",
      "[96,   30] loss 7.486696\n",
      "[96,   40] loss 8.218091\n",
      "[96,   50] loss 6.176610\n",
      "[96,   60] loss 7.315970\n",
      "[96,   70] loss 6.851370\n",
      "[96,   80] loss 7.611516\n",
      "[96,   90] loss 7.756468\n",
      "[96,  100] loss 7.587640\n",
      "[96,  110] loss 6.582917\n",
      "[96,  120] loss 8.582906\n",
      "[96,  130] loss 7.912689\n",
      "[96,  140] loss 6.528331\n",
      "[96,  150] loss 7.375320\n",
      "[96,  160] loss 7.634687\n",
      "[97,   10] loss 7.417150\n",
      "[97,   20] loss 5.668251\n",
      "[97,   30] loss 6.614207\n",
      "[97,   40] loss 7.691374\n",
      "[97,   50] loss 6.660351\n",
      "[97,   60] loss 7.168557\n",
      "[97,   70] loss 6.801125\n",
      "[97,   80] loss 6.479124\n",
      "[97,   90] loss 7.691322\n",
      "[97,  100] loss 8.131638\n",
      "[97,  110] loss 7.131368\n",
      "[97,  120] loss 6.815240\n",
      "[97,  130] loss 8.407400\n",
      "[97,  140] loss 6.556747\n",
      "[97,  150] loss 6.922839\n",
      "[97,  160] loss 7.311467\n",
      "[98,   10] loss 6.993672\n",
      "[98,   20] loss 7.502251\n",
      "[98,   30] loss 7.015866\n",
      "[98,   40] loss 8.140326\n",
      "[98,   50] loss 6.225447\n",
      "[98,   60] loss 8.575427\n",
      "[98,   70] loss 7.977434\n",
      "[98,   80] loss 6.554174\n",
      "[98,   90] loss 8.466283\n",
      "[98,  100] loss 7.306144\n",
      "[98,  110] loss 9.038399\n",
      "[98,  120] loss 6.892541\n",
      "[98,  130] loss 6.629041\n",
      "[98,  140] loss 6.997969\n",
      "[98,  150] loss 5.404794\n",
      "[98,  160] loss 6.999517\n",
      "[99,   10] loss 8.274140\n",
      "[99,   20] loss 7.060457\n",
      "[99,   30] loss 8.234689\n",
      "[99,   40] loss 8.674364\n",
      "[99,   50] loss 6.533864\n",
      "[99,   60] loss 6.985749\n",
      "[99,   70] loss 6.874277\n",
      "[99,   80] loss 6.498389\n",
      "[99,   90] loss 7.556318\n",
      "[99,  100] loss 8.480269\n",
      "[99,  110] loss 8.005002\n",
      "[99,  120] loss 7.831843\n",
      "[99,  130] loss 6.828345\n",
      "[99,  140] loss 6.541068\n",
      "[99,  150] loss 6.869606\n",
      "[99,  160] loss 9.921849\n",
      "[100,   10] loss 7.095896\n",
      "[100,   20] loss 6.379509\n",
      "[100,   30] loss 6.525286\n",
      "[100,   40] loss 6.677055\n",
      "[100,   50] loss 8.234664\n",
      "[100,   60] loss 9.510183\n",
      "[100,   70] loss 7.392285\n",
      "[100,   80] loss 6.739964\n",
      "[100,   90] loss 6.137920\n",
      "[100,  100] loss 5.706204\n",
      "[100,  110] loss 6.238872\n",
      "[100,  120] loss 7.629690\n",
      "[100,  130] loss 9.481587\n",
      "[100,  140] loss 7.834459\n",
      "[100,  150] loss 6.599625\n",
      "[100,  160] loss 7.558180\n",
      "[101,   10] loss 6.974307\n",
      "[101,   20] loss 6.797276\n",
      "[101,   30] loss 5.572328\n",
      "[101,   40] loss 7.093791\n",
      "[101,   50] loss 7.556564\n",
      "[101,   60] loss 7.377835\n",
      "[101,   70] loss 8.324587\n",
      "[101,   80] loss 6.660171\n",
      "[101,   90] loss 9.505848\n",
      "[101,  100] loss 7.813714\n",
      "[101,  110] loss 6.464323\n",
      "[101,  120] loss 8.080237\n",
      "[101,  130] loss 7.595599\n",
      "[101,  140] loss 7.234470\n",
      "[101,  150] loss 6.972660\n",
      "[101,  160] loss 6.604826\n",
      "[102,   10] loss 6.976287\n",
      "[102,   20] loss 8.000147\n",
      "[102,   30] loss 6.502234\n",
      "[102,   40] loss 6.366872\n",
      "[102,   50] loss 6.279669\n",
      "[102,   60] loss 8.842868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102,   70] loss 8.573886\n",
      "[102,   80] loss 9.065668\n",
      "[102,   90] loss 6.929632\n",
      "[102,  100] loss 7.574652\n",
      "[102,  110] loss 7.772755\n",
      "[102,  120] loss 5.898374\n",
      "[102,  130] loss 6.703010\n",
      "[102,  140] loss 6.956129\n",
      "[102,  150] loss 6.250252\n",
      "[102,  160] loss 6.935761\n",
      "[103,   10] loss 7.288781\n",
      "[103,   20] loss 7.668761\n",
      "[103,   30] loss 7.400780\n",
      "[103,   40] loss 6.277049\n",
      "[103,   50] loss 8.791657\n",
      "[103,   60] loss 8.661762\n",
      "[103,   70] loss 8.002969\n",
      "[103,   80] loss 6.588255\n",
      "[103,   90] loss 7.507830\n",
      "[103,  100] loss 6.329530\n",
      "[103,  110] loss 6.639062\n",
      "[103,  120] loss 7.146192\n",
      "[103,  130] loss 6.684461\n",
      "[103,  140] loss 6.159564\n",
      "[103,  150] loss 8.185580\n",
      "[103,  160] loss 6.516721\n",
      "[104,   10] loss 5.910183\n",
      "[104,   20] loss 6.938333\n",
      "[104,   30] loss 7.869914\n",
      "[104,   40] loss 6.906696\n",
      "[104,   50] loss 8.811371\n",
      "[104,   60] loss 7.641794\n",
      "[104,   70] loss 6.190470\n",
      "[104,   80] loss 5.544684\n",
      "[104,   90] loss 6.299822\n",
      "[104,  100] loss 7.725117\n",
      "[104,  110] loss 7.272879\n",
      "[104,  120] loss 6.940223\n",
      "[104,  130] loss 7.513552\n",
      "[104,  140] loss 7.999881\n",
      "[104,  150] loss 6.879342\n",
      "[104,  160] loss 10.135488\n",
      "[105,   10] loss 8.908018\n",
      "[105,   20] loss 14.762642\n",
      "[105,   30] loss 12.096332\n",
      "[105,   40] loss 12.924262\n",
      "[105,   50] loss 12.397382\n",
      "[105,   60] loss 13.067255\n",
      "[105,   70] loss 11.939878\n",
      "[105,   80] loss 10.335711\n",
      "[105,   90] loss 9.605496\n",
      "[105,  100] loss 11.322417\n",
      "[105,  110] loss 12.102638\n",
      "[105,  120] loss 13.827265\n",
      "[105,  130] loss 12.565306\n",
      "[105,  140] loss 11.021583\n",
      "[105,  150] loss 10.940312\n",
      "[105,  160] loss 10.619347\n",
      "[106,   10] loss 11.153348\n",
      "[106,   20] loss 10.377521\n",
      "[106,   30] loss 9.994281\n",
      "[106,   40] loss 9.939289\n",
      "[106,   50] loss 9.733370\n",
      "[106,   60] loss 9.335283\n",
      "[106,   70] loss 9.678162\n",
      "[106,   80] loss 8.280152\n",
      "[106,   90] loss 7.374179\n",
      "[106,  100] loss 10.703001\n",
      "[106,  110] loss 8.937724\n",
      "[106,  120] loss 8.545623\n",
      "[106,  130] loss 8.574898\n",
      "[106,  140] loss 7.465037\n",
      "[106,  150] loss 8.649283\n",
      "[106,  160] loss 7.713664\n",
      "[107,   10] loss 8.663271\n",
      "[107,   20] loss 7.923535\n",
      "[107,   30] loss 8.062673\n",
      "[107,   40] loss 10.562710\n",
      "[107,   50] loss 8.917203\n",
      "[107,   60] loss 8.077542\n",
      "[107,   70] loss 7.800463\n",
      "[107,   80] loss 8.442766\n",
      "[107,   90] loss 9.534171\n",
      "[107,  100] loss 6.697377\n",
      "[107,  110] loss 7.009080\n",
      "[107,  120] loss 7.747145\n",
      "[107,  130] loss 8.880556\n",
      "[107,  140] loss 9.485254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7bfd1a0da5c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#         sample=sample.double()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a1ef1b8ae240>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m         hidden=(torch.zeros([2, concat.size(0), self.hidden_dim],dtype=torch.double),\n\u001b[1;32m    135\u001b[0m                 torch.zeros([2, concat.size(0), self.hidden_dim],dtype=torch.double))\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mLstmOutp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;31m#         ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLstmOutp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLstmOutp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##################\n",
    "#  lr=1e-5\n",
    "##################\n",
    "iteration=0\n",
    "numepoch=0\n",
    "dloador=DataLoader(dataloader,batch_size=10,shuffle=True,num_workers=8,pin_memory=CUDA)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
    "criterion=nn.L1Loss()\n",
    "Net=model.double()\n",
    "for epoch in np.arange(200):\n",
    "    numepoch+=1\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(dloador,0):\n",
    "        \n",
    "        iteration+=1\n",
    "#         ipdb.set_trace()\n",
    "        sample,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        sample.to(device)\n",
    "        labels.to(device)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "\n",
    "#         print(labels)\n",
    "        optimizer.zero_grad()\n",
    "#         sample=sample.double()\n",
    "        outputs=Net(sample.double())\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        if i%10==9:\n",
    "            print('[%d,%5d] loss %f' %(epoch+1,i+1,running_loss/10))\n",
    "            running_loss=0.0\n",
    "print('finish training')\n",
    "t.save(Net.state_dict(),'/home/wcj/ReferenceProject/PPGnet/checkpoints/0430Net-epoch-%d-iteration%d.pth'%(numepoch,iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    1] loss 9.633750\n",
      "[1,    2] loss 10.868176\n",
      "[1,    3] loss 8.799723\n",
      "[1,    4] loss 8.057599\n",
      "[1,    5] loss 9.887116\n",
      "[1,    6] loss 8.868018\n",
      "[1,    7] loss 8.018954\n",
      "[1,    8] loss 8.674002\n",
      "[1,    9] loss 11.006958\n",
      "[1,   10] loss 8.130774\n",
      "[1,   11] loss 8.201711\n",
      "[1,   12] loss 8.026394\n",
      "[1,   13] loss 7.844780\n",
      "[1,   14] loss 8.032592\n",
      "[1,   15] loss 7.374043\n",
      "[1,   16] loss 8.098491\n",
      "[1,   17] loss 7.544830\n",
      "[2,    1] loss 8.448002\n",
      "[2,    2] loss 7.455944\n",
      "[2,    3] loss 7.795868\n",
      "[2,    4] loss 7.627667\n",
      "[2,    5] loss 9.042590\n",
      "[2,    6] loss 7.449772\n",
      "[2,    7] loss 9.121800\n",
      "[2,    8] loss 8.756799\n",
      "[2,    9] loss 7.902645\n",
      "[2,   10] loss 6.771464\n",
      "[2,   11] loss 8.405308\n",
      "[2,   12] loss 7.653183\n",
      "[2,   13] loss 8.430473\n",
      "[2,   14] loss 7.559853\n",
      "[2,   15] loss 7.347771\n",
      "[2,   16] loss 7.339405\n",
      "[2,   17] loss 10.559498\n",
      "[3,    1] loss 8.362894\n",
      "[3,    2] loss 7.804941\n",
      "[3,    3] loss 8.408666\n",
      "[3,    4] loss 6.678304\n",
      "[3,    5] loss 7.920934\n",
      "[3,    6] loss 8.073853\n",
      "[3,    7] loss 7.949444\n",
      "[3,    8] loss 6.727269\n",
      "[3,    9] loss 7.042478\n",
      "[3,   10] loss 6.913246\n",
      "[3,   11] loss 7.124822\n",
      "[3,   12] loss 8.243228\n",
      "[3,   13] loss 7.662376\n",
      "[3,   14] loss 6.271603\n",
      "[3,   15] loss 6.876373\n",
      "[3,   16] loss 7.068955\n",
      "[3,   17] loss 7.113960\n",
      "[4,    1] loss 6.815251\n",
      "[4,    2] loss 6.112313\n",
      "[4,    3] loss 7.109365\n",
      "[4,    4] loss 6.944052\n",
      "[4,    5] loss 7.673149\n",
      "[4,    6] loss 7.549266\n",
      "[4,    7] loss 6.743914\n",
      "[4,    8] loss 7.695510\n",
      "[4,    9] loss 8.270294\n",
      "[4,   10] loss 7.081019\n",
      "[4,   11] loss 6.975279\n",
      "[4,   12] loss 6.585642\n",
      "[4,   13] loss 8.526190\n",
      "[4,   14] loss 6.536358\n",
      "[4,   15] loss 6.319085\n",
      "[4,   16] loss 6.376475\n",
      "[4,   17] loss 6.247245\n",
      "[5,    1] loss 7.413447\n",
      "[5,    2] loss 6.073688\n",
      "[5,    3] loss 6.683382\n",
      "[5,    4] loss 6.022523\n",
      "[5,    5] loss 7.180113\n",
      "[5,    6] loss 7.399004\n",
      "[5,    7] loss 5.995705\n",
      "[5,    8] loss 6.957880\n",
      "[5,    9] loss 6.884784\n",
      "[5,   10] loss 7.396896\n",
      "[5,   11] loss 6.226158\n",
      "[5,   12] loss 7.541138\n",
      "[5,   13] loss 7.382711\n",
      "[5,   14] loss 6.361550\n",
      "[5,   15] loss 7.262642\n",
      "[5,   16] loss 7.655522\n",
      "[5,   17] loss 7.377583\n",
      "[6,    1] loss 6.538173\n",
      "[6,    2] loss 6.742226\n",
      "[6,    3] loss 6.408460\n",
      "[6,    4] loss 6.942892\n",
      "[6,    5] loss 6.112718\n",
      "[6,    6] loss 6.497767\n",
      "[6,    7] loss 6.424529\n",
      "[6,    8] loss 6.377611\n",
      "[6,    9] loss 6.920797\n",
      "[6,   10] loss 6.935352\n",
      "[6,   11] loss 6.949997\n",
      "[6,   12] loss 6.741616\n",
      "[6,   13] loss 6.914252\n",
      "[6,   14] loss 6.696241\n",
      "[6,   15] loss 6.332329\n",
      "[6,   16] loss 6.441272\n",
      "[6,   17] loss 6.601374\n",
      "[7,    1] loss 6.273452\n",
      "[7,    2] loss 7.342873\n",
      "[7,    3] loss 7.443883\n",
      "[7,    4] loss 7.102289\n",
      "[7,    5] loss 6.651771\n",
      "[7,    6] loss 9.081444\n",
      "[7,    7] loss 6.854524\n",
      "[7,    8] loss 5.947291\n",
      "[7,    9] loss 6.811007\n",
      "[7,   10] loss 6.098647\n",
      "[7,   11] loss 6.291721\n",
      "[7,   12] loss 7.014422\n",
      "[7,   13] loss 7.101212\n",
      "[7,   14] loss 5.811315\n",
      "[7,   15] loss 5.496557\n",
      "[7,   16] loss 6.861879\n",
      "[7,   17] loss 8.533049\n",
      "[8,    1] loss 6.013801\n",
      "[8,    2] loss 6.872813\n",
      "[8,    3] loss 6.508055\n",
      "[8,    4] loss 5.308043\n",
      "[8,    5] loss 6.322893\n",
      "[8,    6] loss 8.158491\n",
      "[8,    7] loss 5.172286\n",
      "[8,    8] loss 6.467474\n",
      "[8,    9] loss 5.853932\n",
      "[8,   10] loss 6.118036\n",
      "[8,   11] loss 6.884876\n",
      "[8,   12] loss 7.086659\n",
      "[8,   13] loss 7.823941\n",
      "[8,   14] loss 7.057087\n",
      "[8,   15] loss 6.326685\n",
      "[8,   16] loss 6.941696\n",
      "[8,   17] loss 7.890270\n",
      "[9,    1] loss 6.184648\n",
      "[9,    2] loss 6.284431\n",
      "[9,    3] loss 6.689465\n",
      "[9,    4] loss 6.547464\n",
      "[9,    5] loss 6.965455\n",
      "[9,    6] loss 4.944370\n",
      "[9,    7] loss 6.089054\n",
      "[9,    8] loss 6.035952\n",
      "[9,    9] loss 6.117286\n",
      "[9,   10] loss 5.908083\n",
      "[9,   11] loss 6.760148\n",
      "[9,   12] loss 5.640139\n",
      "[9,   13] loss 7.678070\n",
      "[9,   14] loss 7.144763\n",
      "[9,   15] loss 6.057368\n",
      "[9,   16] loss 6.841266\n",
      "[9,   17] loss 5.681125\n",
      "[10,    1] loss 7.163661\n",
      "[10,    2] loss 6.283796\n",
      "[10,    3] loss 6.643837\n",
      "[10,    4] loss 7.417275\n",
      "[10,    5] loss 5.867009\n",
      "[10,    6] loss 6.024964\n",
      "[10,    7] loss 6.759552\n",
      "[10,    8] loss 5.804875\n",
      "[10,    9] loss 6.433329\n",
      "[10,   10] loss 5.848101\n",
      "[10,   11] loss 5.171621\n",
      "[10,   12] loss 6.630192\n",
      "[10,   13] loss 6.544426\n",
      "[10,   14] loss 7.792646\n",
      "[10,   15] loss 6.109266\n",
      "[10,   16] loss 6.798457\n",
      "[10,   17] loss 7.305593\n",
      "[11,    1] loss 6.338112\n",
      "[11,    2] loss 6.965512\n",
      "[11,    3] loss 7.227238\n",
      "[11,    4] loss 6.031318\n",
      "[11,    5] loss 5.487300\n",
      "[11,    6] loss 6.179790\n",
      "[11,    7] loss 5.659634\n",
      "[11,    8] loss 5.882995\n",
      "[11,    9] loss 6.377626\n",
      "[11,   10] loss 6.836378\n",
      "[11,   11] loss 5.501079\n",
      "[11,   12] loss 7.083026\n",
      "[11,   13] loss 6.193268\n",
      "[11,   14] loss 5.944848\n",
      "[11,   15] loss 5.528768\n",
      "[11,   16] loss 5.421983\n",
      "[11,   17] loss 6.185869\n",
      "[12,    1] loss 6.638137\n",
      "[12,    2] loss 6.159700\n",
      "[12,    3] loss 7.329088\n",
      "[12,    4] loss 7.294964\n",
      "[12,    5] loss 6.070428\n",
      "[12,    6] loss 6.480842\n",
      "[12,    7] loss 5.240105\n",
      "[12,    8] loss 6.251639\n",
      "[12,    9] loss 7.016170\n",
      "[12,   10] loss 6.391823\n",
      "[12,   11] loss 6.301201\n",
      "[12,   12] loss 5.987705\n",
      "[12,   13] loss 6.308899\n",
      "[12,   14] loss 6.993136\n",
      "[12,   15] loss 5.816062\n",
      "[12,   16] loss 6.557467\n",
      "[12,   17] loss 8.431790\n",
      "[13,    1] loss 6.291408\n",
      "[13,    2] loss 6.948579\n",
      "[13,    3] loss 5.757957\n",
      "[13,    4] loss 5.950359\n",
      "[13,    5] loss 6.742453\n",
      "[13,    6] loss 5.524400\n",
      "[13,    7] loss 7.242365\n",
      "[13,    8] loss 5.749118\n",
      "[13,    9] loss 5.247700\n",
      "[13,   10] loss 5.776078\n",
      "[13,   11] loss 5.802167\n",
      "[13,   12] loss 6.182629\n",
      "[13,   13] loss 5.531986\n",
      "[13,   14] loss 6.576420\n",
      "[13,   15] loss 5.944064\n",
      "[13,   16] loss 6.191286\n",
      "[13,   17] loss 8.833335\n",
      "[14,    1] loss 5.904968\n",
      "[14,    2] loss 6.621672\n",
      "[14,    3] loss 6.531731\n",
      "[14,    4] loss 7.233726\n",
      "[14,    5] loss 6.434477\n",
      "[14,    6] loss 5.065738\n",
      "[14,    7] loss 6.139504\n",
      "[14,    8] loss 5.204833\n",
      "[14,    9] loss 7.333566\n",
      "[14,   10] loss 6.118615\n",
      "[14,   11] loss 5.499399\n",
      "[14,   12] loss 6.901882\n",
      "[14,   13] loss 6.746410\n",
      "[14,   14] loss 7.531333\n",
      "[14,   15] loss 6.712820\n",
      "[14,   16] loss 5.939392\n",
      "[14,   17] loss 9.695820\n",
      "[15,    1] loss 5.491041\n",
      "[15,    2] loss 6.262701\n",
      "[15,    3] loss 7.546750\n",
      "[15,    4] loss 5.160679\n",
      "[15,    5] loss 5.025312\n",
      "[15,    6] loss 5.782825\n",
      "[15,    7] loss 6.190111\n",
      "[15,    8] loss 5.385294\n",
      "[15,    9] loss 5.797070\n",
      "[15,   10] loss 6.994989\n",
      "[15,   11] loss 6.820217\n",
      "[15,   12] loss 6.047972\n",
      "[15,   13] loss 5.868383\n",
      "[15,   14] loss 6.392833\n",
      "[15,   15] loss 7.015192\n",
      "[15,   16] loss 5.370693\n",
      "[15,   17] loss 7.452406\n",
      "[16,    1] loss 6.774930\n",
      "[16,    2] loss 6.659841\n",
      "[16,    3] loss 6.564106\n",
      "[16,    4] loss 5.252307\n",
      "[16,    5] loss 6.251742\n",
      "[16,    6] loss 6.269031\n",
      "[16,    7] loss 6.543247\n",
      "[16,    8] loss 5.879356\n",
      "[16,    9] loss 7.408955\n",
      "[16,   10] loss 5.926792\n",
      "[16,   11] loss 5.524086\n",
      "[16,   12] loss 6.392978\n",
      "[16,   13] loss 6.854129\n",
      "[16,   14] loss 5.845249\n",
      "[16,   15] loss 5.811871\n",
      "[16,   16] loss 6.218211\n",
      "[16,   17] loss 7.173890\n",
      "[17,    1] loss 5.900686\n",
      "[17,    2] loss 7.500924\n",
      "[17,    3] loss 6.980805\n",
      "[17,    4] loss 7.771977\n",
      "[17,    5] loss 6.508978\n",
      "[17,    6] loss 5.996377\n",
      "[17,    7] loss 5.146662\n",
      "[17,    8] loss 5.809972\n",
      "[17,    9] loss 6.172125\n",
      "[17,   10] loss 6.276830\n",
      "[17,   11] loss 5.089264\n",
      "[17,   12] loss 5.402220\n",
      "[17,   13] loss 5.316832\n",
      "[17,   14] loss 6.870943\n",
      "[17,   15] loss 5.353376\n",
      "[17,   16] loss 5.470259\n",
      "[17,   17] loss 9.427176\n",
      "[18,    1] loss 6.464581\n",
      "[18,    2] loss 6.920514\n",
      "[18,    3] loss 5.197540\n",
      "[18,    4] loss 6.739104\n",
      "[18,    5] loss 6.013437\n",
      "[18,    6] loss 5.405421\n",
      "[18,    7] loss 6.104404\n",
      "[18,    8] loss 5.069152\n",
      "[18,    9] loss 6.173646\n",
      "[18,   10] loss 5.743603\n",
      "[18,   11] loss 5.562024\n",
      "[18,   12] loss 4.994741\n",
      "[18,   13] loss 6.377957\n",
      "[18,   14] loss 6.327279\n",
      "[18,   15] loss 5.609251\n",
      "[18,   16] loss 7.249424\n",
      "[18,   17] loss 8.174216\n",
      "[19,    1] loss 5.676098\n",
      "[19,    2] loss 5.839097\n",
      "[19,    3] loss 6.382834\n",
      "[19,    4] loss 6.569574\n",
      "[19,    5] loss 5.931902\n",
      "[19,    6] loss 6.202633\n",
      "[19,    7] loss 4.988991\n",
      "[19,    8] loss 5.548036\n",
      "[19,    9] loss 6.977327\n",
      "[19,   10] loss 5.641788\n",
      "[19,   11] loss 5.299043\n",
      "[19,   12] loss 6.989201\n",
      "[19,   13] loss 5.469606\n",
      "[19,   14] loss 7.014837\n",
      "[19,   15] loss 5.922340\n",
      "[19,   16] loss 5.354065\n",
      "[19,   17] loss 7.456953\n",
      "[20,    1] loss 6.704190\n",
      "[20,    2] loss 5.504874\n",
      "[20,    3] loss 6.423544\n",
      "[20,    4] loss 5.881522\n",
      "[20,    5] loss 5.495584\n",
      "[20,    6] loss 5.506822\n",
      "[20,    7] loss 5.843092\n",
      "[20,    8] loss 6.037820\n",
      "[20,    9] loss 5.073038\n",
      "[20,   10] loss 4.945734\n",
      "[20,   11] loss 6.270359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20,   12] loss 5.163803\n",
      "[20,   13] loss 4.661687\n",
      "[20,   14] loss 5.549439\n",
      "[20,   15] loss 6.642075\n",
      "[20,   16] loss 6.075464\n",
      "[20,   17] loss 9.372734\n",
      "[21,    1] loss 5.818889\n",
      "[21,    2] loss 5.433639\n",
      "[21,    3] loss 6.212713\n",
      "[21,    4] loss 5.225773\n",
      "[21,    5] loss 6.133182\n",
      "[21,    6] loss 5.301756\n",
      "[21,    7] loss 6.540111\n",
      "[21,    8] loss 6.726844\n",
      "[21,    9] loss 6.381639\n",
      "[21,   10] loss 5.161125\n",
      "[21,   11] loss 6.705449\n",
      "[21,   12] loss 5.548649\n",
      "[21,   13] loss 5.036759\n",
      "[21,   14] loss 5.899362\n",
      "[21,   15] loss 6.165009\n",
      "[21,   16] loss 5.674161\n",
      "[21,   17] loss 5.051189\n",
      "[22,    1] loss 5.284895\n",
      "[22,    2] loss 5.390654\n",
      "[22,    3] loss 5.421516\n",
      "[22,    4] loss 6.155219\n",
      "[22,    5] loss 6.667243\n",
      "[22,    6] loss 6.259992\n",
      "[22,    7] loss 6.092795\n",
      "[22,    8] loss 5.550002\n",
      "[22,    9] loss 5.527746\n",
      "[22,   10] loss 5.834141\n",
      "[22,   11] loss 5.316856\n",
      "[22,   12] loss 5.423941\n",
      "[22,   13] loss 5.234830\n",
      "[22,   14] loss 5.036290\n",
      "[22,   15] loss 7.124423\n",
      "[22,   16] loss 5.550985\n",
      "[22,   17] loss 8.630186\n",
      "[23,    1] loss 5.873681\n",
      "[23,    2] loss 5.264035\n",
      "[23,    3] loss 5.671136\n",
      "[23,    4] loss 5.823403\n",
      "[23,    5] loss 5.137216\n",
      "[23,    6] loss 6.105360\n",
      "[23,    7] loss 5.373136\n",
      "[23,    8] loss 5.255721\n",
      "[23,    9] loss 5.950754\n",
      "[23,   10] loss 5.579666\n",
      "[23,   11] loss 5.632102\n",
      "[23,   12] loss 6.422280\n",
      "[23,   13] loss 6.511704\n",
      "[23,   14] loss 5.870531\n",
      "[23,   15] loss 6.170711\n",
      "[23,   16] loss 5.073996\n",
      "[23,   17] loss 7.691606\n",
      "[24,    1] loss 5.569423\n",
      "[24,    2] loss 5.535852\n",
      "[24,    3] loss 5.899908\n",
      "[24,    4] loss 4.975316\n",
      "[24,    5] loss 4.956817\n",
      "[24,    6] loss 6.750317\n",
      "[24,    7] loss 5.717259\n",
      "[24,    8] loss 5.794846\n",
      "[24,    9] loss 4.923557\n",
      "[24,   10] loss 6.636632\n",
      "[24,   11] loss 6.101650\n",
      "[24,   12] loss 5.124560\n",
      "[24,   13] loss 6.233122\n",
      "[24,   14] loss 5.948999\n",
      "[24,   15] loss 5.422539\n",
      "[24,   16] loss 5.405511\n",
      "[24,   17] loss 7.254173\n",
      "[25,    1] loss 5.381708\n",
      "[25,    2] loss 6.690084\n",
      "[25,    3] loss 5.543185\n",
      "[25,    4] loss 5.875421\n",
      "[25,    5] loss 4.701113\n",
      "[25,    6] loss 5.407904\n",
      "[25,    7] loss 5.755769\n",
      "[25,    8] loss 5.825824\n",
      "[25,    9] loss 5.989919\n",
      "[25,   10] loss 6.985423\n",
      "[25,   11] loss 5.002271\n",
      "[25,   12] loss 6.516926\n",
      "[25,   13] loss 5.857174\n",
      "[25,   14] loss 5.560324\n",
      "[25,   15] loss 7.061988\n",
      "[25,   16] loss 6.486544\n",
      "[25,   17] loss 4.544321\n",
      "[26,    1] loss 5.381964\n",
      "[26,    2] loss 6.897694\n",
      "[26,    3] loss 4.798061\n",
      "[26,    4] loss 6.251261\n",
      "[26,    5] loss 5.796364\n",
      "[26,    6] loss 5.764843\n",
      "[26,    7] loss 4.972591\n",
      "[26,    8] loss 5.618646\n",
      "[26,    9] loss 6.187659\n",
      "[26,   10] loss 6.058963\n",
      "[26,   11] loss 5.575376\n",
      "[26,   12] loss 5.461781\n",
      "[26,   13] loss 5.170258\n",
      "[26,   14] loss 5.726225\n",
      "[26,   15] loss 4.995806\n",
      "[26,   16] loss 5.801037\n",
      "[26,   17] loss 6.106325\n",
      "[27,    1] loss 5.328620\n",
      "[27,    2] loss 6.124617\n",
      "[27,    3] loss 4.666820\n",
      "[27,    4] loss 5.162493\n",
      "[27,    5] loss 6.220319\n",
      "[27,    6] loss 4.912539\n",
      "[27,    7] loss 5.468770\n",
      "[27,    8] loss 7.252732\n",
      "[27,    9] loss 5.019612\n",
      "[27,   10] loss 5.111177\n",
      "[27,   11] loss 5.364262\n",
      "[27,   12] loss 5.625116\n",
      "[27,   13] loss 5.821860\n",
      "[27,   14] loss 6.252342\n",
      "[27,   15] loss 5.895531\n",
      "[27,   16] loss 4.553393\n",
      "[27,   17] loss 10.350822\n",
      "[28,    1] loss 5.030560\n",
      "[28,    2] loss 5.387058\n",
      "[28,    3] loss 4.748105\n",
      "[28,    4] loss 4.543619\n",
      "[28,    5] loss 5.365018\n",
      "[28,    6] loss 5.839134\n",
      "[28,    7] loss 4.628276\n",
      "[28,    8] loss 5.104806\n",
      "[28,    9] loss 6.512585\n",
      "[28,   10] loss 5.966197\n",
      "[28,   11] loss 5.379840\n",
      "[28,   12] loss 6.394935\n",
      "[28,   13] loss 6.091842\n",
      "[28,   14] loss 6.331073\n",
      "[28,   15] loss 5.712562\n",
      "[28,   16] loss 5.538312\n",
      "[28,   17] loss 7.092238\n",
      "[29,    1] loss 5.147435\n",
      "[29,    2] loss 5.626919\n",
      "[29,    3] loss 5.583664\n",
      "[29,    4] loss 5.730918\n",
      "[29,    5] loss 5.872359\n",
      "[29,    6] loss 5.615245\n",
      "[29,    7] loss 5.545393\n",
      "[29,    8] loss 6.343818\n",
      "[29,    9] loss 5.501091\n",
      "[29,   10] loss 5.156972\n",
      "[29,   11] loss 5.905258\n",
      "[29,   12] loss 4.977841\n",
      "[29,   13] loss 5.351250\n",
      "[29,   14] loss 5.650368\n",
      "[29,   15] loss 5.788550\n",
      "[29,   16] loss 5.137320\n",
      "[29,   17] loss 8.741172\n",
      "[30,    1] loss 5.606396\n",
      "[30,    2] loss 5.236353\n",
      "[30,    3] loss 5.249609\n",
      "[30,    4] loss 5.196826\n",
      "[30,    5] loss 5.752063\n",
      "[30,    6] loss 5.905882\n",
      "[30,    7] loss 4.713293\n",
      "[30,    8] loss 6.036276\n",
      "[30,    9] loss 4.678536\n",
      "[30,   10] loss 5.746158\n",
      "[30,   11] loss 5.581862\n",
      "[30,   12] loss 6.334463\n",
      "[30,   13] loss 5.210483\n",
      "[30,   14] loss 5.736061\n",
      "[30,   15] loss 5.898260\n",
      "[30,   16] loss 5.459708\n",
      "[30,   17] loss 6.345669\n",
      "[31,    1] loss 5.691068\n",
      "[31,    2] loss 6.529342\n",
      "[31,    3] loss 5.654895\n",
      "[31,    4] loss 6.327826\n",
      "[31,    5] loss 4.914778\n",
      "[31,    6] loss 5.514518\n",
      "[31,    7] loss 4.248765\n",
      "[31,    8] loss 5.111029\n",
      "[31,    9] loss 5.031304\n",
      "[31,   10] loss 5.580106\n",
      "[31,   11] loss 5.461702\n",
      "[31,   12] loss 5.813970\n",
      "[31,   13] loss 5.415834\n",
      "[31,   14] loss 6.640189\n",
      "[31,   15] loss 5.854246\n",
      "[31,   16] loss 5.407959\n",
      "[31,   17] loss 6.411613\n",
      "[32,    1] loss 5.768035\n",
      "[32,    2] loss 6.801681\n",
      "[32,    3] loss 4.617484\n",
      "[32,    4] loss 5.462955\n",
      "[32,    5] loss 5.841485\n",
      "[32,    6] loss 6.172000\n",
      "[32,    7] loss 5.843538\n",
      "[32,    8] loss 5.395673\n",
      "[32,    9] loss 5.658249\n",
      "[32,   10] loss 5.579963\n",
      "[32,   11] loss 6.313849\n",
      "[32,   12] loss 5.969525\n",
      "[32,   13] loss 5.779836\n",
      "[32,   14] loss 5.379391\n",
      "[32,   15] loss 5.505811\n",
      "[32,   16] loss 6.073791\n",
      "[32,   17] loss 6.855802\n",
      "[33,    1] loss 5.334478\n",
      "[33,    2] loss 5.424481\n",
      "[33,    3] loss 6.062498\n",
      "[33,    4] loss 5.373324\n",
      "[33,    5] loss 5.780130\n",
      "[33,    6] loss 6.358213\n",
      "[33,    7] loss 5.501735\n",
      "[33,    8] loss 5.290276\n",
      "[33,    9] loss 5.056331\n",
      "[33,   10] loss 5.081243\n",
      "[33,   11] loss 5.907104\n",
      "[33,   12] loss 5.123069\n",
      "[33,   13] loss 6.401847\n",
      "[33,   14] loss 5.603730\n",
      "[33,   15] loss 4.864083\n",
      "[33,   16] loss 5.549260\n",
      "[33,   17] loss 8.717881\n",
      "[34,    1] loss 5.133057\n",
      "[34,    2] loss 5.258961\n",
      "[34,    3] loss 5.165637\n",
      "[34,    4] loss 5.666292\n",
      "[34,    5] loss 6.582307\n",
      "[34,    6] loss 4.784397\n",
      "[34,    7] loss 5.927282\n",
      "[34,    8] loss 5.604281\n",
      "[34,    9] loss 5.427503\n",
      "[34,   10] loss 5.123618\n",
      "[34,   11] loss 6.625421\n",
      "[34,   12] loss 5.150198\n",
      "[34,   13] loss 4.697964\n",
      "[34,   14] loss 5.803914\n",
      "[34,   15] loss 4.841431\n",
      "[34,   16] loss 5.291062\n",
      "[34,   17] loss 6.685151\n",
      "[35,    1] loss 5.267137\n",
      "[35,    2] loss 6.155924\n",
      "[35,    3] loss 5.833047\n",
      "[35,    4] loss 5.150491\n",
      "[35,    5] loss 4.835409\n",
      "[35,    6] loss 4.599390\n",
      "[35,    7] loss 5.078985\n",
      "[35,    8] loss 5.656461\n",
      "[35,    9] loss 4.959267\n",
      "[35,   10] loss 4.704910\n",
      "[35,   11] loss 4.896835\n",
      "[35,   12] loss 5.244472\n",
      "[35,   13] loss 5.218064\n",
      "[35,   14] loss 6.120492\n",
      "[35,   15] loss 6.217026\n",
      "[35,   16] loss 5.810302\n",
      "[35,   17] loss 4.743846\n",
      "[36,    1] loss 5.437131\n",
      "[36,    2] loss 5.676311\n",
      "[36,    3] loss 5.278320\n",
      "[36,    4] loss 4.335551\n",
      "[36,    5] loss 4.626474\n",
      "[36,    6] loss 5.092281\n",
      "[36,    7] loss 5.066601\n",
      "[36,    8] loss 5.601173\n",
      "[36,    9] loss 5.375421\n",
      "[36,   10] loss 4.885326\n",
      "[36,   11] loss 5.650382\n",
      "[36,   12] loss 5.984891\n",
      "[36,   13] loss 5.978349\n",
      "[36,   14] loss 5.834849\n",
      "[36,   15] loss 5.129435\n",
      "[36,   16] loss 5.662934\n",
      "[36,   17] loss 6.011045\n",
      "[37,    1] loss 4.883486\n",
      "[37,    2] loss 4.992852\n",
      "[37,    3] loss 4.735303\n",
      "[37,    4] loss 6.356061\n",
      "[37,    5] loss 4.648811\n",
      "[37,    6] loss 6.065114\n",
      "[37,    7] loss 5.089327\n",
      "[37,    8] loss 5.525861\n",
      "[37,    9] loss 5.596651\n",
      "[37,   10] loss 5.229006\n",
      "[37,   11] loss 4.824080\n",
      "[37,   12] loss 5.335621\n",
      "[37,   13] loss 5.322428\n",
      "[37,   14] loss 5.164253\n",
      "[37,   15] loss 5.445389\n",
      "[37,   16] loss 6.045378\n",
      "[37,   17] loss 7.905622\n",
      "[38,    1] loss 4.913909\n",
      "[38,    2] loss 5.974499\n",
      "[38,    3] loss 5.571778\n",
      "[38,    4] loss 4.997710\n",
      "[38,    5] loss 5.206573\n",
      "[38,    6] loss 5.118409\n",
      "[38,    7] loss 6.102409\n",
      "[38,    8] loss 4.468127\n",
      "[38,    9] loss 5.792239\n",
      "[38,   10] loss 4.828607\n",
      "[38,   11] loss 4.678708\n",
      "[38,   12] loss 4.589713\n",
      "[38,   13] loss 3.839413\n",
      "[38,   14] loss 7.016531\n",
      "[38,   15] loss 5.407561\n",
      "[38,   16] loss 4.939887\n",
      "[38,   17] loss 5.568889\n",
      "[39,    1] loss 5.542844\n",
      "[39,    2] loss 5.740713\n",
      "[39,    3] loss 4.234814\n",
      "[39,    4] loss 4.947650\n",
      "[39,    5] loss 5.564160\n",
      "[39,    6] loss 5.046643\n",
      "[39,    7] loss 5.846415\n",
      "[39,    8] loss 5.515048\n",
      "[39,    9] loss 6.020774\n",
      "[39,   10] loss 4.745046\n",
      "[39,   11] loss 5.241971\n",
      "[39,   12] loss 6.104280\n",
      "[39,   13] loss 5.024405\n",
      "[39,   14] loss 5.556820\n",
      "[39,   15] loss 5.355096\n",
      "[39,   16] loss 5.486490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39,   17] loss 5.817623\n",
      "[40,    1] loss 4.272336\n",
      "[40,    2] loss 6.127992\n",
      "[40,    3] loss 5.042657\n",
      "[40,    4] loss 4.461900\n",
      "[40,    5] loss 5.119647\n",
      "[40,    6] loss 6.269674\n",
      "[40,    7] loss 5.277754\n",
      "[40,    8] loss 4.596263\n",
      "[40,    9] loss 6.395602\n",
      "[40,   10] loss 4.929570\n",
      "[40,   11] loss 5.195877\n",
      "[40,   12] loss 4.689006\n",
      "[40,   13] loss 5.297588\n",
      "[40,   14] loss 5.197390\n",
      "[40,   15] loss 5.674514\n",
      "[40,   16] loss 6.054297\n",
      "[40,   17] loss 6.243636\n",
      "[41,    1] loss 4.892457\n",
      "[41,    2] loss 5.926201\n",
      "[41,    3] loss 4.415909\n",
      "[41,    4] loss 5.487018\n",
      "[41,    5] loss 4.557688\n",
      "[41,    6] loss 5.989112\n",
      "[41,    7] loss 6.008581\n",
      "[41,    8] loss 5.729324\n",
      "[41,    9] loss 5.314916\n",
      "[41,   10] loss 5.704570\n",
      "[41,   11] loss 5.709727\n",
      "[41,   12] loss 5.077909\n",
      "[41,   13] loss 5.578302\n",
      "[41,   14] loss 5.312421\n",
      "[41,   15] loss 7.600875\n",
      "[41,   16] loss 5.047894\n",
      "[41,   17] loss 6.706929\n",
      "[42,    1] loss 5.842254\n",
      "[42,    2] loss 5.334108\n",
      "[42,    3] loss 5.721776\n",
      "[42,    4] loss 5.187110\n",
      "[42,    5] loss 5.377655\n",
      "[42,    6] loss 4.257520\n",
      "[42,    7] loss 5.981420\n",
      "[42,    8] loss 5.714698\n",
      "[42,    9] loss 6.462452\n",
      "[42,   10] loss 5.820955\n",
      "[42,   11] loss 4.080387\n",
      "[42,   12] loss 6.753253\n",
      "[42,   13] loss 5.214183\n",
      "[42,   14] loss 5.006493\n",
      "[42,   15] loss 5.489463\n",
      "[42,   16] loss 5.826433\n",
      "[42,   17] loss 6.761969\n",
      "[43,    1] loss 4.813975\n",
      "[43,    2] loss 5.013471\n",
      "[43,    3] loss 4.893215\n",
      "[43,    4] loss 5.346137\n",
      "[43,    5] loss 4.846659\n",
      "[43,    6] loss 5.274335\n",
      "[43,    7] loss 5.089607\n",
      "[43,    8] loss 5.671530\n",
      "[43,    9] loss 5.787998\n",
      "[43,   10] loss 5.842830\n",
      "[43,   11] loss 5.766238\n",
      "[43,   12] loss 5.491621\n",
      "[43,   13] loss 5.446893\n",
      "[43,   14] loss 5.169510\n",
      "[43,   15] loss 5.745163\n",
      "[43,   16] loss 5.365683\n",
      "[43,   17] loss 5.386924\n",
      "[44,    1] loss 5.724900\n",
      "[44,    2] loss 4.851356\n",
      "[44,    3] loss 5.864957\n",
      "[44,    4] loss 5.737618\n",
      "[44,    5] loss 5.115034\n",
      "[44,    6] loss 5.084995\n",
      "[44,    7] loss 5.995651\n",
      "[44,    8] loss 4.652473\n",
      "[44,    9] loss 4.993730\n",
      "[44,   10] loss 6.017574\n",
      "[44,   11] loss 4.994189\n",
      "[44,   12] loss 5.952026\n",
      "[44,   13] loss 4.745238\n",
      "[44,   14] loss 4.776134\n",
      "[44,   15] loss 6.191358\n",
      "[44,   16] loss 6.557099\n",
      "[44,   17] loss 4.830837\n",
      "[45,    1] loss 5.029878\n",
      "[45,    2] loss 5.440398\n",
      "[45,    3] loss 6.363034\n",
      "[45,    4] loss 4.897808\n",
      "[45,    5] loss 4.571909\n",
      "[45,    6] loss 4.590671\n",
      "[45,    7] loss 5.455410\n",
      "[45,    8] loss 4.593361\n",
      "[45,    9] loss 5.620229\n",
      "[45,   10] loss 4.910655\n",
      "[45,   11] loss 5.257123\n",
      "[45,   12] loss 5.195121\n",
      "[45,   13] loss 4.861829\n",
      "[45,   14] loss 5.601523\n",
      "[45,   15] loss 5.359940\n",
      "[45,   16] loss 6.183576\n",
      "[45,   17] loss 5.983198\n",
      "[46,    1] loss 5.716061\n",
      "[46,    2] loss 5.519800\n",
      "[46,    3] loss 6.008843\n",
      "[46,    4] loss 5.236331\n",
      "[46,    5] loss 4.997175\n",
      "[46,    6] loss 4.673394\n",
      "[46,    7] loss 5.832337\n",
      "[46,    8] loss 6.215757\n",
      "[46,    9] loss 5.571403\n",
      "[46,   10] loss 5.924004\n",
      "[46,   11] loss 6.086271\n",
      "[46,   12] loss 4.548154\n",
      "[46,   13] loss 5.293955\n",
      "[46,   14] loss 4.969140\n",
      "[46,   15] loss 5.278559\n",
      "[46,   16] loss 4.983001\n",
      "[46,   17] loss 7.950733\n",
      "[47,    1] loss 5.875929\n",
      "[47,    2] loss 4.650474\n",
      "[47,    3] loss 5.184231\n",
      "[47,    4] loss 5.241805\n",
      "[47,    5] loss 4.791012\n",
      "[47,    6] loss 5.547691\n",
      "[47,    7] loss 5.449543\n",
      "[47,    8] loss 5.092839\n",
      "[47,    9] loss 6.193040\n",
      "[47,   10] loss 5.495536\n",
      "[47,   11] loss 5.406244\n",
      "[47,   12] loss 5.978287\n",
      "[47,   13] loss 4.844770\n",
      "[47,   14] loss 5.620320\n",
      "[47,   15] loss 4.822773\n",
      "[47,   16] loss 4.430885\n",
      "[47,   17] loss 6.161531\n",
      "[48,    1] loss 4.849953\n",
      "[48,    2] loss 4.894402\n",
      "[48,    3] loss 4.926756\n",
      "[48,    4] loss 6.345323\n",
      "[48,    5] loss 5.174300\n",
      "[48,    6] loss 4.485367\n",
      "[48,    7] loss 4.976850\n",
      "[48,    8] loss 5.188462\n",
      "[48,    9] loss 5.545804\n",
      "[48,   10] loss 5.529047\n",
      "[48,   11] loss 4.452601\n",
      "[48,   12] loss 4.291357\n",
      "[48,   13] loss 4.524018\n",
      "[48,   14] loss 5.096407\n",
      "[48,   15] loss 4.900999\n",
      "[48,   16] loss 4.728048\n",
      "[48,   17] loss 8.318590\n",
      "[49,    1] loss 4.734231\n",
      "[49,    2] loss 5.075495\n",
      "[49,    3] loss 5.329794\n",
      "[49,    4] loss 4.570732\n",
      "[49,    5] loss 5.530552\n",
      "[49,    6] loss 5.919837\n",
      "[49,    7] loss 6.459316\n",
      "[49,    8] loss 5.363760\n",
      "[49,    9] loss 4.670800\n",
      "[49,   10] loss 5.695510\n",
      "[49,   11] loss 5.131805\n",
      "[49,   12] loss 5.566001\n",
      "[49,   13] loss 5.407064\n",
      "[49,   14] loss 5.907213\n",
      "[49,   15] loss 5.131329\n",
      "[49,   16] loss 5.523473\n",
      "[49,   17] loss 5.372028\n",
      "[50,    1] loss 4.823411\n",
      "[50,    2] loss 5.463656\n",
      "[50,    3] loss 5.479339\n",
      "[50,    4] loss 5.422043\n",
      "[50,    5] loss 5.292351\n",
      "[50,    6] loss 4.605100\n",
      "[50,    7] loss 5.986942\n",
      "[50,    8] loss 5.267896\n",
      "[50,    9] loss 4.563471\n",
      "[50,   10] loss 5.171001\n",
      "[50,   11] loss 5.012115\n",
      "[50,   12] loss 4.135700\n",
      "[50,   13] loss 5.669729\n",
      "[50,   14] loss 5.996699\n",
      "[50,   15] loss 4.842852\n",
      "[50,   16] loss 4.668538\n",
      "[50,   17] loss 4.679582\n",
      "finish training\n"
     ]
    }
   ],
   "source": [
    "dloador=DataLoader(dataloader,batch_size=100,shuffle=True,num_workers=8,pin_memory=CUDA)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion=nn.L1Loss()\n",
    "Net=model.double()\n",
    "for epoch in np.arange(50):\n",
    "    numepoch+=1\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(dloador,0):\n",
    "        \n",
    "        iteration+=1\n",
    "#         ipdb.set_trace()\n",
    "        sample,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        sample.to(device)\n",
    "        labels.to(device)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "\n",
    "#         print(labels)\n",
    "        optimizer.zero_grad()\n",
    "#         sample=sample.double()\n",
    "        outputs=Net(sample.double())\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "#         if i%10==9:\n",
    "        print('[%d,%5d] loss %f' %(epoch+1,i+1,running_loss))\n",
    "        running_loss=0.0\n",
    "print('finish training')\n",
    "t.save(Net.state_dict(),'/home/wcj/ReferenceProject/PPGnet/checkpoints/0501Net-epoch-%d-iteration%d.pth'%(numepoch,iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1622个样本计算MAE: 8.453950\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 模型实时测试 在12组数据(训练)上\n",
    "###############################\n",
    "testfilepath='/home/wcj/ReferenceProject/PPGnet/ppghr'\n",
    "Testdataloader=PPGData(testfilepath)\n",
    "testloader=DataLoader(\n",
    "Testdataloader,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# for i in np.arange(7):\n",
    "sub=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        samples,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "        outputs=model(samples)\n",
    "    #         _,predicted=t.max(outputs,1)# 在矩阵的第一个维度上\n",
    "        total+=labels.size(0)\n",
    "#         ipdb.set_trace()\n",
    "        sub+=abs(outputs-labels).sum()\n",
    "    #         print('++++')\n",
    "    print(' %d个样本计算MAE: %.6f'%(total,sub/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1328个样本计算MAE: 28.039368\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 模型实时测试 在10组数据上\n",
    "###############################\n",
    "\n",
    "testfilepath='/home/wcj/ReferenceProject/PPGnet/testdata'\n",
    "Testdataloader=PPGData(testfilepath)\n",
    "testloader=DataLoader(\n",
    "Testdataloader,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# for i in np.arange(7):\n",
    "sub=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        samples,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "        outputs=model(samples)\n",
    "    #         _,predicted=t.max(outputs,1)# 在矩阵的第一个维度上\n",
    "        total+=labels.size(0)\n",
    "#         ipdb.set_trace()\n",
    "        sub+=abs(outputs-labels).sum()\n",
    "    #         print('++++')\n",
    "    print(' %d个样本计算MAE: %.6f'%(total,sub/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 146个样本计算MAE: 10.356282\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# leave one\n",
    "########################\n",
    "\n",
    "# Net=PPGNet(kernels,125,125,125).eval()\n",
    "# Net.double()\n",
    "# Net.load_state_dict(t.load('/home/wcj/ReferenceProject/PPGnet/checkpoints/0430Net_epoch200-iteration32600.pth'))\n",
    "\n",
    "testfilepath='/home/wcj/ReferenceProject/PPGnet/12thData'\n",
    "Testdataloader=PPGData(testfilepath)\n",
    "testloader=DataLoader(\n",
    "Testdataloader,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# for i in np.arange(7):\n",
    "sub=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        samples,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "        outputs=model(samples)\n",
    "    #         _,predicted=t.max(outputs,1)# 在矩阵的第一个维度上\n",
    "        total+=labels.size(0)\n",
    "#         ipdb.set_trace()\n",
    "        sub+=abs(outputs-labels).sum()\n",
    "    #         print('++++')\n",
    "    print(' %d个样本计算MAE: %.6f'%(total,sub/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
