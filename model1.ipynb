{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import ipdb\n",
    "torch.manual_seed(1)\n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "# from IPython.core.debugger import Pdb\n",
    "# debug_here = Tracer()\n",
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "#     ipdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3])\n",
      "torch.Size([5, 1, 3])\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(inputs.shape)\n",
    "# print(hidden.shape)\n",
    "print(out.shape)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20,2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(125, 80,2)\n",
    "input = torch.randn(8, 1, 125)# 一个句子，8个单词 每个单词维度为125\n",
    "h0 = torch.randn(2, 1, 80)\n",
    "c0 = torch.randn(2, 1, 80)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(125, 80, num_layers=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 125])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 80])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 80])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 80])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "try:\n",
    "    import ipdb\n",
    "except:\n",
    "    import pdb as ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Conv2dBlock(nn.Module):\n",
    "        def __init__(self,ninp,fmaps,kwidth,stride):\n",
    "            super().__init__()\n",
    "            self.conv=nn.Conv2d(ninp,fmaps,kwidth,stride)\n",
    "            self.pool=nn.MaxPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "            self.act=nn.ELU()\n",
    "\n",
    "        def forward(self,x):\n",
    "            x=self.conv(x)\n",
    "            x=self.pool(x)\n",
    "            x=self.act(x)\n",
    "            return x\n",
    "\n",
    "    class Conv1dBlock(nn.Module):\n",
    "        def __init__(self,ninp,noutp,kernel_size,stride=None,padding=0,Handle=True):\n",
    "            super(Conv1dBlock,self).__init__()\n",
    "            self.conv=nn.Conv1d(ninp,noutp,kernel_size,stride=stride,padding=padding)\n",
    "            self.Handle=Handle\n",
    "            self.norm=nn.BatchNorm1d(noutp)\n",
    "            self.pool=nn.MaxPool1d(5,stride=1,padding=2)\n",
    "            self.act=nn.ReLU()\n",
    "            self.dropout=nn.Dropout()\n",
    "        def forward(self,x):\n",
    "            x=self.conv(x)\n",
    "            if self.Handle:\n",
    "                x=self.norm(x)\n",
    "                x=self.pool(x)\n",
    "                x=self.act(x)\n",
    "                x=self.dropout(x)\n",
    "            return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    网络结构定义\n",
    "    \"\"\"\n",
    "    '''\n",
    "    def __init__(self,ntr,nc):\n",
    "        super(PPGNet,self).__init__()\n",
    "#       ngf=opt.ngf # \n",
    "        self.ntr=ntr #每一段信号追踪的分片数\n",
    "        self.nc=nc #中间卷积层的个数\n",
    "        self.ConvBlocks=nn.ModuleList()\n",
    "        self.conv1=nn.Conv2d(4,8,1)\n",
    "        self.conv2=nn.Conv2d(8,16,(ntr,3))\n",
    "        self.act2=nn.MaxPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "        for i in np.arange(nc):\n",
    "            ConvBlock=Conv2dBlock(2**(i+4),2**(i+5),(1,3),(1,1))\n",
    "            self.ConvBlocks.append(ConvBlock)\n",
    "        self.conv3=nn.Conv2d(2**(nc+4),32,(1,1),(1,1))\n",
    "        self.act3=nn.ELU()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(64,512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512,1),\n",
    "            nn.ELU()\n",
    "        )\n",
    "    '''\n",
    "    def __init__(self,fmaps,input_dim1,input_dim2,hidden_dim):\n",
    "            super(PPGNet,self).__init__()\n",
    "\n",
    "            # fisrt CNN feature extractor\n",
    "            self.hidden_dim=hidden_dim\n",
    "            self.ParallelBlocks=nn.ModuleList()\n",
    "            for idx,kernel in enumerate(fmaps,start=1):\n",
    "                ConvBlock=Conv1dBlock(1,1,kernel,stride=2,Handle=False)\n",
    "                self.ParallelBlocks.append(ConvBlock)\n",
    "            # lstm feature extractor      \n",
    "            self.lstm1=nn.LSTM(input_dim1,hidden_dim,2) #   \n",
    "            self.lstm2=nn.LSTM(input_dim2,hidden_dim,2)# \n",
    "        \n",
    "           # middle sequential blocks \n",
    "            self.ConvBlocks=nn.ModuleList()\n",
    "            ConvBlock1=Conv1dBlock(1,32,40,stride=1)\n",
    "            self.ConvBlocks.append(ConvBlock1)\n",
    "#             for jdx in np.arange(8):\n",
    "            ConvBlock2=Conv1dBlock(32,47,50,stride=1)\n",
    "            self.ConvBlocks.append(ConvBlock2)\n",
    "        \n",
    "            # linear\n",
    "            self.linear=nn.Sequential(\n",
    "                nn.Linear(384*125,1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "                \n",
    "#             self.conv2=nn.Conv2d(8,16,(ntr,3))\n",
    "#             self.act2=nn.MaxPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "#             for i in np.arange(nc):\n",
    "#                 ConvBlock=Conv2dBlock(2**(i+4),2**(i+5),(1,3),(1,1))\n",
    "#                 self.ConvBlocks.append(ConvBlock)\n",
    "#             self.conv3=nn.Conv2d(2**(nc+4),32,(1,1),(1,1))\n",
    "#             self.act3=nn.ELU()\n",
    "#             self.fc=nn.Sequential(\n",
    "#                 nn.Linear(64,512),\n",
    "#                 nn.ELU(),\n",
    "#                 nn.Linear(512,1),\n",
    "#                 nn.ELU()\n",
    "#             )\n",
    "\n",
    "    def forward(self,x):\n",
    "#         channel=x.size(1)\n",
    "#         ipdb.set_trace()\n",
    "#         print('1:',x.shape)\n",
    "#         length=len(self.ParallelBlocks)\n",
    "        ParallelOut=[]\n",
    "        channelout=[]#每一个通道平行卷积后处理\n",
    "        serialout=[]\n",
    "        for chidx in np.arange(x.size(1)):\n",
    "            for i in np.arange(len(self.ParallelBlocks)):\n",
    "                Block=self.ParallelBlocks[i]\n",
    "                InitOut=Block(x[:,chidx,:].unsqueeze(1))\n",
    "                ParallelOut.append(InitOut)\n",
    "\n",
    "            channelout=torch.cat(ParallelOut,2)\n",
    "            ParallelOut=[]\n",
    "            # sequential block\n",
    "#             ipdb.set_trace()\n",
    "\n",
    "            for chidy in np.arange(len(self.ConvBlocks)):\n",
    "                SecondBlock=self.ConvBlocks[chidy]\n",
    "                if chidy == 0:\n",
    "                    SecondOut=SecondBlock(channelout)\n",
    "                else:\n",
    "                    SecondOut=SecondBlock(SecondOut)\n",
    "\n",
    "                if chidy+1==len(self.ConvBlocks):\n",
    "                    serialout.append(SecondOut)\n",
    "        # 5 channel concat\n",
    "    #   SecondIn=torch.cat(ParallelOut,1) # n*40*__\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "#         X=t.cat(ParallelOut,1)\n",
    "#         X=self.conv1(X)\n",
    "            \n",
    "#         ipdb.set_trace()\n",
    "#         for j in np.arange(len(self.ConvBlocks)):\n",
    "#             Block=self.ConvBlocks[j]\n",
    "#             X=Block(X)\n",
    "#         ipdb.set_trace()\n",
    "        hidden=(torch.zeros([2, x.size(0), self.hidden_dim],dtype=torch.double),\n",
    "                torch.zeros([2, x.size(0), self.hidden_dim],dtype=torch.double))\n",
    "        LstmOut,self.hidden=self.lstm1(x.view(x.size(1),-1,x.size(2)),hidden)\n",
    "        \n",
    "        concat1=torch.cat(serialout,1)\n",
    "        LstmOut1=LstmOut.view(LstmOut.size(1),-1,LstmOut.size(2))\n",
    "        concat=torch.cat((LstmOut1,concat1),1)\n",
    "        \n",
    "        hidden=(torch.zeros([2, concat.size(0), self.hidden_dim],dtype=torch.double),\n",
    "                torch.zeros([2, concat.size(0), self.hidden_dim],dtype=torch.double))\n",
    "        LstmOutp,self.hidden=self.lstm1(concat.view(concat.size(1),-1,concat.size(2)),hidden)\n",
    "#         ipdb.set_trace()\n",
    "        out=self.linear(LstmOutp.view(LstmOutp.size(1),-1))\n",
    "        return out\n",
    "#         x1=self.conv1(x)\n",
    "#         print('2:',x.shape)\n",
    "#         x=self.conv2(x)\n",
    "#         print('3:',x.shape)\n",
    "#         x=self.act2(x)\n",
    "#         print('4:',x.shape)\n",
    "#         pdb.set_trace()\n",
    "#         for i in np.arange(self.nc):\n",
    "#             ConvBlock=self.ConvBlocks[i]\n",
    "#             x=ConvBlock(x)\n",
    "#             print('{}:'.format(i+5),x.shape)\n",
    "#         pdb.set_trace()\n",
    "#         x=self.conv3(x)\n",
    "#         x=self.act3(x)\n",
    "#         x=x.view(x.size(0),1,-1)\n",
    "#         out=self.fc(x)\n",
    "#         return out.squeeze()\n",
    "    \n",
    "    def  get_n_params(self):\n",
    "        pp=0\n",
    "        for p in list(self.parameters()):\n",
    "            nn=1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn*s\n",
    "            pp += nn\n",
    "        return pp  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2dBlock') != -1:\n",
    "        print('Initializing weights of convresblock to 0.0, 0.02')\n",
    "        for k, p in m.named_parameters():\n",
    "            if 'weight' in k and 'conv' in k:\n",
    "                p.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('Conv2d') != -1:\n",
    "        print('Initialzing weight to 0.0, 0.02 for module: ', m)\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            print('bias to 0 for module: ', m)\n",
    "            m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        print('Initializing FC weight to xavier uniform')\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "def build_norm_layer(norm_type, param=None, num_feats=None):\n",
    "    if norm_type == 'bnorm':\n",
    "        return nn.BatchNorm2d(num_feats)\n",
    "    elif norm_type == 'snorm':\n",
    "        spectral_norm(param)\n",
    "        return None\n",
    "    elif norm_type is None:\n",
    "        return None\n",
    "    else:\n",
    "        raise TypeError('Unrecognized norm type: ', norm_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters:  628938\n"
     ]
    }
   ],
   "source": [
    "kernels=[5,20,40,60,80]\n",
    "model=PPGNet(kernels,125,125,125)\n",
    "\n",
    "print('Total model parameters: ',model.get_n_params())\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from torch.utils import data\n",
    "    import os\n",
    "    import scipy.io as sio\n",
    "    class PPGData(data.Dataset):\n",
    "        def __init__(self,root):\n",
    "            datas=os.listdir(root)\n",
    "            self.totaldata=[os.path.join(root,data) for data in datas]\n",
    "        def __getitem__(self,index):\n",
    "    #       timedata=np.zeros((4,7,1025))\n",
    "#             sipdb.set_trace()\n",
    "            ppgpath=self.totaldata[index]\n",
    "            time=sio.loadmat(ppgpath)\n",
    "            time=time['ppg']\n",
    "            time=time.reshape(8,125)\n",
    "            listslice=ppgpath.split('-')\n",
    "            label=listslice[2][:-4]\n",
    "            return time,label\n",
    "        def __len__(self):\n",
    "            return len(self.totaldata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda True\n",
      "1622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "filepath='/home/wcj/ReferenceProject/PPGnet/ppghrnormalization'\n",
    "dataloader=PPGData(filepath)\n",
    "# pdb.set_trace()\n",
    "device = 'cpu'\n",
    "if t.cuda.is_available:\n",
    "#     device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "    device = 'cuda'\n",
    "CUDA = (device == 'cuda')\n",
    "print(device,CUDA)\n",
    "# pdb.set_trace()\n",
    "dloador=DataLoader(dataloader,batch_size=10,shuffle=True,num_workers=8,pin_memory=CUDA)\n",
    "\n",
    "print(len(dataloader))\n",
    "dataiter=iter(dloador)\n",
    "sample,label=dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    1] loss 134.092827\n",
      "[1,    2] loss 129.274290\n",
      "[1,    3] loss 128.791579\n",
      "[1,    4] loss 118.235663\n",
      "[1,    5] loss 105.868375\n",
      "[1,    6] loss 74.809943\n",
      "[1,    7] loss 43.395651\n",
      "[1,    8] loss 19.855660\n",
      "[1,    9] loss 45.413553\n",
      "[1,   10] loss 53.809704\n",
      "[1,   11] loss 45.876168\n",
      "[1,   12] loss 23.412966\n",
      "[1,   13] loss 17.809565\n",
      "[1,   14] loss 27.122727\n",
      "[1,   15] loss 29.639796\n",
      "[1,   16] loss 32.432791\n",
      "[1,   17] loss 32.789542\n",
      "[2,    1] loss 23.644853\n",
      "[2,    2] loss 22.780210\n",
      "[2,    3] loss 21.503678\n",
      "[2,    4] loss 19.429078\n",
      "[2,    5] loss 25.493596\n",
      "[2,    6] loss 23.965543\n",
      "[2,    7] loss 25.221651\n",
      "[2,    8] loss 18.932323\n",
      "[2,    9] loss 20.856776\n",
      "[2,   10] loss 19.119598\n",
      "[2,   11] loss 18.913142\n",
      "[2,   12] loss 21.213000\n",
      "[2,   13] loss 20.959008\n",
      "[2,   14] loss 19.532109\n",
      "[2,   15] loss 19.702069\n",
      "[2,   16] loss 19.569937\n",
      "[2,   17] loss 21.859047\n",
      "[3,    1] loss 20.085658\n",
      "[3,    2] loss 15.880426\n",
      "[3,    3] loss 23.686701\n",
      "[3,    4] loss 19.109917\n",
      "[3,    5] loss 18.507562\n",
      "[3,    6] loss 18.410151\n",
      "[3,    7] loss 18.226864\n",
      "[3,    8] loss 18.331360\n",
      "[3,    9] loss 17.578490\n",
      "[3,   10] loss 19.792839\n",
      "[3,   11] loss 19.020653\n",
      "[3,   12] loss 18.014832\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e87f77b0118b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mrunning_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Net.apply(weights_init)\n",
    "##################\n",
    "#  lr=1e-3 afternormalization\n",
    "##################\n",
    "epochnum=0\n",
    "iteration=0\n",
    "dloador=DataLoader(dataloader,batch_size=100,shuffle=True,num_workers=8,pin_memory=CUDA)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion=nn.L1Loss()\n",
    "Net=model.double()\n",
    "for epoch in np.arange(200):\n",
    "    epochnum+=1\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(dloador,0):\n",
    "#         ipdb.set_trace()\n",
    "        iteration+=1\n",
    "        sample,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        sample.to(device)\n",
    "        labels.to(device)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "\n",
    "#         print(labels)\n",
    "        optimizer.zero_grad()\n",
    "#         sample=sample.double()\n",
    "        outputs=Net(sample.double())\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "#         if i%10==9:\n",
    "        print('[%d,%5d] loss %f' %(epoch+1,i+1,running_loss))\n",
    "        running_loss=0.0\n",
    "print('finish training')\n",
    "t.save(Net.state_dict(),'/home/wcj/ReferenceProject/PPGnet/checkpoints/0501Net_epoch%d-iteration%d.pth'%(epochnum,iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    1] loss 16.211209\n",
      "[1,    2] loss 18.107575\n",
      "[1,    3] loss 18.357925\n",
      "[1,    4] loss 18.324435\n",
      "[1,    5] loss 16.720637\n",
      "[1,    6] loss 18.338521\n",
      "[1,    7] loss 19.285154\n",
      "[1,    8] loss 17.622786\n",
      "[1,    9] loss 18.366259\n",
      "[1,   10] loss 18.893629\n",
      "[1,   11] loss 19.094107\n",
      "[1,   12] loss 19.132169\n",
      "[1,   13] loss 18.367290\n",
      "[1,   14] loss 20.716530\n",
      "[1,   15] loss 19.003687\n",
      "[1,   16] loss 17.385453\n",
      "[1,   17] loss 14.196033\n",
      "[2,    1] loss 18.235874\n",
      "[2,    2] loss 16.567494\n",
      "[2,    3] loss 16.401013\n",
      "[2,    4] loss 17.091832\n",
      "[2,    5] loss 19.506608\n",
      "[2,    6] loss 20.021504\n",
      "[2,    7] loss 17.635600\n",
      "[2,    8] loss 17.978462\n",
      "[2,    9] loss 18.155143\n",
      "[2,   10] loss 14.680262\n",
      "[2,   11] loss 16.439854\n",
      "[2,   12] loss 17.612507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/wcj/anaconda2/envs/pytorch/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/wcj/anaconda2/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/wcj/anaconda2/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/wcj/anaconda2/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-150e5454ea38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mrunning_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# weights_init(model)\n",
    "##################\n",
    "#  lr=1e-4 afternormalization\n",
    "##################\n",
    "# epochnum=0\n",
    "# iteration=0\n",
    "dloador=DataLoader(dataloader,batch_size=100,shuffle=True,num_workers=8,pin_memory=CUDA)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-4)\n",
    "criterion=nn.L1Loss()\n",
    "Net=model.double()\n",
    "for epoch in np.arange(200):\n",
    "    epochnum+=1\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(dloador,0):\n",
    "#         ipdb.set_trace()\n",
    "        iteration+=1\n",
    "        sample,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        sample.to(device)\n",
    "        labels.to(device)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "\n",
    "#         print(labels)\n",
    "        optimizer.zero_grad()\n",
    "#         sample=sample.double()\n",
    "        outputs=Net(sample.double())\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "#         if i%10==9:\n",
    "        print('[%d,%5d] loss %f' %(epoch+1,i+1,running_loss))\n",
    "        running_loss=0.0\n",
    "print('finish training')\n",
    "t.save(Net.state_dict(),'/home/wcj/ReferenceProject/PPGnet/checkpoints/0501Net_epoch%d-iteration%d.pth'%(epochnum,iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    1] loss 17.891596\n",
      "[1,    2] loss 17.157533\n",
      "[1,    3] loss 17.042480\n",
      "[1,    4] loss 16.967996\n",
      "[1,    5] loss 15.609225\n",
      "[1,    6] loss 18.509522\n",
      "[1,    7] loss 15.846373\n",
      "[1,    8] loss 14.574996\n",
      "[1,    9] loss 18.512734\n",
      "[1,   10] loss 17.848633\n",
      "[1,   11] loss 16.317942\n",
      "[1,   12] loss 17.116231\n",
      "[1,   13] loss 15.721198\n",
      "[1,   14] loss 16.113704\n",
      "[1,   15] loss 14.936858\n",
      "[1,   16] loss 15.386524\n",
      "[1,   17] loss 15.900111\n",
      "[2,    1] loss 16.811064\n",
      "[2,    2] loss 15.158420\n",
      "[2,    3] loss 17.331233\n",
      "[2,    4] loss 17.252523\n",
      "[2,    5] loss 17.641889\n",
      "[2,    6] loss 14.640988\n",
      "[2,    7] loss 15.988531\n",
      "[2,    8] loss 18.585344\n",
      "[2,    9] loss 15.400597\n",
      "[2,   10] loss 17.280231\n",
      "[2,   11] loss 15.688201\n",
      "[2,   12] loss 15.065075\n",
      "[2,   13] loss 14.124302\n",
      "[2,   14] loss 17.115859\n",
      "[2,   15] loss 17.738061\n",
      "[2,   16] loss 17.447287\n",
      "[2,   17] loss 17.219784\n",
      "[3,    1] loss 14.572923\n",
      "[3,    2] loss 18.592610\n",
      "[3,    3] loss 15.993784\n",
      "[3,    4] loss 16.348481\n",
      "[3,    5] loss 16.737167\n",
      "[3,    6] loss 18.471870\n",
      "[3,    7] loss 17.092356\n",
      "[3,    8] loss 16.013626\n",
      "[3,    9] loss 14.857914\n",
      "[3,   10] loss 16.428715\n",
      "[3,   11] loss 14.702547\n",
      "[3,   12] loss 16.568897\n",
      "[3,   13] loss 16.885620\n",
      "[3,   14] loss 17.621498\n",
      "[3,   15] loss 15.786415\n",
      "[3,   16] loss 16.920903\n",
      "[3,   17] loss 15.656870\n",
      "[4,    1] loss 18.095106\n",
      "[4,    2] loss 17.196116\n",
      "[4,    3] loss 20.692232\n",
      "[4,    4] loss 18.426350\n",
      "[4,    5] loss 15.653432\n",
      "[4,    6] loss 15.006227\n",
      "[4,    7] loss 16.271623\n",
      "[4,    8] loss 15.634704\n",
      "[4,    9] loss 16.252814\n",
      "[4,   10] loss 17.983005\n",
      "[4,   11] loss 15.938116\n",
      "[4,   12] loss 13.996554\n",
      "[4,   13] loss 17.159553\n",
      "[4,   14] loss 14.424224\n",
      "[4,   15] loss 15.281985\n",
      "[4,   16] loss 17.327458\n",
      "[4,   17] loss 12.035644\n",
      "[5,    1] loss 16.696382\n",
      "[5,    2] loss 19.393045\n",
      "[5,    3] loss 14.655487\n",
      "[5,    4] loss 16.618322\n",
      "[5,    5] loss 14.964995\n",
      "[5,    6] loss 15.395484\n",
      "[5,    7] loss 12.677004\n",
      "[5,    8] loss 18.333544\n",
      "[5,    9] loss 14.990588\n",
      "[5,   10] loss 19.329668\n",
      "[5,   11] loss 19.234855\n",
      "[5,   12] loss 15.645897\n",
      "[5,   13] loss 14.124649\n",
      "[5,   14] loss 16.745213\n",
      "[5,   15] loss 17.428926\n",
      "[5,   16] loss 16.771636\n",
      "[5,   17] loss 11.121418\n",
      "[6,    1] loss 17.046684\n",
      "[6,    2] loss 14.612589\n",
      "[6,    3] loss 18.455631\n",
      "[6,    4] loss 16.682610\n",
      "[6,    5] loss 16.361428\n",
      "[6,    6] loss 18.250672\n",
      "[6,    7] loss 15.804505\n",
      "[6,    8] loss 16.376314\n",
      "[6,    9] loss 14.183508\n",
      "[6,   10] loss 15.261674\n",
      "[6,   11] loss 14.901518\n",
      "[6,   12] loss 15.561775\n",
      "[6,   13] loss 15.046337\n",
      "[6,   14] loss 17.762359\n",
      "[6,   15] loss 15.968251\n",
      "[6,   16] loss 17.104056\n",
      "[6,   17] loss 15.898126\n",
      "[7,    1] loss 17.803504\n",
      "[7,    2] loss 14.569745\n",
      "[7,    3] loss 16.553059\n",
      "[7,    4] loss 17.348394\n",
      "[7,    5] loss 15.812421\n",
      "[7,    6] loss 15.818203\n",
      "[7,    7] loss 14.571856\n",
      "[7,    8] loss 14.693322\n",
      "[7,    9] loss 17.136878\n",
      "[7,   10] loss 15.826155\n",
      "[7,   11] loss 14.543935\n",
      "[7,   12] loss 15.020086\n",
      "[7,   13] loss 17.195612\n",
      "[7,   14] loss 16.880635\n",
      "[7,   15] loss 17.523654\n",
      "[7,   16] loss 17.149371\n",
      "[7,   17] loss 18.286856\n",
      "[8,    1] loss 15.330985\n",
      "[8,    2] loss 20.798775\n",
      "[8,    3] loss 16.697167\n",
      "[8,    4] loss 13.678470\n",
      "[8,    5] loss 18.022174\n",
      "[8,    6] loss 15.527151\n",
      "[8,    7] loss 15.380974\n",
      "[8,    8] loss 16.562025\n",
      "[8,    9] loss 17.152832\n",
      "[8,   10] loss 13.966853\n",
      "[8,   11] loss 15.157464\n",
      "[8,   12] loss 16.948233\n",
      "[8,   13] loss 16.436856\n",
      "[8,   14] loss 15.184056\n",
      "[8,   15] loss 16.298103\n",
      "[8,   16] loss 14.658433\n",
      "[8,   17] loss 17.258744\n",
      "[9,    1] loss 15.454145\n",
      "[9,    2] loss 17.418296\n",
      "[9,    3] loss 15.953064\n",
      "[9,    4] loss 15.508496\n",
      "[9,    5] loss 13.960621\n",
      "[9,    6] loss 18.996912\n",
      "[9,    7] loss 16.295298\n",
      "[9,    8] loss 13.836843\n",
      "[9,    9] loss 17.863310\n",
      "[9,   10] loss 15.745524\n",
      "[9,   11] loss 17.175447\n",
      "[9,   12] loss 15.157924\n",
      "[9,   13] loss 14.646380\n",
      "[9,   14] loss 15.434397\n",
      "[9,   15] loss 15.274065\n",
      "[9,   16] loss 16.516528\n",
      "[9,   17] loss 13.603051\n",
      "[10,    1] loss 17.650510\n",
      "[10,    2] loss 17.326883\n",
      "[10,    3] loss 12.801620\n",
      "[10,    4] loss 18.519501\n",
      "[10,    5] loss 16.934289\n",
      "[10,    6] loss 15.520850\n",
      "[10,    7] loss 15.554126\n",
      "[10,    8] loss 16.130312\n",
      "[10,    9] loss 15.902677\n",
      "[10,   10] loss 15.236129\n",
      "[10,   11] loss 15.284055\n",
      "[10,   12] loss 16.282280\n",
      "[10,   13] loss 15.328093\n",
      "[10,   14] loss 16.231769\n",
      "[10,   15] loss 15.886031\n",
      "[10,   16] loss 17.372684\n",
      "[10,   17] loss 12.645443\n",
      "[11,    1] loss 14.812174\n",
      "[11,    2] loss 14.804184\n",
      "[11,    3] loss 15.165302\n",
      "[11,    4] loss 15.194906\n",
      "[11,    5] loss 17.971057\n",
      "[11,    6] loss 16.213156\n",
      "[11,    7] loss 17.293611\n",
      "[11,    8] loss 17.475177\n",
      "[11,    9] loss 14.261896\n",
      "[11,   10] loss 15.986413\n",
      "[11,   11] loss 15.831881\n",
      "[11,   12] loss 13.806786\n",
      "[11,   13] loss 16.673347\n",
      "[11,   14] loss 15.401611\n",
      "[11,   15] loss 16.169789\n",
      "[11,   16] loss 17.902286\n",
      "[11,   17] loss 10.691272\n",
      "[12,    1] loss 17.613493\n",
      "[12,    2] loss 13.884694\n",
      "[12,    3] loss 17.670158\n",
      "[12,    4] loss 15.872510\n",
      "[12,    5] loss 15.395536\n",
      "[12,    6] loss 15.931473\n",
      "[12,    7] loss 14.336533\n",
      "[12,    8] loss 14.586036\n",
      "[12,    9] loss 15.553287\n",
      "[12,   10] loss 14.362410\n",
      "[12,   11] loss 17.535340\n",
      "[12,   12] loss 15.075630\n",
      "[12,   13] loss 15.363915\n",
      "[12,   14] loss 14.684604\n",
      "[12,   15] loss 15.287759\n",
      "[12,   16] loss 17.198565\n",
      "[12,   17] loss 10.835978\n",
      "[13,    1] loss 15.919717\n",
      "[13,    2] loss 14.039746\n",
      "[13,    3] loss 15.451313\n",
      "[13,    4] loss 14.600251\n",
      "[13,    5] loss 16.593555\n",
      "[13,    6] loss 16.768246\n",
      "[13,    7] loss 14.314457\n",
      "[13,    8] loss 14.358394\n",
      "[13,    9] loss 12.574846\n",
      "[13,   10] loss 15.738252\n",
      "[13,   11] loss 17.245336\n",
      "[13,   12] loss 15.015083\n",
      "[13,   13] loss 14.431706\n",
      "[13,   14] loss 17.664401\n",
      "[13,   15] loss 19.829451\n",
      "[13,   16] loss 16.652498\n",
      "[13,   17] loss 15.863002\n",
      "[14,    1] loss 14.927150\n",
      "[14,    2] loss 12.819152\n",
      "[14,    3] loss 17.143835\n",
      "[14,    4] loss 14.890529\n",
      "[14,    5] loss 15.234908\n",
      "[14,    6] loss 15.203111\n",
      "[14,    7] loss 14.173539\n",
      "[14,    8] loss 13.188173\n",
      "[14,    9] loss 17.609311\n",
      "[14,   10] loss 16.403494\n",
      "[14,   11] loss 15.427531\n",
      "[14,   12] loss 19.087579\n",
      "[14,   13] loss 17.044785\n",
      "[14,   14] loss 15.306830\n",
      "[14,   15] loss 14.188698\n",
      "[14,   16] loss 17.347624\n",
      "[14,   17] loss 16.217266\n",
      "[15,    1] loss 17.251486\n",
      "[15,    2] loss 14.961644\n",
      "[15,    3] loss 17.891865\n",
      "[15,    4] loss 15.303112\n",
      "[15,    5] loss 14.590599\n",
      "[15,    6] loss 15.430004\n",
      "[15,    7] loss 15.724595\n",
      "[15,    8] loss 14.744270\n",
      "[15,    9] loss 12.966178\n",
      "[15,   10] loss 12.590394\n",
      "[15,   11] loss 14.996719\n",
      "[15,   12] loss 14.452866\n",
      "[15,   13] loss 16.736819\n",
      "[15,   14] loss 15.315031\n",
      "[15,   15] loss 16.744741\n",
      "[15,   16] loss 15.227376\n",
      "[15,   17] loss 14.201657\n",
      "[16,    1] loss 15.229708\n",
      "[16,    2] loss 13.658966\n",
      "[16,    3] loss 14.945298\n",
      "[16,    4] loss 16.814089\n",
      "[16,    5] loss 15.981715\n",
      "[16,    6] loss 19.494769\n",
      "[16,    7] loss 15.512036\n",
      "[16,    8] loss 14.576515\n",
      "[16,    9] loss 17.073483\n",
      "[16,   10] loss 13.396637\n",
      "[16,   11] loss 14.355889\n",
      "[16,   12] loss 18.815356\n",
      "[16,   13] loss 14.982479\n",
      "[16,   14] loss 14.189112\n",
      "[16,   15] loss 18.368994\n",
      "[16,   16] loss 13.875250\n",
      "[16,   17] loss 12.822700\n",
      "[17,    1] loss 15.505053\n",
      "[17,    2] loss 14.994974\n",
      "[17,    3] loss 14.167856\n",
      "[17,    4] loss 14.540589\n",
      "[17,    5] loss 13.820488\n",
      "[17,    6] loss 16.466437\n",
      "[17,    7] loss 16.638816\n",
      "[17,    8] loss 15.006960\n",
      "[17,    9] loss 14.665212\n",
      "[17,   10] loss 14.256556\n",
      "[17,   11] loss 14.887702\n",
      "[17,   12] loss 15.064360\n",
      "[17,   13] loss 15.686695\n",
      "[17,   14] loss 14.206088\n",
      "[17,   15] loss 16.193777\n",
      "[17,   16] loss 18.445198\n",
      "[17,   17] loss 20.545993\n",
      "[18,    1] loss 14.737175\n",
      "[18,    2] loss 13.615627\n",
      "[18,    3] loss 16.480534\n",
      "[18,    4] loss 17.781939\n",
      "[18,    5] loss 14.818708\n",
      "[18,    6] loss 14.827626\n",
      "[18,    7] loss 14.741983\n",
      "[18,    8] loss 15.650026\n",
      "[18,    9] loss 18.593012\n",
      "[18,   10] loss 15.357369\n",
      "[18,   11] loss 14.295308\n",
      "[18,   12] loss 16.129533\n",
      "[18,   13] loss 13.207674\n",
      "[18,   14] loss 13.757687\n",
      "[18,   15] loss 16.273495\n",
      "[18,   16] loss 15.876799\n",
      "[18,   17] loss 14.082308\n",
      "[19,    1] loss 16.050558\n",
      "[19,    2] loss 17.001881\n",
      "[19,    3] loss 13.832478\n",
      "[19,    4] loss 14.041182\n",
      "[19,    5] loss 13.923394\n",
      "[19,    6] loss 15.775714\n",
      "[19,    7] loss 14.248884\n",
      "[19,    8] loss 13.759323\n",
      "[19,    9] loss 15.776981\n",
      "[19,   10] loss 15.391880\n",
      "[19,   11] loss 17.442082\n",
      "[19,   12] loss 14.690821\n",
      "[19,   13] loss 16.647867\n",
      "[19,   14] loss 14.604379\n",
      "[19,   15] loss 15.626489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19,   16] loss 14.364961\n",
      "[19,   17] loss 13.147450\n",
      "[20,    1] loss 15.801803\n",
      "[20,    2] loss 14.173079\n",
      "[20,    3] loss 13.975084\n",
      "[20,    4] loss 17.965181\n",
      "[20,    5] loss 12.425075\n",
      "[20,    6] loss 14.626095\n",
      "[20,    7] loss 17.265002\n",
      "[20,    8] loss 14.614478\n",
      "[20,    9] loss 16.299786\n",
      "[20,   10] loss 14.949564\n",
      "[20,   11] loss 15.338228\n",
      "[20,   12] loss 13.548396\n",
      "[20,   13] loss 14.594032\n",
      "[20,   14] loss 15.990938\n",
      "[20,   15] loss 14.911098\n",
      "[20,   16] loss 16.287061\n",
      "[20,   17] loss 13.060734\n",
      "[21,    1] loss 15.170767\n",
      "[21,    2] loss 14.147226\n",
      "[21,    3] loss 14.180678\n",
      "[21,    4] loss 17.187362\n",
      "[21,    5] loss 14.445876\n",
      "[21,    6] loss 12.994238\n",
      "[21,    7] loss 16.651205\n",
      "[21,    8] loss 13.709904\n",
      "[21,    9] loss 14.270817\n",
      "[21,   10] loss 15.834649\n",
      "[21,   11] loss 17.457604\n",
      "[21,   12] loss 17.333244\n",
      "[21,   13] loss 14.061455\n",
      "[21,   14] loss 15.251537\n",
      "[21,   15] loss 14.524360\n",
      "[21,   16] loss 14.044674\n",
      "[21,   17] loss 15.970793\n",
      "[22,    1] loss 15.240684\n",
      "[22,    2] loss 15.089419\n",
      "[22,    3] loss 15.788117\n",
      "[22,    4] loss 15.712355\n",
      "[22,    5] loss 14.856597\n",
      "[22,    6] loss 14.627075\n",
      "[22,    7] loss 15.915754\n",
      "[22,    8] loss 14.922982\n",
      "[22,    9] loss 15.639146\n",
      "[22,   10] loss 14.560442\n",
      "[22,   11] loss 15.276436\n",
      "[22,   12] loss 14.833079\n",
      "[22,   13] loss 15.250121\n",
      "[22,   14] loss 13.819758\n",
      "[22,   15] loss 16.475356\n",
      "[22,   16] loss 14.210343\n",
      "[22,   17] loss 16.589017\n",
      "[23,    1] loss 14.469656\n",
      "[23,    2] loss 15.414349\n",
      "[23,    3] loss 12.052741\n",
      "[23,    4] loss 13.778509\n",
      "[23,    5] loss 14.841787\n",
      "[23,    6] loss 18.078580\n",
      "[23,    7] loss 15.566893\n",
      "[23,    8] loss 14.606698\n",
      "[23,    9] loss 17.174189\n",
      "[23,   10] loss 12.466284\n",
      "[23,   11] loss 16.171859\n",
      "[23,   12] loss 16.354681\n",
      "[23,   13] loss 15.281244\n",
      "[23,   14] loss 12.584403\n",
      "[23,   15] loss 17.608429\n",
      "[23,   16] loss 15.495797\n",
      "[23,   17] loss 12.593392\n",
      "[24,    1] loss 13.537154\n",
      "[24,    2] loss 15.766794\n",
      "[24,    3] loss 14.498658\n",
      "[24,    4] loss 15.260884\n",
      "[24,    5] loss 14.884658\n",
      "[24,    6] loss 15.292169\n",
      "[24,    7] loss 14.309400\n",
      "[24,    8] loss 14.636329\n",
      "[24,    9] loss 14.995163\n",
      "[24,   10] loss 14.616008\n",
      "[24,   11] loss 13.724202\n",
      "[24,   12] loss 14.520300\n",
      "[24,   13] loss 18.172122\n",
      "[24,   14] loss 16.349646\n",
      "[24,   15] loss 18.392011\n",
      "[24,   16] loss 15.107336\n",
      "[24,   17] loss 9.176104\n",
      "[25,    1] loss 12.058316\n",
      "[25,    2] loss 14.510939\n",
      "[25,    3] loss 17.398312\n",
      "[25,    4] loss 13.706035\n",
      "[25,    5] loss 12.881441\n",
      "[25,    6] loss 14.553665\n",
      "[25,    7] loss 14.559495\n",
      "[25,    8] loss 16.700977\n",
      "[25,    9] loss 13.446163\n",
      "[25,   10] loss 16.628640\n",
      "[25,   11] loss 13.215693\n",
      "[25,   12] loss 16.779309\n",
      "[25,   13] loss 12.698502\n",
      "[25,   14] loss 16.953277\n",
      "[25,   15] loss 14.660702\n",
      "[25,   16] loss 13.765888\n",
      "[25,   17] loss 13.111294\n",
      "[26,    1] loss 12.335412\n",
      "[26,    2] loss 13.153456\n",
      "[26,    3] loss 13.881851\n",
      "[26,    4] loss 10.725498\n",
      "[26,    5] loss 15.117181\n",
      "[26,    6] loss 18.671333\n",
      "[26,    7] loss 20.293945\n",
      "[26,    8] loss 16.892787\n",
      "[26,    9] loss 14.869703\n",
      "[26,   10] loss 13.337615\n",
      "[26,   11] loss 13.206484\n",
      "[26,   12] loss 14.764179\n",
      "[26,   13] loss 11.883572\n",
      "[26,   14] loss 16.803177\n",
      "[26,   15] loss 16.502840\n",
      "[26,   16] loss 16.791606\n",
      "[26,   17] loss 13.243349\n",
      "[27,    1] loss 15.129932\n",
      "[27,    2] loss 14.846814\n",
      "[27,    3] loss 14.976345\n",
      "[27,    4] loss 12.457063\n",
      "[27,    5] loss 12.537139\n",
      "[27,    6] loss 15.114494\n",
      "[27,    7] loss 15.105794\n",
      "[27,    8] loss 15.329957\n",
      "[27,    9] loss 14.775269\n",
      "[27,   10] loss 15.312227\n",
      "[27,   11] loss 14.393604\n",
      "[27,   12] loss 15.315982\n",
      "[27,   13] loss 17.339628\n",
      "[27,   14] loss 13.056985\n",
      "[27,   15] loss 16.404121\n",
      "[27,   16] loss 15.877839\n",
      "[27,   17] loss 15.181524\n",
      "[28,    1] loss 14.083270\n",
      "[28,    2] loss 12.813851\n",
      "[28,    3] loss 15.840632\n",
      "[28,    4] loss 15.830098\n",
      "[28,    5] loss 16.059210\n",
      "[28,    6] loss 17.938619\n",
      "[28,    7] loss 13.861580\n",
      "[28,    8] loss 14.934960\n",
      "[28,    9] loss 14.595292\n",
      "[28,   10] loss 13.615857\n",
      "[28,   11] loss 12.642328\n",
      "[28,   12] loss 14.946607\n",
      "[28,   13] loss 16.587321\n",
      "[28,   14] loss 16.334995\n",
      "[28,   15] loss 15.102427\n",
      "[28,   16] loss 14.278207\n",
      "[28,   17] loss 11.133282\n",
      "[29,    1] loss 14.554311\n",
      "[29,    2] loss 13.752215\n",
      "[29,    3] loss 14.232283\n",
      "[29,    4] loss 14.220261\n",
      "[29,    5] loss 12.897769\n",
      "[29,    6] loss 13.062321\n",
      "[29,    7] loss 13.880061\n",
      "[29,    8] loss 14.386720\n",
      "[29,    9] loss 14.746635\n",
      "[29,   10] loss 14.508768\n",
      "[29,   11] loss 14.899316\n",
      "[29,   12] loss 15.846668\n",
      "[29,   13] loss 15.147184\n",
      "[29,   14] loss 15.836834\n",
      "[29,   15] loss 17.017676\n",
      "[29,   16] loss 14.414803\n",
      "[29,   17] loss 22.031771\n",
      "[30,    1] loss 15.426204\n",
      "[30,    2] loss 15.413914\n",
      "[30,    3] loss 15.136692\n",
      "[30,    4] loss 11.547544\n",
      "[30,    5] loss 14.784246\n",
      "[30,    6] loss 14.090913\n",
      "[30,    7] loss 14.740598\n",
      "[30,    8] loss 15.806145\n",
      "[30,    9] loss 14.312567\n",
      "[30,   10] loss 14.533239\n",
      "[30,   11] loss 16.266503\n",
      "[30,   12] loss 15.987547\n",
      "[30,   13] loss 15.585622\n",
      "[30,   14] loss 12.595581\n",
      "[30,   15] loss 13.915278\n",
      "[30,   16] loss 14.585010\n",
      "[30,   17] loss 15.297491\n",
      "[31,    1] loss 15.599482\n",
      "[31,    2] loss 13.843494\n",
      "[31,    3] loss 19.176094\n",
      "[31,    4] loss 11.410952\n",
      "[31,    5] loss 15.719707\n",
      "[31,    6] loss 14.001269\n",
      "[31,    7] loss 15.845442\n",
      "[31,    8] loss 13.054243\n",
      "[31,    9] loss 14.152654\n",
      "[31,   10] loss 14.909925\n",
      "[31,   11] loss 14.513293\n",
      "[31,   12] loss 11.181875\n",
      "[31,   13] loss 13.559575\n",
      "[31,   14] loss 13.129744\n",
      "[31,   15] loss 15.466161\n",
      "[31,   16] loss 16.718477\n",
      "[31,   17] loss 19.757323\n",
      "[32,    1] loss 15.084298\n",
      "[32,    2] loss 12.895974\n",
      "[32,    3] loss 14.968976\n",
      "[32,    4] loss 14.233232\n",
      "[32,    5] loss 13.904137\n",
      "[32,    6] loss 13.881160\n",
      "[32,    7] loss 16.544663\n",
      "[32,    8] loss 14.037342\n",
      "[32,    9] loss 15.569908\n",
      "[32,   10] loss 15.201449\n",
      "[32,   11] loss 16.339646\n",
      "[32,   12] loss 13.542633\n",
      "[32,   13] loss 17.690915\n",
      "[32,   14] loss 14.638888\n",
      "[32,   15] loss 12.982973\n",
      "[32,   16] loss 12.741410\n",
      "[32,   17] loss 16.235991\n",
      "[33,    1] loss 15.286731\n",
      "[33,    2] loss 16.540368\n",
      "[33,    3] loss 13.806628\n",
      "[33,    4] loss 13.916848\n",
      "[33,    5] loss 15.736232\n",
      "[33,    6] loss 11.542979\n",
      "[33,    7] loss 16.162859\n",
      "[33,    8] loss 14.740033\n",
      "[33,    9] loss 14.186929\n",
      "[33,   10] loss 15.512653\n",
      "[33,   11] loss 15.840500\n",
      "[33,   12] loss 13.665211\n",
      "[33,   13] loss 15.039541\n",
      "[33,   14] loss 15.556917\n",
      "[33,   15] loss 13.681612\n",
      "[33,   16] loss 14.555778\n",
      "[33,   17] loss 11.054835\n",
      "[34,    1] loss 13.736358\n",
      "[34,    2] loss 13.563436\n",
      "[34,    3] loss 13.225475\n",
      "[34,    4] loss 16.185743\n",
      "[34,    5] loss 15.254162\n",
      "[34,    6] loss 14.624101\n",
      "[34,    7] loss 12.492772\n",
      "[34,    8] loss 12.763807\n",
      "[34,    9] loss 16.285611\n",
      "[34,   10] loss 13.519906\n",
      "[34,   11] loss 17.071584\n",
      "[34,   12] loss 14.517374\n",
      "[34,   13] loss 14.274257\n",
      "[34,   14] loss 16.428138\n",
      "[34,   15] loss 16.305876\n",
      "[34,   16] loss 14.228450\n",
      "[34,   17] loss 14.335114\n",
      "[35,    1] loss 13.524129\n",
      "[35,    2] loss 15.564957\n",
      "[35,    3] loss 13.532235\n",
      "[35,    4] loss 11.402365\n",
      "[35,    5] loss 13.953023\n",
      "[35,    6] loss 13.055707\n",
      "[35,    7] loss 14.091566\n",
      "[35,    8] loss 14.787348\n",
      "[35,    9] loss 15.887996\n",
      "[35,   10] loss 15.866157\n",
      "[35,   11] loss 16.291604\n",
      "[35,   12] loss 14.517311\n",
      "[35,   13] loss 16.378705\n",
      "[35,   14] loss 13.499939\n",
      "[35,   15] loss 14.130929\n",
      "[35,   16] loss 15.657034\n",
      "[35,   17] loss 15.237880\n",
      "[36,    1] loss 14.152594\n",
      "[36,    2] loss 14.429506\n",
      "[36,    3] loss 17.681386\n",
      "[36,    4] loss 15.354434\n",
      "[36,    5] loss 12.833618\n",
      "[36,    6] loss 14.254484\n",
      "[36,    7] loss 13.683812\n",
      "[36,    8] loss 13.985774\n",
      "[36,    9] loss 14.101963\n",
      "[36,   10] loss 13.055543\n",
      "[36,   11] loss 13.592292\n",
      "[36,   12] loss 13.075852\n",
      "[36,   13] loss 17.348379\n",
      "[36,   14] loss 13.605754\n",
      "[36,   15] loss 14.902908\n",
      "[36,   16] loss 15.006242\n",
      "[36,   17] loss 15.030517\n",
      "[37,    1] loss 13.934435\n",
      "[37,    2] loss 13.757315\n",
      "[37,    3] loss 13.783222\n",
      "[37,    4] loss 11.827683\n",
      "[37,    5] loss 15.374975\n",
      "[37,    6] loss 16.913421\n",
      "[37,    7] loss 14.594712\n",
      "[37,    8] loss 16.461097\n",
      "[37,    9] loss 14.061491\n",
      "[37,   10] loss 13.961090\n",
      "[37,   11] loss 14.837630\n",
      "[37,   12] loss 14.952311\n",
      "[37,   13] loss 15.791632\n",
      "[37,   14] loss 14.388102\n",
      "[37,   15] loss 13.620816\n",
      "[37,   16] loss 14.702092\n",
      "[37,   17] loss 10.651196\n",
      "[38,    1] loss 13.307188\n",
      "[38,    2] loss 14.651382\n",
      "[38,    3] loss 15.862386\n",
      "[38,    4] loss 14.826768\n",
      "[38,    5] loss 16.770780\n",
      "[38,    6] loss 12.360284\n",
      "[38,    7] loss 14.838985\n",
      "[38,    8] loss 14.327050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38,    9] loss 14.164148\n",
      "[38,   10] loss 10.823217\n",
      "[38,   11] loss 13.884088\n",
      "[38,   12] loss 16.554945\n",
      "[38,   13] loss 16.210943\n",
      "[38,   14] loss 14.451707\n",
      "[38,   15] loss 14.904278\n",
      "[38,   16] loss 14.807605\n",
      "[38,   17] loss 14.967807\n",
      "[39,    1] loss 14.329214\n",
      "[39,    2] loss 13.102538\n",
      "[39,    3] loss 15.135828\n",
      "[39,    4] loss 14.842800\n",
      "[39,    5] loss 12.463686\n",
      "[39,    6] loss 15.648658\n",
      "[39,    7] loss 14.830049\n",
      "[39,    8] loss 15.295338\n",
      "[39,    9] loss 15.364039\n",
      "[39,   10] loss 14.189307\n",
      "[39,   11] loss 12.895155\n",
      "[39,   12] loss 13.939942\n",
      "[39,   13] loss 17.480331\n",
      "[39,   14] loss 16.455208\n",
      "[39,   15] loss 12.516817\n",
      "[39,   16] loss 12.181778\n",
      "[39,   17] loss 14.239548\n",
      "[40,    1] loss 13.607251\n",
      "[40,    2] loss 14.878580\n",
      "[40,    3] loss 14.449371\n",
      "[40,    4] loss 14.111319\n",
      "[40,    5] loss 13.853257\n",
      "[40,    6] loss 14.226723\n",
      "[40,    7] loss 14.115510\n",
      "[40,    8] loss 12.528407\n",
      "[40,    9] loss 14.668769\n",
      "[40,   10] loss 12.779054\n",
      "[40,   11] loss 15.294491\n",
      "[40,   12] loss 16.388584\n",
      "[40,   13] loss 15.629926\n",
      "[40,   14] loss 11.673524\n",
      "[40,   15] loss 12.869473\n",
      "[40,   16] loss 13.272299\n",
      "[40,   17] loss 15.730358\n",
      "[41,    1] loss 15.824994\n",
      "[41,    2] loss 12.544595\n",
      "[41,    3] loss 16.746310\n",
      "[41,    4] loss 15.992370\n",
      "[41,    5] loss 13.348244\n",
      "[41,    6] loss 13.557235\n",
      "[41,    7] loss 16.497936\n",
      "[41,    8] loss 11.992496\n",
      "[41,    9] loss 15.128002\n",
      "[41,   10] loss 16.011346\n",
      "[41,   11] loss 14.078888\n",
      "[41,   12] loss 11.812285\n",
      "[41,   13] loss 13.380463\n",
      "[41,   14] loss 13.646536\n",
      "[41,   15] loss 15.364814\n",
      "[41,   16] loss 13.177433\n",
      "[41,   17] loss 10.808618\n",
      "[42,    1] loss 14.544218\n",
      "[42,    2] loss 12.453176\n",
      "[42,    3] loss 13.713011\n",
      "[42,    4] loss 14.879455\n",
      "[42,    5] loss 16.202688\n",
      "[42,    6] loss 13.998649\n",
      "[42,    7] loss 12.380789\n",
      "[42,    8] loss 15.758403\n",
      "[42,    9] loss 14.701740\n",
      "[42,   10] loss 14.771502\n",
      "[42,   11] loss 14.860200\n",
      "[42,   12] loss 14.403629\n",
      "[42,   13] loss 11.470427\n",
      "[42,   14] loss 15.658072\n",
      "[42,   15] loss 15.887023\n",
      "[42,   16] loss 15.234200\n",
      "[42,   17] loss 12.623744\n",
      "[43,    1] loss 13.738391\n",
      "[43,    2] loss 16.211355\n",
      "[43,    3] loss 12.409998\n",
      "[43,    4] loss 15.198538\n",
      "[43,    5] loss 12.747586\n",
      "[43,    6] loss 14.054804\n",
      "[43,    7] loss 15.365960\n",
      "[43,    8] loss 13.299142\n",
      "[43,    9] loss 13.048182\n",
      "[43,   10] loss 14.984314\n",
      "[43,   11] loss 12.865104\n",
      "[43,   12] loss 16.441915\n",
      "[43,   13] loss 14.540305\n",
      "[43,   14] loss 14.891060\n",
      "[43,   15] loss 14.103970\n",
      "[43,   16] loss 13.858193\n",
      "[43,   17] loss 16.095488\n",
      "[44,    1] loss 12.098411\n",
      "[44,    2] loss 14.042076\n",
      "[44,    3] loss 12.442337\n",
      "[44,    4] loss 14.446601\n",
      "[44,    5] loss 14.452719\n",
      "[44,    6] loss 15.238325\n",
      "[44,    7] loss 14.647590\n",
      "[44,    8] loss 14.072989\n",
      "[44,    9] loss 16.525114\n",
      "[44,   10] loss 17.017096\n",
      "[44,   11] loss 16.267858\n",
      "[44,   12] loss 13.032323\n",
      "[44,   13] loss 11.228100\n",
      "[44,   14] loss 15.674934\n",
      "[44,   15] loss 12.356458\n",
      "[44,   16] loss 14.822250\n",
      "[44,   17] loss 12.774203\n",
      "[45,    1] loss 15.123342\n",
      "[45,    2] loss 15.554700\n",
      "[45,    3] loss 12.155362\n",
      "[45,    4] loss 13.832805\n",
      "[45,    5] loss 14.611514\n",
      "[45,    6] loss 14.223050\n",
      "[45,    7] loss 14.919219\n",
      "[45,    8] loss 15.748777\n",
      "[45,    9] loss 14.023009\n",
      "[45,   10] loss 14.907928\n",
      "[45,   11] loss 14.410411\n",
      "[45,   12] loss 14.179347\n",
      "[45,   13] loss 13.008908\n",
      "[45,   14] loss 14.861774\n",
      "[45,   15] loss 14.215655\n",
      "[45,   16] loss 12.350566\n",
      "[45,   17] loss 18.815165\n",
      "[46,    1] loss 12.504962\n",
      "[46,    2] loss 14.726495\n",
      "[46,    3] loss 12.078543\n",
      "[46,    4] loss 13.847689\n",
      "[46,    5] loss 15.411868\n",
      "[46,    6] loss 15.009993\n",
      "[46,    7] loss 16.337263\n",
      "[46,    8] loss 16.887561\n",
      "[46,    9] loss 15.096330\n",
      "[46,   10] loss 12.559750\n",
      "[46,   11] loss 13.148124\n",
      "[46,   12] loss 13.358463\n",
      "[46,   13] loss 13.826829\n",
      "[46,   14] loss 15.249599\n",
      "[46,   15] loss 14.164817\n",
      "[46,   16] loss 14.320088\n",
      "[46,   17] loss 15.158295\n",
      "[47,    1] loss 13.070782\n",
      "[47,    2] loss 14.176598\n",
      "[47,    3] loss 13.173877\n",
      "[47,    4] loss 15.600969\n",
      "[47,    5] loss 14.198554\n",
      "[47,    6] loss 14.442738\n",
      "[47,    7] loss 14.753687\n",
      "[47,    8] loss 13.681882\n",
      "[47,    9] loss 15.231967\n",
      "[47,   10] loss 12.269943\n",
      "[47,   11] loss 14.393727\n",
      "[47,   12] loss 14.900343\n",
      "[47,   13] loss 15.152890\n",
      "[47,   14] loss 13.827692\n",
      "[47,   15] loss 14.421154\n",
      "[47,   16] loss 15.178945\n",
      "[47,   17] loss 16.782584\n",
      "[48,    1] loss 16.637387\n",
      "[48,    2] loss 13.035410\n",
      "[48,    3] loss 13.583394\n",
      "[48,    4] loss 14.032720\n",
      "[48,    5] loss 13.263052\n",
      "[48,    6] loss 15.646646\n",
      "[48,    7] loss 13.402686\n",
      "[48,    8] loss 14.898449\n",
      "[48,    9] loss 11.289931\n",
      "[48,   10] loss 13.170401\n",
      "[48,   11] loss 12.436314\n",
      "[48,   12] loss 14.316409\n",
      "[48,   13] loss 13.239849\n",
      "[48,   14] loss 14.042265\n",
      "[48,   15] loss 15.135249\n",
      "[48,   16] loss 16.005832\n",
      "[48,   17] loss 9.795693\n",
      "[49,    1] loss 15.063846\n",
      "[49,    2] loss 13.731091\n",
      "[49,    3] loss 14.033644\n",
      "[49,    4] loss 14.123478\n",
      "[49,    5] loss 14.475250\n",
      "[49,    6] loss 14.924964\n",
      "[49,    7] loss 13.573858\n",
      "[49,    8] loss 15.840792\n",
      "[49,    9] loss 10.587078\n",
      "[49,   10] loss 15.336768\n",
      "[49,   11] loss 13.464321\n",
      "[49,   12] loss 15.321083\n",
      "[49,   13] loss 14.563104\n",
      "[49,   14] loss 12.134868\n",
      "[49,   15] loss 13.738987\n",
      "[49,   16] loss 14.284509\n",
      "[49,   17] loss 18.632860\n",
      "[50,    1] loss 11.885510\n",
      "[50,    2] loss 16.459809\n",
      "[50,    3] loss 14.723801\n",
      "[50,    4] loss 13.705418\n",
      "[50,    5] loss 13.861910\n",
      "[50,    6] loss 13.521670\n",
      "[50,    7] loss 13.721236\n",
      "[50,    8] loss 14.801944\n",
      "[50,    9] loss 13.869532\n",
      "[50,   10] loss 11.029771\n",
      "[50,   11] loss 13.836559\n",
      "[50,   12] loss 12.485794\n",
      "[50,   13] loss 16.298413\n",
      "[50,   14] loss 15.492862\n",
      "[50,   15] loss 15.399876\n",
      "[50,   16] loss 14.326809\n",
      "[50,   17] loss 16.184400\n",
      "finish training\n"
     ]
    }
   ],
   "source": [
    "# weights_init(model)\n",
    "# Net.apply(weights_init)\n",
    "\n",
    "##################\n",
    "#  lr=1e-5 afternormalization\n",
    "##################\n",
    "epochnum=0\n",
    "iteration=0\n",
    "dloador=DataLoader(dataloader,batch_size=100,shuffle=True,num_workers=8,pin_memory=CUDA)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-5)\n",
    "criterion=nn.L1Loss()\n",
    "Net=model.double()\n",
    "for epoch in np.arange(50):\n",
    "    epochnum+=1\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(dloador,0):\n",
    "#         ipdb.set_trace()\n",
    "        iteration+=1\n",
    "        sample,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        sample.to(device)\n",
    "        labels.to(device)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "\n",
    "#         print(labels)\n",
    "        optimizer.zero_grad()\n",
    "#         sample=sample.double()\n",
    "        outputs=Net(sample.double())\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "#         if i%10==9:\n",
    "        print('[%d,%5d] loss %f' %(epoch+1,i+1,running_loss))\n",
    "        running_loss=0.0\n",
    "print('finish training')\n",
    "t.save(Net.state_dict(),'/home/wcj/ReferenceProject/PPGnet/checkpoints/0501Net_epoch%d-iteration%d.pth'%(epochnum,iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   10] loss 134.299568\n",
      "[1,   20] loss 132.329617\n",
      "[1,   30] loss 135.687647\n",
      "[1,   40] loss 132.201125\n",
      "[1,   50] loss 136.078483\n",
      "[1,   60] loss 133.065465\n",
      "[1,   70] loss 134.326528\n",
      "[1,   80] loss 132.030489\n",
      "[1,   90] loss 129.741983\n",
      "[1,  100] loss 126.645403\n",
      "[1,  110] loss 129.519817\n",
      "[1,  120] loss 124.410084\n",
      "[1,  130] loss 125.953030\n",
      "[1,  140] loss 128.519710\n",
      "[1,  150] loss 125.545708\n",
      "[1,  160] loss 121.291107\n",
      "[2,   10] loss 119.981345\n",
      "[2,   20] loss 110.817435\n",
      "[2,   30] loss 105.803995\n",
      "[2,   40] loss 98.362232\n",
      "[2,   50] loss 89.049048\n",
      "[2,   60] loss 86.946223\n",
      "[2,   70] loss 74.033331\n",
      "[2,   80] loss 67.284744\n",
      "[2,   90] loss 63.127860\n",
      "[2,  100] loss 48.260866\n",
      "[2,  110] loss 40.214488\n",
      "[2,  120] loss 33.705005\n",
      "[2,  130] loss 28.295592\n",
      "[2,  140] loss 23.182423\n",
      "[2,  150] loss 21.661089\n",
      "[2,  160] loss 19.642079\n",
      "[3,   10] loss 20.283816\n",
      "[3,   20] loss 18.566830\n",
      "[3,   30] loss 18.825332\n",
      "[3,   40] loss 21.997239\n",
      "[3,   50] loss 17.086562\n",
      "[3,   60] loss 18.954543\n",
      "[3,   70] loss 18.920628\n",
      "[3,   80] loss 19.238714\n",
      "[3,   90] loss 18.103341\n",
      "[3,  100] loss 19.466696\n",
      "[3,  110] loss 16.340400\n",
      "[3,  120] loss 19.246597\n",
      "[3,  130] loss 18.592904\n",
      "[3,  140] loss 18.715121\n",
      "[3,  150] loss 16.468539\n",
      "[3,  160] loss 17.920168\n",
      "[4,   10] loss 17.511461\n",
      "[4,   20] loss 20.050130\n",
      "[4,   30] loss 19.540598\n",
      "[4,   40] loss 16.951025\n",
      "[4,   50] loss 19.074968\n",
      "[4,   60] loss 15.747012\n",
      "[4,   70] loss 19.240213\n",
      "[4,   80] loss 18.021844\n",
      "[4,   90] loss 20.617538\n",
      "[4,  100] loss 20.486003\n",
      "[4,  110] loss 17.526296\n",
      "[4,  120] loss 17.803273\n",
      "[4,  130] loss 19.033874\n",
      "[4,  140] loss 17.653223\n",
      "[4,  150] loss 19.807701\n",
      "[4,  160] loss 19.443840\n",
      "[5,   10] loss 16.784595\n",
      "[5,   20] loss 21.313651\n",
      "[5,   30] loss 17.181766\n",
      "[5,   40] loss 17.681252\n",
      "[5,   50] loss 18.306556\n",
      "[5,   60] loss 18.713389\n",
      "[5,   70] loss 17.290282\n",
      "[5,   80] loss 17.611033\n",
      "[5,   90] loss 18.631890\n",
      "[5,  100] loss 17.952314\n",
      "[5,  110] loss 16.503447\n",
      "[5,  120] loss 16.592653\n",
      "[5,  130] loss 17.918853\n",
      "[5,  140] loss 20.327016\n",
      "[5,  150] loss 20.118775\n",
      "[5,  160] loss 19.723321\n",
      "[6,   10] loss 18.548081\n",
      "[6,   20] loss 18.838576\n",
      "[6,   30] loss 17.065815\n",
      "[6,   40] loss 20.188976\n",
      "[6,   50] loss 16.821334\n",
      "[6,   60] loss 19.350807\n",
      "[6,   70] loss 18.160091\n",
      "[6,   80] loss 19.391701\n",
      "[6,   90] loss 20.625646\n",
      "[6,  100] loss 18.169503\n",
      "[6,  110] loss 17.829397\n",
      "[6,  120] loss 17.080767\n",
      "[6,  130] loss 17.601620\n",
      "[6,  140] loss 17.632268\n",
      "[6,  150] loss 18.038623\n",
      "[6,  160] loss 17.454334\n",
      "[7,   10] loss 16.537092\n",
      "[7,   20] loss 20.714671\n",
      "[7,   30] loss 18.755502\n",
      "[7,   40] loss 17.967635\n",
      "[7,   50] loss 18.852985\n",
      "[7,   60] loss 17.963303\n",
      "[7,   70] loss 18.407875\n",
      "[7,   80] loss 18.736306\n",
      "[7,   90] loss 17.787742\n",
      "[7,  100] loss 18.489465\n",
      "[7,  110] loss 19.941144\n",
      "[7,  120] loss 16.144779\n",
      "[7,  130] loss 21.363557\n",
      "[7,  140] loss 16.997189\n",
      "[7,  150] loss 16.791871\n",
      "[7,  160] loss 16.886236\n",
      "[8,   10] loss 17.572789\n",
      "[8,   20] loss 17.800359\n",
      "[8,   30] loss 18.322514\n",
      "[8,   40] loss 17.150405\n",
      "[8,   50] loss 19.366266\n",
      "[8,   60] loss 18.891547\n",
      "[8,   70] loss 17.282944\n",
      "[8,   80] loss 18.057471\n",
      "[8,   90] loss 18.691141\n",
      "[8,  100] loss 17.096373\n",
      "[8,  110] loss 18.432829\n",
      "[8,  120] loss 16.434148\n",
      "[8,  130] loss 17.483061\n",
      "[8,  140] loss 18.887358\n",
      "[8,  150] loss 19.472276\n",
      "[8,  160] loss 18.528059\n",
      "[9,   10] loss 17.712120\n",
      "[9,   20] loss 18.824844\n",
      "[9,   30] loss 15.169555\n",
      "[9,   40] loss 17.127683\n",
      "[9,   50] loss 17.248924\n",
      "[9,   60] loss 15.059936\n",
      "[9,   70] loss 16.166088\n",
      "[9,   80] loss 16.826003\n",
      "[9,   90] loss 20.963762\n",
      "[9,  100] loss 18.013615\n",
      "[9,  110] loss 17.602249\n",
      "[9,  120] loss 19.573966\n",
      "[9,  130] loss 17.444355\n",
      "[9,  140] loss 18.039388\n",
      "[9,  150] loss 19.994989\n",
      "[9,  160] loss 21.075496\n",
      "[10,   10] loss 19.912656\n",
      "[10,   20] loss 15.696898\n",
      "[10,   30] loss 14.819392\n",
      "[10,   40] loss 18.744798\n",
      "[10,   50] loss 17.819312\n",
      "[10,   60] loss 19.295287\n",
      "[10,   70] loss 18.688887\n",
      "[10,   80] loss 18.043890\n",
      "[10,   90] loss 16.468479\n",
      "[10,  100] loss 21.483919\n",
      "[10,  110] loss 18.186334\n",
      "[10,  120] loss 18.748924\n",
      "[10,  130] loss 19.356189\n",
      "[10,  140] loss 15.391112\n",
      "[10,  150] loss 17.123156\n",
      "[10,  160] loss 15.758071\n",
      "[11,   10] loss 15.906641\n",
      "[11,   20] loss 19.521048\n",
      "[11,   30] loss 19.675473\n",
      "[11,   40] loss 14.979401\n",
      "[11,   50] loss 18.058106\n",
      "[11,   60] loss 16.952107\n",
      "[11,   70] loss 16.458655\n",
      "[11,   80] loss 20.571951\n",
      "[11,   90] loss 16.948444\n",
      "[11,  100] loss 19.593441\n",
      "[11,  110] loss 19.324819\n",
      "[11,  120] loss 17.692489\n",
      "[11,  130] loss 18.524328\n",
      "[11,  140] loss 15.836286\n",
      "[11,  150] loss 17.993990\n",
      "[11,  160] loss 20.656966\n",
      "[12,   10] loss 19.295872\n",
      "[12,   20] loss 16.459051\n",
      "[12,   30] loss 17.143353\n",
      "[12,   40] loss 20.571116\n",
      "[12,   50] loss 17.550176\n",
      "[12,   60] loss 17.841595\n",
      "[12,   70] loss 17.286652\n",
      "[12,   80] loss 15.711765\n",
      "[12,   90] loss 16.184669\n",
      "[12,  100] loss 18.153436\n",
      "[12,  110] loss 19.058322\n",
      "[12,  120] loss 15.527855\n",
      "[12,  130] loss 16.692871\n",
      "[12,  140] loss 19.275797\n",
      "[12,  150] loss 16.804148\n",
      "[12,  160] loss 19.317203\n",
      "[13,   10] loss 16.855869\n",
      "[13,   20] loss 16.892296\n",
      "[13,   30] loss 19.076283\n",
      "[13,   40] loss 16.532113\n",
      "[13,   50] loss 17.286897\n",
      "[13,   60] loss 16.853562\n",
      "[13,   70] loss 18.573068\n",
      "[13,   80] loss 16.517906\n",
      "[13,   90] loss 18.334349\n",
      "[13,  100] loss 17.703950\n",
      "[13,  110] loss 18.051144\n",
      "[13,  120] loss 19.486624\n",
      "[13,  130] loss 16.830649\n",
      "[13,  140] loss 17.759091\n",
      "[13,  150] loss 16.980484\n",
      "[13,  160] loss 17.287912\n",
      "[14,   10] loss 16.220715\n",
      "[14,   20] loss 16.774833\n",
      "[14,   30] loss 18.258303\n",
      "[14,   40] loss 20.765920\n",
      "[14,   50] loss 17.211893\n",
      "[14,   60] loss 17.666101\n",
      "[14,   70] loss 17.512989\n",
      "[14,   80] loss 16.996199\n",
      "[14,   90] loss 18.549807\n",
      "[14,  100] loss 16.026461\n",
      "[14,  110] loss 15.982320\n",
      "[14,  120] loss 16.049908\n",
      "[14,  130] loss 16.766638\n",
      "[14,  140] loss 17.175913\n",
      "[14,  150] loss 16.337868\n",
      "[14,  160] loss 18.958533\n",
      "[15,   10] loss 17.505743\n",
      "[15,   20] loss 17.093997\n",
      "[15,   30] loss 16.779972\n",
      "[15,   40] loss 18.554101\n",
      "[15,   50] loss 18.334037\n",
      "[15,   60] loss 20.045108\n",
      "[15,   70] loss 15.377479\n",
      "[15,   80] loss 17.897995\n",
      "[15,   90] loss 16.422613\n",
      "[15,  100] loss 18.084661\n",
      "[15,  110] loss 20.118904\n",
      "[15,  120] loss 17.293954\n",
      "[15,  130] loss 16.296987\n",
      "[15,  140] loss 16.707737\n",
      "[15,  150] loss 18.522061\n",
      "[15,  160] loss 17.381304\n",
      "[16,   10] loss 15.244687\n",
      "[16,   20] loss 18.983307\n",
      "[16,   30] loss 16.368877\n",
      "[16,   40] loss 15.924706\n",
      "[16,   50] loss 16.807555\n",
      "[16,   60] loss 17.621815\n",
      "[16,   70] loss 15.799669\n",
      "[16,   80] loss 18.348631\n",
      "[16,   90] loss 19.114450\n",
      "[16,  100] loss 18.757007\n",
      "[16,  110] loss 16.914152\n",
      "[16,  120] loss 18.346841\n",
      "[16,  130] loss 16.231932\n",
      "[16,  140] loss 17.344290\n",
      "[16,  150] loss 15.518208\n",
      "[16,  160] loss 18.422889\n",
      "[17,   10] loss 19.960845\n",
      "[17,   20] loss 17.954870\n",
      "[17,   30] loss 16.809902\n",
      "[17,   40] loss 16.404453\n",
      "[17,   50] loss 15.622585\n",
      "[17,   60] loss 17.441609\n",
      "[17,   70] loss 15.985264\n",
      "[17,   80] loss 16.468650\n",
      "[17,   90] loss 15.949023\n",
      "[17,  100] loss 15.091434\n",
      "[17,  110] loss 18.026653\n",
      "[17,  120] loss 16.189670\n",
      "[17,  130] loss 16.906297\n",
      "[17,  140] loss 15.498654\n",
      "[17,  150] loss 18.966504\n",
      "[17,  160] loss 16.866268\n",
      "[18,   10] loss 16.350205\n",
      "[18,   20] loss 16.976261\n",
      "[18,   30] loss 16.838838\n",
      "[18,   40] loss 17.671902\n",
      "[18,   50] loss 16.434803\n",
      "[18,   60] loss 17.511488\n",
      "[18,   70] loss 17.549288\n",
      "[18,   80] loss 20.008649\n",
      "[18,   90] loss 17.267671\n",
      "[18,  100] loss 17.098529\n",
      "[18,  110] loss 16.396622\n",
      "[18,  120] loss 16.090755\n",
      "[18,  130] loss 14.676605\n",
      "[18,  140] loss 15.623179\n",
      "[18,  150] loss 15.000066\n",
      "[18,  160] loss 18.822207\n",
      "[19,   10] loss 14.736716\n",
      "[19,   20] loss 21.054120\n",
      "[19,   30] loss 17.502002\n",
      "[19,   40] loss 15.392452\n",
      "[19,   50] loss 13.844151\n",
      "[19,   60] loss 17.964484\n",
      "[19,   70] loss 15.384772\n",
      "[19,   80] loss 16.987083\n",
      "[19,   90] loss 16.006284\n",
      "[19,  100] loss 17.539408\n",
      "[19,  110] loss 16.208763\n",
      "[19,  120] loss 16.096303\n",
      "[19,  130] loss 16.484839\n",
      "[19,  140] loss 15.566790\n",
      "[19,  150] loss 14.970437\n",
      "[19,  160] loss 15.060301\n",
      "[20,   10] loss 16.499741\n",
      "[20,   20] loss 16.184650\n",
      "[20,   30] loss 15.083576\n",
      "[20,   40] loss 14.845695\n",
      "[20,   50] loss 15.609850\n",
      "[20,   60] loss 16.841796\n",
      "[20,   70] loss 15.127866\n",
      "[20,   80] loss 15.498202\n",
      "[20,   90] loss 15.413654\n",
      "[20,  100] loss 16.196569\n",
      "[20,  110] loss 13.787953\n",
      "[20,  120] loss 16.196488\n",
      "[20,  130] loss 16.307505\n",
      "[20,  140] loss 15.330883\n",
      "[20,  150] loss 15.787089\n",
      "[20,  160] loss 16.885570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21,   10] loss 15.398592\n",
      "[21,   20] loss 14.184346\n",
      "[21,   30] loss 14.559223\n",
      "[21,   40] loss 14.653625\n",
      "[21,   50] loss 13.568805\n",
      "[21,   60] loss 19.832872\n",
      "[21,   70] loss 18.582728\n",
      "[21,   80] loss 15.407980\n",
      "[21,   90] loss 16.185096\n",
      "[21,  100] loss 15.114405\n",
      "[21,  110] loss 16.868146\n",
      "[21,  120] loss 13.496922\n",
      "[21,  130] loss 13.195046\n",
      "[21,  140] loss 17.172034\n",
      "[21,  150] loss 14.979118\n",
      "[21,  160] loss 15.308647\n",
      "[22,   10] loss 12.710864\n",
      "[22,   20] loss 15.011645\n",
      "[22,   30] loss 15.218285\n",
      "[22,   40] loss 13.752074\n",
      "[22,   50] loss 13.418766\n",
      "[22,   60] loss 15.356521\n",
      "[22,   70] loss 17.146041\n",
      "[22,   80] loss 14.713213\n",
      "[22,   90] loss 15.835199\n",
      "[22,  100] loss 15.728870\n",
      "[22,  110] loss 17.430233\n",
      "[22,  120] loss 13.499685\n",
      "[22,  130] loss 16.181136\n",
      "[22,  140] loss 14.150157\n",
      "[22,  150] loss 14.442425\n",
      "[22,  160] loss 13.809524\n",
      "[23,   10] loss 12.966034\n",
      "[23,   20] loss 13.540684\n",
      "[23,   30] loss 13.202906\n",
      "[23,   40] loss 15.274282\n",
      "[23,   50] loss 13.887502\n",
      "[23,   60] loss 14.676743\n",
      "[23,   70] loss 14.462775\n",
      "[23,   80] loss 15.235828\n",
      "[23,   90] loss 14.109855\n",
      "[23,  100] loss 15.195397\n",
      "[23,  110] loss 14.314183\n",
      "[23,  120] loss 13.578073\n",
      "[23,  130] loss 15.802281\n",
      "[23,  140] loss 16.887867\n",
      "[23,  150] loss 16.565813\n",
      "[23,  160] loss 13.280168\n",
      "[24,   10] loss 14.541806\n",
      "[24,   20] loss 15.352975\n",
      "[24,   30] loss 16.645203\n",
      "[24,   40] loss 15.565947\n",
      "[24,   50] loss 14.091496\n",
      "[24,   60] loss 14.864229\n",
      "[24,   70] loss 14.608169\n",
      "[24,   80] loss 13.532147\n",
      "[24,   90] loss 12.942105\n",
      "[24,  100] loss 15.845921\n",
      "[24,  110] loss 16.196816\n",
      "[24,  120] loss 17.104536\n",
      "[24,  130] loss 14.029477\n",
      "[24,  140] loss 13.525114\n",
      "[24,  150] loss 10.608143\n",
      "[24,  160] loss 13.979089\n",
      "[25,   10] loss 13.672003\n",
      "[25,   20] loss 14.126776\n",
      "[25,   30] loss 17.412755\n",
      "[25,   40] loss 13.263608\n",
      "[25,   50] loss 15.109080\n",
      "[25,   60] loss 17.767053\n",
      "[25,   70] loss 14.246503\n",
      "[25,   80] loss 14.620903\n",
      "[25,   90] loss 13.981946\n",
      "[25,  100] loss 13.886985\n",
      "[25,  110] loss 14.534999\n",
      "[25,  120] loss 12.974965\n",
      "[25,  130] loss 13.064345\n",
      "[25,  140] loss 14.555342\n",
      "[25,  150] loss 11.007017\n",
      "[25,  160] loss 13.855131\n",
      "[26,   10] loss 11.442252\n",
      "[26,   20] loss 13.946302\n",
      "[26,   30] loss 16.817942\n",
      "[26,   40] loss 15.534402\n",
      "[26,   50] loss 13.834487\n",
      "[26,   60] loss 15.843618\n",
      "[26,   70] loss 11.961519\n",
      "[26,   80] loss 13.558787\n",
      "[26,   90] loss 16.060983\n",
      "[26,  100] loss 15.911804\n",
      "[26,  110] loss 12.129048\n",
      "[26,  120] loss 12.519030\n",
      "[26,  130] loss 14.192741\n",
      "[26,  140] loss 16.144788\n",
      "[26,  150] loss 11.973658\n",
      "[26,  160] loss 14.079277\n",
      "[27,   10] loss 11.929773\n",
      "[27,   20] loss 13.881665\n",
      "[27,   30] loss 15.879782\n",
      "[27,   40] loss 14.118748\n",
      "[27,   50] loss 16.446579\n",
      "[27,   60] loss 13.204712\n",
      "[27,   70] loss 16.050268\n",
      "[27,   80] loss 13.256237\n",
      "[27,   90] loss 12.026937\n",
      "[27,  100] loss 12.700445\n",
      "[27,  110] loss 16.652866\n",
      "[27,  120] loss 13.507606\n",
      "[27,  130] loss 14.182665\n",
      "[27,  140] loss 14.196485\n",
      "[27,  150] loss 12.093057\n",
      "[27,  160] loss 13.163926\n",
      "[28,   10] loss 12.206549\n",
      "[28,   20] loss 16.202935\n",
      "[28,   30] loss 13.019148\n",
      "[28,   40] loss 14.764496\n",
      "[28,   50] loss 14.296991\n",
      "[28,   60] loss 13.106751\n",
      "[28,   70] loss 13.316340\n",
      "[28,   80] loss 12.417260\n",
      "[28,   90] loss 14.111726\n",
      "[28,  100] loss 14.885133\n",
      "[28,  110] loss 16.651696\n",
      "[28,  120] loss 12.680354\n",
      "[28,  130] loss 14.068674\n",
      "[28,  140] loss 14.178933\n",
      "[28,  150] loss 13.645359\n",
      "[28,  160] loss 12.791473\n",
      "[29,   10] loss 14.846418\n",
      "[29,   20] loss 13.302594\n",
      "[29,   30] loss 15.358358\n",
      "[29,   40] loss 14.853655\n",
      "[29,   50] loss 11.960250\n",
      "[29,   60] loss 13.957421\n",
      "[29,   70] loss 12.684632\n",
      "[29,   80] loss 12.739225\n",
      "[29,   90] loss 15.954609\n",
      "[29,  100] loss 13.391417\n",
      "[29,  110] loss 12.263362\n",
      "[29,  120] loss 12.330494\n",
      "[29,  130] loss 16.381593\n",
      "[29,  140] loss 13.952067\n",
      "[29,  150] loss 15.099839\n",
      "[29,  160] loss 12.349454\n",
      "[30,   10] loss 14.070116\n",
      "[30,   20] loss 12.190786\n",
      "[30,   30] loss 14.509317\n",
      "[30,   40] loss 13.559891\n",
      "[30,   50] loss 14.377392\n",
      "[30,   60] loss 12.361722\n",
      "[30,   70] loss 15.252016\n",
      "[30,   80] loss 12.401413\n",
      "[30,   90] loss 12.318031\n",
      "[30,  100] loss 13.462636\n",
      "[30,  110] loss 12.815992\n",
      "[30,  120] loss 14.138193\n",
      "[30,  130] loss 16.338878\n",
      "[30,  140] loss 14.603918\n",
      "[30,  150] loss 14.103285\n",
      "[30,  160] loss 11.378766\n",
      "[31,   10] loss 12.388819\n",
      "[31,   20] loss 15.861358\n",
      "[31,   30] loss 13.700134\n",
      "[31,   40] loss 13.177831\n",
      "[31,   50] loss 14.478086\n",
      "[31,   60] loss 12.614026\n",
      "[31,   70] loss 13.430918\n",
      "[31,   80] loss 11.313575\n",
      "[31,   90] loss 15.131226\n",
      "[31,  100] loss 11.449491\n",
      "[31,  110] loss 12.681117\n",
      "[31,  120] loss 11.996153\n",
      "[31,  130] loss 13.169805\n",
      "[31,  140] loss 17.019919\n",
      "[31,  150] loss 13.610360\n",
      "[31,  160] loss 13.689962\n",
      "[32,   10] loss 12.029337\n",
      "[32,   20] loss 13.638737\n",
      "[32,   30] loss 14.355271\n",
      "[32,   40] loss 11.851991\n",
      "[32,   50] loss 15.484440\n",
      "[32,   60] loss 13.825794\n",
      "[32,   70] loss 12.828603\n",
      "[32,   80] loss 13.096995\n",
      "[32,   90] loss 12.870162\n",
      "[32,  100] loss 12.265604\n",
      "[32,  110] loss 13.321514\n",
      "[32,  120] loss 13.444617\n",
      "[32,  130] loss 12.600666\n",
      "[32,  140] loss 14.989584\n",
      "[32,  150] loss 14.557384\n",
      "[32,  160] loss 14.429399\n",
      "[33,   10] loss 14.222504\n",
      "[33,   20] loss 13.343261\n",
      "[33,   30] loss 12.328563\n",
      "[33,   40] loss 15.295533\n",
      "[33,   50] loss 13.454223\n",
      "[33,   60] loss 13.501661\n",
      "[33,   70] loss 14.060687\n",
      "[33,   80] loss 13.735036\n",
      "[33,   90] loss 14.403147\n",
      "[33,  100] loss 12.886643\n",
      "[33,  110] loss 15.557376\n",
      "[33,  120] loss 13.310430\n",
      "[33,  130] loss 13.349148\n",
      "[33,  140] loss 11.118438\n",
      "[33,  150] loss 13.119222\n",
      "[33,  160] loss 12.781799\n",
      "[34,   10] loss 13.244910\n",
      "[34,   20] loss 10.509695\n",
      "[34,   30] loss 11.643023\n",
      "[34,   40] loss 13.256573\n",
      "[34,   50] loss 12.427973\n",
      "[34,   60] loss 12.797345\n",
      "[34,   70] loss 13.726258\n",
      "[34,   80] loss 16.011632\n",
      "[34,   90] loss 13.813024\n",
      "[34,  100] loss 12.541198\n",
      "[34,  110] loss 13.010215\n",
      "[34,  120] loss 12.489540\n",
      "[34,  130] loss 13.392026\n",
      "[34,  140] loss 15.007246\n",
      "[34,  150] loss 14.104231\n",
      "[34,  160] loss 15.015085\n",
      "[35,   10] loss 13.966323\n",
      "[35,   20] loss 16.019958\n",
      "[35,   30] loss 11.745391\n",
      "[35,   40] loss 11.476481\n",
      "[35,   50] loss 12.434823\n",
      "[35,   60] loss 15.315220\n",
      "[35,   70] loss 13.567386\n",
      "[35,   80] loss 14.412596\n",
      "[35,   90] loss 12.804561\n",
      "[35,  100] loss 13.182777\n",
      "[35,  110] loss 12.152700\n",
      "[35,  120] loss 14.513322\n",
      "[35,  130] loss 12.955872\n",
      "[35,  140] loss 13.504688\n",
      "[35,  150] loss 10.892396\n",
      "[35,  160] loss 12.972451\n",
      "[36,   10] loss 13.699350\n",
      "[36,   20] loss 12.423494\n",
      "[36,   30] loss 12.936049\n",
      "[36,   40] loss 13.337451\n",
      "[36,   50] loss 12.091190\n",
      "[36,   60] loss 13.469284\n",
      "[36,   70] loss 14.171543\n",
      "[36,   80] loss 13.361514\n",
      "[36,   90] loss 12.529083\n",
      "[36,  100] loss 12.967976\n",
      "[36,  110] loss 14.104004\n",
      "[36,  120] loss 13.210973\n",
      "[36,  130] loss 13.978733\n",
      "[36,  140] loss 14.819759\n",
      "[36,  150] loss 11.936907\n",
      "[36,  160] loss 11.961552\n",
      "[37,   10] loss 14.342823\n",
      "[37,   20] loss 13.766533\n",
      "[37,   30] loss 14.606076\n",
      "[37,   40] loss 13.661793\n",
      "[37,   50] loss 11.667392\n",
      "[37,   60] loss 13.406829\n",
      "[37,   70] loss 14.824133\n",
      "[37,   80] loss 13.841827\n",
      "[37,   90] loss 11.916249\n",
      "[37,  100] loss 12.884900\n",
      "[37,  110] loss 15.952362\n",
      "[37,  120] loss 12.015870\n",
      "[37,  130] loss 12.291565\n",
      "[37,  140] loss 11.016252\n",
      "[37,  150] loss 10.825385\n",
      "[37,  160] loss 12.272303\n",
      "[38,   10] loss 12.489646\n",
      "[38,   20] loss 14.338466\n",
      "[38,   30] loss 10.678453\n",
      "[38,   40] loss 12.293002\n",
      "[38,   50] loss 13.267452\n",
      "[38,   60] loss 14.520913\n",
      "[38,   70] loss 12.888984\n",
      "[38,   80] loss 12.605671\n",
      "[38,   90] loss 13.260923\n",
      "[38,  100] loss 11.258957\n",
      "[38,  110] loss 12.987233\n",
      "[38,  120] loss 13.678859\n",
      "[38,  130] loss 12.464459\n",
      "[38,  140] loss 13.931760\n",
      "[38,  150] loss 12.484357\n",
      "[38,  160] loss 14.973354\n",
      "[39,   10] loss 12.881187\n",
      "[39,   20] loss 12.163824\n",
      "[39,   30] loss 13.379840\n",
      "[39,   40] loss 13.780558\n",
      "[39,   50] loss 12.778970\n",
      "[39,   60] loss 11.201070\n",
      "[39,   70] loss 11.713289\n",
      "[39,   80] loss 11.997744\n",
      "[39,   90] loss 13.562978\n",
      "[39,  100] loss 15.256830\n",
      "[39,  110] loss 13.627202\n",
      "[39,  120] loss 12.086203\n",
      "[39,  130] loss 14.288439\n",
      "[39,  140] loss 13.046873\n",
      "[39,  150] loss 13.090559\n",
      "[39,  160] loss 15.446918\n",
      "[40,   10] loss 13.904773\n",
      "[40,   20] loss 11.107932\n",
      "[40,   30] loss 12.685479\n",
      "[40,   40] loss 10.507645\n",
      "[40,   50] loss 13.075373\n",
      "[40,   60] loss 13.152137\n",
      "[40,   70] loss 13.882510\n",
      "[40,   80] loss 14.666746\n",
      "[40,   90] loss 14.104955\n",
      "[40,  100] loss 13.040678\n",
      "[40,  110] loss 14.100347\n",
      "[40,  120] loss 11.812839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40,  130] loss 14.541113\n",
      "[40,  140] loss 13.643483\n",
      "[40,  150] loss 11.010568\n",
      "[40,  160] loss 11.929895\n",
      "[41,   10] loss 11.131218\n",
      "[41,   20] loss 13.791050\n",
      "[41,   30] loss 13.350237\n",
      "[41,   40] loss 12.458562\n",
      "[41,   50] loss 15.574514\n",
      "[41,   60] loss 12.241973\n",
      "[41,   70] loss 12.969036\n",
      "[41,   80] loss 14.381458\n",
      "[41,   90] loss 14.875616\n",
      "[41,  100] loss 11.990353\n",
      "[41,  110] loss 13.243507\n",
      "[41,  120] loss 11.180364\n",
      "[41,  130] loss 14.073673\n",
      "[41,  140] loss 11.537700\n",
      "[41,  150] loss 14.458933\n",
      "[41,  160] loss 12.524456\n",
      "[42,   10] loss 16.146683\n",
      "[42,   20] loss 11.963037\n",
      "[42,   30] loss 13.870425\n",
      "[42,   40] loss 12.125495\n",
      "[42,   50] loss 12.274877\n",
      "[42,   60] loss 13.509826\n",
      "[42,   70] loss 10.899777\n",
      "[42,   80] loss 13.114007\n",
      "[42,   90] loss 14.578181\n",
      "[42,  100] loss 13.751219\n",
      "[42,  110] loss 12.967936\n",
      "[42,  120] loss 12.494351\n",
      "[42,  130] loss 12.262715\n",
      "[42,  140] loss 13.349425\n",
      "[42,  150] loss 9.822184\n",
      "[42,  160] loss 11.622746\n",
      "[43,   10] loss 13.017532\n",
      "[43,   20] loss 12.166746\n",
      "[43,   30] loss 15.340547\n",
      "[43,   40] loss 12.017972\n",
      "[43,   50] loss 11.933692\n",
      "[43,   60] loss 11.782267\n",
      "[43,   70] loss 11.649457\n",
      "[43,   80] loss 13.731452\n",
      "[43,   90] loss 15.423322\n",
      "[43,  100] loss 13.151148\n",
      "[43,  110] loss 12.781460\n",
      "[43,  120] loss 13.186456\n",
      "[43,  130] loss 11.854483\n",
      "[43,  140] loss 14.005222\n",
      "[43,  150] loss 11.090741\n",
      "[43,  160] loss 13.314732\n",
      "[44,   10] loss 13.673486\n",
      "[44,   20] loss 12.034923\n",
      "[44,   30] loss 10.997078\n",
      "[44,   40] loss 12.241465\n",
      "[44,   50] loss 11.680716\n",
      "[44,   60] loss 13.036376\n",
      "[44,   70] loss 12.754885\n",
      "[44,   80] loss 13.832679\n",
      "[44,   90] loss 11.319833\n",
      "[44,  100] loss 14.194477\n",
      "[44,  110] loss 13.617025\n",
      "[44,  120] loss 12.235979\n",
      "[44,  130] loss 11.747732\n",
      "[44,  140] loss 13.757274\n",
      "[44,  150] loss 15.387998\n",
      "[44,  160] loss 13.952145\n",
      "[45,   10] loss 13.409457\n",
      "[45,   20] loss 14.755841\n",
      "[45,   30] loss 12.675257\n",
      "[45,   40] loss 12.285767\n",
      "[45,   50] loss 12.304674\n",
      "[45,   60] loss 12.626807\n",
      "[45,   70] loss 9.521459\n",
      "[45,   80] loss 13.373770\n",
      "[45,   90] loss 14.007029\n",
      "[45,  100] loss 14.253628\n",
      "[45,  110] loss 13.072605\n",
      "[45,  120] loss 12.654458\n",
      "[45,  130] loss 11.879309\n",
      "[45,  140] loss 13.656077\n",
      "[45,  150] loss 12.138858\n",
      "[45,  160] loss 12.222958\n",
      "[46,   10] loss 11.097712\n",
      "[46,   20] loss 13.745890\n",
      "[46,   30] loss 12.018445\n",
      "[46,   40] loss 12.341407\n",
      "[46,   50] loss 12.769318\n",
      "[46,   60] loss 10.741769\n",
      "[46,   70] loss 12.195714\n",
      "[46,   80] loss 13.891457\n",
      "[46,   90] loss 12.519666\n",
      "[46,  100] loss 13.046647\n",
      "[46,  110] loss 13.786914\n",
      "[46,  120] loss 14.347896\n",
      "[46,  130] loss 9.933349\n",
      "[46,  140] loss 15.047070\n",
      "[46,  150] loss 12.684597\n",
      "[46,  160] loss 11.834260\n",
      "[47,   10] loss 13.762564\n",
      "[47,   20] loss 13.786672\n",
      "[47,   30] loss 11.020161\n",
      "[47,   40] loss 13.456709\n",
      "[47,   50] loss 13.822533\n",
      "[47,   60] loss 13.609483\n",
      "[47,   70] loss 12.162718\n",
      "[47,   80] loss 13.091158\n",
      "[47,   90] loss 11.501932\n",
      "[47,  100] loss 12.960199\n",
      "[47,  110] loss 14.032232\n",
      "[47,  120] loss 10.404348\n",
      "[47,  130] loss 12.024936\n",
      "[47,  140] loss 12.111323\n",
      "[47,  150] loss 11.816255\n",
      "[47,  160] loss 12.669055\n",
      "[48,   10] loss 13.271787\n",
      "[48,   20] loss 12.855941\n",
      "[48,   30] loss 13.989953\n",
      "[48,   40] loss 14.937267\n",
      "[48,   50] loss 13.746759\n",
      "[48,   60] loss 11.641088\n",
      "[48,   70] loss 9.860402\n",
      "[48,   80] loss 11.729047\n",
      "[48,   90] loss 12.161499\n",
      "[48,  100] loss 12.160079\n",
      "[48,  110] loss 13.144130\n",
      "[48,  120] loss 12.216833\n",
      "[48,  130] loss 12.866661\n",
      "[48,  140] loss 13.466453\n",
      "[48,  150] loss 13.369017\n",
      "[48,  160] loss 10.049238\n",
      "[49,   10] loss 15.510604\n",
      "[49,   20] loss 12.430816\n",
      "[49,   30] loss 15.266054\n",
      "[49,   40] loss 13.479998\n",
      "[49,   50] loss 11.091651\n",
      "[49,   60] loss 12.933966\n",
      "[49,   70] loss 12.350171\n",
      "[49,   80] loss 11.650707\n",
      "[49,   90] loss 13.069387\n",
      "[49,  100] loss 12.790620\n",
      "[49,  110] loss 14.093877\n",
      "[49,  120] loss 11.456881\n",
      "[49,  130] loss 13.932388\n",
      "[49,  140] loss 12.396788\n",
      "[49,  150] loss 10.760549\n",
      "[49,  160] loss 8.823710\n",
      "[50,   10] loss 13.676604\n",
      "[50,   20] loss 13.008245\n",
      "[50,   30] loss 10.731878\n",
      "[50,   40] loss 12.920315\n",
      "[50,   50] loss 11.561470\n",
      "[50,   60] loss 13.916334\n",
      "[50,   70] loss 13.027016\n",
      "[50,   80] loss 13.417916\n",
      "[50,   90] loss 12.284959\n",
      "[50,  100] loss 12.046046\n",
      "[50,  110] loss 12.112337\n",
      "[50,  120] loss 9.780623\n",
      "[50,  130] loss 14.018301\n",
      "[50,  140] loss 12.075147\n",
      "[50,  150] loss 12.404862\n",
      "[50,  160] loss 10.032031\n",
      "[51,   10] loss 13.935348\n",
      "[51,   20] loss 12.869116\n",
      "[51,   30] loss 11.571931\n",
      "[51,   40] loss 13.472824\n",
      "[51,   50] loss 13.693154\n",
      "[51,   60] loss 12.295996\n",
      "[51,   70] loss 12.774803\n",
      "[51,   80] loss 12.302311\n",
      "[51,   90] loss 11.309366\n",
      "[51,  100] loss 11.147409\n",
      "[51,  110] loss 9.764985\n",
      "[51,  120] loss 14.460471\n",
      "[51,  130] loss 13.021836\n",
      "[51,  140] loss 10.627494\n",
      "[51,  150] loss 13.450784\n",
      "[51,  160] loss 13.125934\n",
      "[52,   10] loss 14.239054\n",
      "[52,   20] loss 12.462121\n",
      "[52,   30] loss 11.762158\n",
      "[52,   40] loss 10.713861\n",
      "[52,   50] loss 12.965913\n",
      "[52,   60] loss 12.044248\n",
      "[52,   70] loss 11.742535\n",
      "[52,   80] loss 14.038355\n",
      "[52,   90] loss 10.405337\n",
      "[52,  100] loss 12.122795\n",
      "[52,  110] loss 11.671526\n",
      "[52,  120] loss 11.900423\n",
      "[52,  130] loss 14.056963\n",
      "[52,  140] loss 15.320077\n",
      "[52,  150] loss 10.928141\n",
      "[52,  160] loss 11.289245\n",
      "[53,   10] loss 13.206381\n",
      "[53,   20] loss 13.293948\n",
      "[53,   30] loss 11.594793\n",
      "[53,   40] loss 12.185031\n",
      "[53,   50] loss 11.742752\n",
      "[53,   60] loss 12.187515\n",
      "[53,   70] loss 11.057551\n",
      "[53,   80] loss 13.677906\n",
      "[53,   90] loss 13.009824\n",
      "[53,  100] loss 12.709638\n",
      "[53,  110] loss 13.793288\n",
      "[53,  120] loss 12.544523\n",
      "[53,  130] loss 10.055651\n",
      "[53,  140] loss 13.883467\n",
      "[53,  150] loss 10.024500\n",
      "[53,  160] loss 12.478042\n",
      "[54,   10] loss 9.906554\n",
      "[54,   20] loss 11.905897\n",
      "[54,   30] loss 11.947335\n",
      "[54,   40] loss 12.380250\n",
      "[54,   50] loss 10.283057\n",
      "[54,   60] loss 14.080878\n",
      "[54,   70] loss 13.672345\n",
      "[54,   80] loss 10.413053\n",
      "[54,   90] loss 14.029855\n",
      "[54,  100] loss 10.616836\n",
      "[54,  110] loss 11.195482\n",
      "[54,  120] loss 11.453667\n",
      "[54,  130] loss 13.454434\n",
      "[54,  140] loss 11.796047\n",
      "[54,  150] loss 14.982719\n",
      "[54,  160] loss 11.182367\n",
      "[55,   10] loss 14.960497\n",
      "[55,   20] loss 12.656766\n",
      "[55,   30] loss 13.625571\n",
      "[55,   40] loss 12.756458\n",
      "[55,   50] loss 12.549641\n",
      "[55,   60] loss 13.641143\n",
      "[55,   70] loss 10.656394\n",
      "[55,   80] loss 11.635756\n",
      "[55,   90] loss 11.984259\n",
      "[55,  100] loss 9.419654\n",
      "[55,  110] loss 10.599246\n",
      "[55,  120] loss 12.677752\n",
      "[55,  130] loss 12.512659\n",
      "[55,  140] loss 11.822187\n",
      "[55,  150] loss 12.045642\n",
      "[55,  160] loss 13.420521\n",
      "[56,   10] loss 13.447543\n",
      "[56,   20] loss 12.393160\n",
      "[56,   30] loss 12.422978\n",
      "[56,   40] loss 9.692001\n",
      "[56,   50] loss 10.302092\n",
      "[56,   60] loss 11.339847\n",
      "[56,   70] loss 11.119118\n",
      "[56,   80] loss 11.634811\n",
      "[56,   90] loss 13.346901\n",
      "[56,  100] loss 13.018195\n",
      "[56,  110] loss 10.574129\n",
      "[56,  120] loss 12.728330\n",
      "[56,  130] loss 12.031675\n",
      "[56,  140] loss 13.383247\n",
      "[56,  150] loss 15.028648\n",
      "[56,  160] loss 12.138346\n",
      "[57,   10] loss 12.591455\n",
      "[57,   20] loss 11.210994\n",
      "[57,   30] loss 11.540552\n",
      "[57,   40] loss 13.022166\n",
      "[57,   50] loss 12.066425\n",
      "[57,   60] loss 10.771064\n",
      "[57,   70] loss 11.844490\n",
      "[57,   80] loss 13.551557\n",
      "[57,   90] loss 10.529528\n",
      "[57,  100] loss 11.077861\n",
      "[57,  110] loss 10.890194\n",
      "[57,  120] loss 13.726331\n",
      "[57,  130] loss 12.707703\n",
      "[57,  140] loss 14.192941\n",
      "[57,  150] loss 14.597987\n",
      "[57,  160] loss 11.271037\n",
      "[58,   10] loss 11.179085\n",
      "[58,   20] loss 14.231023\n",
      "[58,   30] loss 11.584227\n",
      "[58,   40] loss 11.692238\n",
      "[58,   50] loss 10.603267\n",
      "[58,   60] loss 13.177565\n",
      "[58,   70] loss 9.614152\n",
      "[58,   80] loss 12.058808\n",
      "[58,   90] loss 9.868140\n",
      "[58,  100] loss 12.893277\n",
      "[58,  110] loss 11.585537\n",
      "[58,  120] loss 14.133513\n",
      "[58,  130] loss 10.659387\n",
      "[58,  140] loss 13.851021\n",
      "[58,  150] loss 12.858197\n",
      "[58,  160] loss 11.346212\n",
      "[59,   10] loss 12.962762\n",
      "[59,   20] loss 10.046547\n",
      "[59,   30] loss 12.370316\n",
      "[59,   40] loss 10.554453\n",
      "[59,   50] loss 11.308140\n",
      "[59,   60] loss 12.458436\n",
      "[59,   70] loss 13.102944\n",
      "[59,   80] loss 10.591857\n",
      "[59,   90] loss 12.721928\n",
      "[59,  100] loss 11.376827\n",
      "[59,  110] loss 12.013856\n",
      "[59,  120] loss 10.739566\n",
      "[59,  130] loss 14.142637\n",
      "[59,  140] loss 12.474512\n",
      "[59,  150] loss 12.448441\n",
      "[59,  160] loss 12.567855\n",
      "[60,   10] loss 11.433418\n",
      "[60,   20] loss 12.679560\n",
      "[60,   30] loss 10.569857\n",
      "[60,   40] loss 12.299763\n",
      "[60,   50] loss 14.515435\n",
      "[60,   60] loss 14.049085\n",
      "[60,   70] loss 9.862813\n",
      "[60,   80] loss 12.681293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60,   90] loss 11.583981\n",
      "[60,  100] loss 12.649878\n",
      "[60,  110] loss 11.352081\n",
      "[60,  120] loss 9.260697\n",
      "[60,  130] loss 14.203349\n",
      "[60,  140] loss 11.212363\n",
      "[60,  150] loss 12.598390\n",
      "[60,  160] loss 10.690963\n",
      "[61,   10] loss 12.183540\n",
      "[61,   20] loss 11.997730\n",
      "[61,   30] loss 11.462947\n",
      "[61,   40] loss 12.278993\n",
      "[61,   50] loss 14.276967\n",
      "[61,   60] loss 13.396092\n",
      "[61,   70] loss 9.484837\n",
      "[61,   80] loss 10.540526\n",
      "[61,   90] loss 13.084164\n",
      "[61,  100] loss 9.348747\n",
      "[61,  110] loss 12.296851\n",
      "[61,  120] loss 13.481287\n",
      "[61,  130] loss 12.779035\n",
      "[61,  140] loss 10.793849\n",
      "[61,  150] loss 10.893150\n",
      "[61,  160] loss 12.445921\n",
      "[62,   10] loss 12.816801\n",
      "[62,   20] loss 13.014676\n",
      "[62,   30] loss 10.769566\n",
      "[62,   40] loss 13.093299\n",
      "[62,   50] loss 12.571456\n",
      "[62,   60] loss 9.344928\n",
      "[62,   70] loss 13.896401\n",
      "[62,   80] loss 10.618738\n",
      "[62,   90] loss 10.402408\n",
      "[62,  100] loss 14.865508\n",
      "[62,  110] loss 12.306596\n",
      "[62,  120] loss 11.622593\n",
      "[62,  130] loss 12.241851\n",
      "[62,  140] loss 10.633454\n",
      "[62,  150] loss 10.186414\n",
      "[62,  160] loss 12.027955\n",
      "[63,   10] loss 12.783604\n",
      "[63,   20] loss 10.922564\n",
      "[63,   30] loss 8.707674\n",
      "[63,   40] loss 12.395830\n",
      "[63,   50] loss 14.014259\n",
      "[63,   60] loss 11.851173\n",
      "[63,   70] loss 11.420250\n",
      "[63,   80] loss 13.692672\n",
      "[63,   90] loss 11.444237\n",
      "[63,  100] loss 13.876709\n",
      "[63,  110] loss 11.072170\n",
      "[63,  120] loss 12.077879\n",
      "[63,  130] loss 10.864358\n",
      "[63,  140] loss 12.579318\n",
      "[63,  150] loss 9.865458\n",
      "[63,  160] loss 13.664346\n",
      "[64,   10] loss 11.783178\n",
      "[64,   20] loss 14.139330\n",
      "[64,   30] loss 11.538693\n",
      "[64,   40] loss 12.942674\n",
      "[64,   50] loss 13.242063\n",
      "[64,   60] loss 10.896678\n",
      "[64,   70] loss 12.194849\n",
      "[64,   80] loss 10.175000\n",
      "[64,   90] loss 13.042390\n",
      "[64,  100] loss 12.256850\n",
      "[64,  110] loss 11.314842\n",
      "[64,  120] loss 10.953732\n",
      "[64,  130] loss 9.653097\n",
      "[64,  140] loss 11.713975\n",
      "[64,  150] loss 11.586616\n",
      "[64,  160] loss 11.407155\n",
      "[65,   10] loss 12.257771\n",
      "[65,   20] loss 12.457873\n",
      "[65,   30] loss 11.970865\n",
      "[65,   40] loss 13.062695\n",
      "[65,   50] loss 13.642008\n",
      "[65,   60] loss 12.215590\n",
      "[65,   70] loss 11.679579\n",
      "[65,   80] loss 10.312872\n",
      "[65,   90] loss 10.226065\n",
      "[65,  100] loss 10.245341\n",
      "[65,  110] loss 11.824239\n",
      "[65,  120] loss 11.171952\n",
      "[65,  130] loss 11.876185\n",
      "[65,  140] loss 14.104638\n",
      "[65,  150] loss 11.659621\n",
      "[65,  160] loss 11.103958\n",
      "[66,   10] loss 14.044863\n",
      "[66,   20] loss 11.357125\n",
      "[66,   30] loss 12.903613\n",
      "[66,   40] loss 11.292273\n",
      "[66,   50] loss 12.387868\n",
      "[66,   60] loss 12.494845\n",
      "[66,   70] loss 11.841669\n",
      "[66,   80] loss 12.805305\n",
      "[66,   90] loss 14.005000\n",
      "[66,  100] loss 8.419347\n",
      "[66,  110] loss 11.603404\n",
      "[66,  120] loss 10.850958\n",
      "[66,  130] loss 13.217535\n",
      "[66,  140] loss 12.051394\n",
      "[66,  150] loss 11.753987\n",
      "[66,  160] loss 8.979418\n",
      "[67,   10] loss 11.504192\n",
      "[67,   20] loss 11.821034\n",
      "[67,   30] loss 16.326291\n",
      "[67,   40] loss 10.787374\n",
      "[67,   50] loss 11.320737\n",
      "[67,   60] loss 13.697081\n",
      "[67,   70] loss 11.666497\n",
      "[67,   80] loss 11.287661\n",
      "[67,   90] loss 10.316595\n",
      "[67,  100] loss 12.634798\n",
      "[67,  110] loss 11.340634\n",
      "[67,  120] loss 13.965488\n",
      "[67,  130] loss 11.064053\n",
      "[67,  140] loss 11.223084\n",
      "[67,  150] loss 8.941856\n",
      "[67,  160] loss 9.250911\n",
      "[68,   10] loss 13.248240\n",
      "[68,   20] loss 11.531281\n",
      "[68,   30] loss 10.731440\n",
      "[68,   40] loss 10.137299\n",
      "[68,   50] loss 11.887733\n",
      "[68,   60] loss 11.598379\n",
      "[68,   70] loss 10.422499\n",
      "[68,   80] loss 11.771409\n",
      "[68,   90] loss 10.345906\n",
      "[68,  100] loss 14.764674\n",
      "[68,  110] loss 11.944527\n",
      "[68,  120] loss 12.384833\n",
      "[68,  130] loss 11.181487\n",
      "[68,  140] loss 11.868163\n",
      "[68,  150] loss 13.168499\n",
      "[68,  160] loss 12.188926\n",
      "[69,   10] loss 12.657897\n",
      "[69,   20] loss 10.939156\n",
      "[69,   30] loss 12.175088\n",
      "[69,   40] loss 13.573238\n",
      "[69,   50] loss 11.855340\n",
      "[69,   60] loss 14.129825\n",
      "[69,   70] loss 10.767831\n",
      "[69,   80] loss 11.528551\n",
      "[69,   90] loss 11.038629\n",
      "[69,  100] loss 12.654155\n",
      "[69,  110] loss 11.716949\n",
      "[69,  120] loss 9.901715\n",
      "[69,  130] loss 12.538874\n",
      "[69,  140] loss 9.135551\n",
      "[69,  150] loss 10.439835\n",
      "[69,  160] loss 12.912199\n",
      "[70,   10] loss 11.838715\n",
      "[70,   20] loss 9.984821\n",
      "[70,   30] loss 9.912241\n",
      "[70,   40] loss 12.146367\n",
      "[70,   50] loss 12.271001\n",
      "[70,   60] loss 13.047596\n",
      "[70,   70] loss 10.351712\n",
      "[70,   80] loss 12.964699\n",
      "[70,   90] loss 11.001886\n",
      "[70,  100] loss 12.521233\n",
      "[70,  110] loss 14.191303\n",
      "[70,  120] loss 11.776302\n",
      "[70,  130] loss 12.048583\n",
      "[70,  140] loss 8.907225\n",
      "[70,  150] loss 13.647417\n",
      "[70,  160] loss 10.170353\n",
      "[71,   10] loss 12.566347\n",
      "[71,   20] loss 12.371649\n",
      "[71,   30] loss 8.756986\n",
      "[71,   40] loss 8.003229\n",
      "[71,   50] loss 11.672926\n",
      "[71,   60] loss 12.199888\n",
      "[71,   70] loss 12.264224\n",
      "[71,   80] loss 11.698722\n",
      "[71,   90] loss 10.851529\n",
      "[71,  100] loss 12.617412\n",
      "[71,  110] loss 12.248462\n",
      "[71,  120] loss 11.382849\n",
      "[71,  130] loss 11.158812\n",
      "[71,  140] loss 12.693576\n",
      "[71,  150] loss 11.290596\n",
      "[71,  160] loss 13.402189\n",
      "[72,   10] loss 9.536378\n",
      "[72,   20] loss 11.322563\n",
      "[72,   30] loss 11.523781\n",
      "[72,   40] loss 10.950265\n",
      "[72,   50] loss 11.211465\n",
      "[72,   60] loss 13.306459\n",
      "[72,   70] loss 11.260080\n",
      "[72,   80] loss 10.510765\n",
      "[72,   90] loss 11.910365\n",
      "[72,  100] loss 14.092291\n",
      "[72,  110] loss 9.498657\n",
      "[72,  120] loss 13.282987\n",
      "[72,  130] loss 11.682393\n",
      "[72,  140] loss 10.678245\n",
      "[72,  150] loss 10.651145\n",
      "[72,  160] loss 11.375946\n",
      "[73,   10] loss 11.911677\n",
      "[73,   20] loss 10.784818\n",
      "[73,   30] loss 9.296718\n",
      "[73,   40] loss 12.730287\n",
      "[73,   50] loss 11.451072\n",
      "[73,   60] loss 12.530840\n",
      "[73,   70] loss 10.225731\n",
      "[73,   80] loss 12.964045\n",
      "[73,   90] loss 12.017467\n",
      "[73,  100] loss 13.239594\n",
      "[73,  110] loss 11.256305\n",
      "[73,  120] loss 11.324170\n",
      "[73,  130] loss 11.275715\n",
      "[73,  140] loss 9.555015\n",
      "[73,  150] loss 12.766419\n",
      "[73,  160] loss 11.354543\n",
      "[74,   10] loss 9.806924\n",
      "[74,   20] loss 10.995778\n",
      "[74,   30] loss 12.442286\n",
      "[74,   40] loss 9.920454\n",
      "[74,   50] loss 13.152180\n",
      "[74,   60] loss 11.285708\n",
      "[74,   70] loss 12.870800\n",
      "[74,   80] loss 12.880240\n",
      "[74,   90] loss 12.246223\n",
      "[74,  100] loss 12.087143\n",
      "[74,  110] loss 10.112336\n",
      "[74,  120] loss 11.342451\n",
      "[74,  130] loss 11.555102\n",
      "[74,  140] loss 11.418409\n",
      "[74,  150] loss 11.206218\n",
      "[74,  160] loss 10.838523\n",
      "[75,   10] loss 10.053657\n",
      "[75,   20] loss 11.400180\n",
      "[75,   30] loss 9.410603\n",
      "[75,   40] loss 11.573601\n",
      "[75,   50] loss 10.680326\n",
      "[75,   60] loss 9.965376\n",
      "[75,   70] loss 11.441041\n",
      "[75,   80] loss 8.467339\n",
      "[75,   90] loss 11.262911\n",
      "[75,  100] loss 10.598009\n",
      "[75,  110] loss 15.565183\n",
      "[75,  120] loss 12.198863\n",
      "[75,  130] loss 10.225357\n",
      "[75,  140] loss 13.382782\n",
      "[75,  150] loss 13.004014\n",
      "[75,  160] loss 11.776563\n",
      "[76,   10] loss 11.903719\n",
      "[76,   20] loss 10.769073\n",
      "[76,   30] loss 11.965253\n",
      "[76,   40] loss 12.025004\n",
      "[76,   50] loss 12.960775\n",
      "[76,   60] loss 13.146463\n",
      "[76,   70] loss 12.627055\n",
      "[76,   80] loss 12.590419\n",
      "[76,   90] loss 12.500041\n",
      "[76,  100] loss 10.624727\n",
      "[76,  110] loss 9.659154\n",
      "[76,  120] loss 10.731262\n",
      "[76,  130] loss 8.220645\n",
      "[76,  140] loss 12.680519\n",
      "[76,  150] loss 10.603707\n",
      "[76,  160] loss 9.924084\n",
      "[77,   10] loss 10.518363\n",
      "[77,   20] loss 9.698687\n",
      "[77,   30] loss 10.074187\n",
      "[77,   40] loss 9.407426\n",
      "[77,   50] loss 9.757703\n",
      "[77,   60] loss 13.144723\n",
      "[77,   70] loss 12.355227\n",
      "[77,   80] loss 13.947805\n",
      "[77,   90] loss 10.528937\n",
      "[77,  100] loss 11.576511\n",
      "[77,  110] loss 12.032042\n",
      "[77,  120] loss 12.613872\n",
      "[77,  130] loss 11.784370\n",
      "[77,  140] loss 10.818067\n",
      "[77,  150] loss 10.207938\n",
      "[77,  160] loss 13.386456\n",
      "[78,   10] loss 13.602575\n",
      "[78,   20] loss 11.434171\n",
      "[78,   30] loss 12.002115\n",
      "[78,   40] loss 10.189718\n",
      "[78,   50] loss 11.027296\n",
      "[78,   60] loss 11.962268\n",
      "[78,   70] loss 12.454889\n",
      "[78,   80] loss 11.882535\n",
      "[78,   90] loss 10.799719\n",
      "[78,  100] loss 10.611727\n",
      "[78,  110] loss 11.365950\n",
      "[78,  120] loss 10.094385\n",
      "[78,  130] loss 10.136855\n",
      "[78,  140] loss 12.086000\n",
      "[78,  150] loss 12.907637\n",
      "[78,  160] loss 10.322934\n",
      "[79,   10] loss 11.913011\n",
      "[79,   20] loss 9.280115\n",
      "[79,   30] loss 9.790697\n",
      "[79,   40] loss 12.074915\n",
      "[79,   50] loss 13.702913\n",
      "[79,   60] loss 9.569147\n",
      "[79,   70] loss 11.840240\n",
      "[79,   80] loss 13.561767\n",
      "[79,   90] loss 11.069851\n",
      "[79,  100] loss 11.110333\n",
      "[79,  110] loss 10.042690\n",
      "[79,  120] loss 10.615212\n",
      "[79,  130] loss 11.673271\n",
      "[79,  140] loss 11.879485\n",
      "[79,  150] loss 11.542227\n",
      "[79,  160] loss 11.142188\n",
      "[80,   10] loss 10.893455\n",
      "[80,   20] loss 10.777145\n",
      "[80,   30] loss 12.224793\n",
      "[80,   40] loss 12.280660\n",
      "[80,   50] loss 9.966557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80,   60] loss 9.844217\n",
      "[80,   70] loss 12.308881\n",
      "[80,   80] loss 13.002188\n",
      "[80,   90] loss 11.512635\n",
      "[80,  100] loss 10.161814\n",
      "[80,  110] loss 11.579223\n",
      "[80,  120] loss 13.219178\n",
      "[80,  130] loss 13.535559\n",
      "[80,  140] loss 10.344421\n",
      "[80,  150] loss 9.828506\n",
      "[80,  160] loss 11.523191\n",
      "[81,   10] loss 9.905086\n",
      "[81,   20] loss 9.913093\n",
      "[81,   30] loss 9.300433\n",
      "[81,   40] loss 12.022934\n",
      "[81,   50] loss 9.805184\n",
      "[81,   60] loss 12.022980\n",
      "[81,   70] loss 13.615220\n",
      "[81,   80] loss 9.839054\n",
      "[81,   90] loss 10.747940\n",
      "[81,  100] loss 11.701450\n",
      "[81,  110] loss 10.557802\n",
      "[81,  120] loss 13.386785\n",
      "[81,  130] loss 12.899425\n",
      "[81,  140] loss 9.442037\n",
      "[81,  150] loss 11.377544\n",
      "[81,  160] loss 13.024476\n",
      "[82,   10] loss 11.118600\n",
      "[82,   20] loss 11.228141\n",
      "[82,   30] loss 10.180442\n",
      "[82,   40] loss 10.254639\n",
      "[82,   50] loss 14.361083\n",
      "[82,   60] loss 12.181726\n",
      "[82,   70] loss 11.420161\n",
      "[82,   80] loss 11.292575\n",
      "[82,   90] loss 11.563914\n",
      "[82,  100] loss 11.064414\n",
      "[82,  110] loss 11.218466\n",
      "[82,  120] loss 11.287560\n",
      "[82,  130] loss 11.147682\n",
      "[82,  140] loss 9.138108\n",
      "[82,  150] loss 10.648254\n",
      "[82,  160] loss 12.799666\n",
      "[83,   10] loss 10.880459\n",
      "[83,   20] loss 11.560034\n",
      "[83,   30] loss 11.808892\n",
      "[83,   40] loss 10.841158\n",
      "[83,   50] loss 11.717150\n",
      "[83,   60] loss 10.896556\n",
      "[83,   70] loss 13.014986\n",
      "[83,   80] loss 11.236496\n",
      "[83,   90] loss 12.152504\n",
      "[83,  100] loss 11.341242\n",
      "[83,  110] loss 9.390729\n",
      "[83,  120] loss 11.661230\n",
      "[83,  130] loss 8.862362\n",
      "[83,  140] loss 10.702715\n",
      "[83,  150] loss 10.541820\n",
      "[83,  160] loss 12.018235\n",
      "[84,   10] loss 10.938950\n",
      "[84,   20] loss 9.728217\n",
      "[84,   30] loss 11.109930\n",
      "[84,   40] loss 11.823722\n",
      "[84,   50] loss 12.161242\n",
      "[84,   60] loss 11.579708\n",
      "[84,   70] loss 11.771050\n",
      "[84,   80] loss 9.383836\n",
      "[84,   90] loss 11.548442\n",
      "[84,  100] loss 8.068723\n",
      "[84,  110] loss 11.759854\n",
      "[84,  120] loss 10.837236\n",
      "[84,  130] loss 11.437860\n",
      "[84,  140] loss 13.077546\n",
      "[84,  150] loss 10.989066\n",
      "[84,  160] loss 11.674624\n",
      "[85,   10] loss 10.021717\n",
      "[85,   20] loss 10.571715\n",
      "[85,   30] loss 11.124451\n",
      "[85,   40] loss 9.215042\n",
      "[85,   50] loss 10.997666\n",
      "[85,   60] loss 14.028741\n",
      "[85,   70] loss 11.610290\n",
      "[85,   80] loss 11.112009\n",
      "[85,   90] loss 13.043781\n",
      "[85,  100] loss 12.542653\n",
      "[85,  110] loss 9.652857\n",
      "[85,  120] loss 8.831353\n",
      "[85,  130] loss 11.267055\n",
      "[85,  140] loss 10.963738\n",
      "[85,  150] loss 12.602720\n",
      "[85,  160] loss 10.649162\n",
      "[86,   10] loss 11.656092\n",
      "[86,   20] loss 8.841502\n",
      "[86,   30] loss 12.421802\n",
      "[86,   40] loss 10.143652\n",
      "[86,   50] loss 11.095398\n",
      "[86,   60] loss 12.716657\n",
      "[86,   70] loss 10.801012\n",
      "[86,   80] loss 10.297177\n",
      "[86,   90] loss 10.190547\n",
      "[86,  100] loss 9.723671\n",
      "[86,  110] loss 12.069116\n",
      "[86,  120] loss 11.672126\n",
      "[86,  130] loss 10.899491\n",
      "[86,  140] loss 10.991709\n",
      "[86,  150] loss 12.400594\n",
      "[86,  160] loss 10.723174\n",
      "[87,   10] loss 9.911129\n",
      "[87,   20] loss 12.190916\n",
      "[87,   30] loss 10.066849\n",
      "[87,   40] loss 11.226197\n",
      "[87,   50] loss 10.241736\n",
      "[87,   60] loss 12.873990\n",
      "[87,   70] loss 10.682451\n",
      "[87,   80] loss 11.775666\n",
      "[87,   90] loss 11.411412\n",
      "[87,  100] loss 10.414329\n",
      "[87,  110] loss 10.351267\n",
      "[87,  120] loss 9.358439\n",
      "[87,  130] loss 13.508933\n",
      "[87,  140] loss 12.375758\n",
      "[87,  150] loss 10.172189\n",
      "[87,  160] loss 13.255808\n",
      "[88,   10] loss 11.854230\n",
      "[88,   20] loss 10.940381\n",
      "[88,   30] loss 12.387506\n",
      "[88,   40] loss 12.662493\n",
      "[88,   50] loss 11.692624\n",
      "[88,   60] loss 9.485438\n",
      "[88,   70] loss 13.150552\n",
      "[88,   80] loss 12.702856\n",
      "[88,   90] loss 11.591371\n",
      "[88,  100] loss 10.208026\n",
      "[88,  110] loss 8.766594\n",
      "[88,  120] loss 9.684800\n",
      "[88,  130] loss 7.745842\n",
      "[88,  140] loss 9.996871\n",
      "[88,  150] loss 10.061455\n",
      "[88,  160] loss 12.092026\n",
      "[89,   10] loss 9.252348\n",
      "[89,   20] loss 11.954671\n",
      "[89,   30] loss 11.521814\n",
      "[89,   40] loss 11.707005\n",
      "[89,   50] loss 10.939877\n",
      "[89,   60] loss 12.504289\n",
      "[89,   70] loss 11.879048\n",
      "[89,   80] loss 10.471949\n",
      "[89,   90] loss 9.475399\n",
      "[89,  100] loss 10.460204\n",
      "[89,  110] loss 11.078940\n",
      "[89,  120] loss 11.358741\n",
      "[89,  130] loss 9.531037\n",
      "[89,  140] loss 11.926098\n",
      "[89,  150] loss 11.111260\n",
      "[89,  160] loss 8.772890\n",
      "[90,   10] loss 11.333635\n",
      "[90,   20] loss 10.355319\n",
      "[90,   30] loss 10.575474\n",
      "[90,   40] loss 10.108865\n",
      "[90,   50] loss 11.218986\n",
      "[90,   60] loss 10.446273\n",
      "[90,   70] loss 11.287680\n",
      "[90,   80] loss 10.546897\n",
      "[90,   90] loss 13.236625\n",
      "[90,  100] loss 11.557540\n",
      "[90,  110] loss 10.106048\n",
      "[90,  120] loss 11.528164\n",
      "[90,  130] loss 11.341377\n",
      "[90,  140] loss 10.654116\n",
      "[90,  150] loss 9.875063\n",
      "[90,  160] loss 12.157436\n",
      "[91,   10] loss 8.999734\n",
      "[91,   20] loss 10.657000\n",
      "[91,   30] loss 10.521311\n",
      "[91,   40] loss 9.484098\n",
      "[91,   50] loss 9.246898\n",
      "[91,   60] loss 9.960024\n",
      "[91,   70] loss 8.373618\n",
      "[91,   80] loss 12.798586\n",
      "[91,   90] loss 9.141922\n",
      "[91,  100] loss 11.989970\n",
      "[91,  110] loss 12.852157\n",
      "[91,  120] loss 10.662242\n",
      "[91,  130] loss 14.439843\n",
      "[91,  140] loss 11.677571\n",
      "[91,  150] loss 11.153688\n",
      "[91,  160] loss 11.103981\n",
      "[92,   10] loss 10.224798\n",
      "[92,   20] loss 10.263912\n",
      "[92,   30] loss 10.315839\n",
      "[92,   40] loss 11.609068\n",
      "[92,   50] loss 12.240939\n",
      "[92,   60] loss 13.772328\n",
      "[92,   70] loss 9.671522\n",
      "[92,   80] loss 11.293496\n",
      "[92,   90] loss 9.657168\n",
      "[92,  100] loss 11.800881\n",
      "[92,  110] loss 8.845408\n",
      "[92,  120] loss 10.190213\n",
      "[92,  130] loss 13.146610\n",
      "[92,  140] loss 11.334287\n",
      "[92,  150] loss 8.920211\n",
      "[92,  160] loss 10.884807\n",
      "[93,   10] loss 11.885195\n",
      "[93,   20] loss 9.450831\n",
      "[93,   30] loss 10.535033\n",
      "[93,   40] loss 9.594669\n",
      "[93,   50] loss 10.450096\n",
      "[93,   60] loss 10.346359\n",
      "[93,   70] loss 12.296990\n",
      "[93,   80] loss 9.501837\n",
      "[93,   90] loss 8.904168\n",
      "[93,  100] loss 11.951458\n",
      "[93,  110] loss 9.940440\n",
      "[93,  120] loss 13.369335\n",
      "[93,  130] loss 11.198211\n",
      "[93,  140] loss 9.200022\n",
      "[93,  150] loss 10.240863\n",
      "[93,  160] loss 13.007858\n",
      "[94,   10] loss 12.258268\n",
      "[94,   20] loss 9.901183\n",
      "[94,   30] loss 7.959987\n",
      "[94,   40] loss 11.898127\n",
      "[94,   50] loss 11.173445\n",
      "[94,   60] loss 10.885877\n",
      "[94,   70] loss 9.661815\n",
      "[94,   80] loss 9.532463\n",
      "[94,   90] loss 12.099145\n",
      "[94,  100] loss 9.220520\n",
      "[94,  110] loss 9.230259\n",
      "[94,  120] loss 12.527675\n",
      "[94,  130] loss 12.171243\n",
      "[94,  140] loss 10.825891\n",
      "[94,  150] loss 11.174340\n",
      "[94,  160] loss 9.999093\n",
      "[95,   10] loss 9.612779\n",
      "[95,   20] loss 10.409470\n",
      "[95,   30] loss 10.394617\n",
      "[95,   40] loss 11.031633\n",
      "[95,   50] loss 11.458702\n",
      "[95,   60] loss 8.386837\n",
      "[95,   70] loss 11.649094\n",
      "[95,   80] loss 10.009628\n",
      "[95,   90] loss 11.823685\n",
      "[95,  100] loss 10.079875\n",
      "[95,  110] loss 9.929576\n",
      "[95,  120] loss 10.878629\n",
      "[95,  130] loss 10.660805\n",
      "[95,  140] loss 13.025502\n",
      "[95,  150] loss 10.667650\n",
      "[95,  160] loss 9.017003\n",
      "[96,   10] loss 10.149375\n",
      "[96,   20] loss 11.368060\n",
      "[96,   30] loss 9.096448\n",
      "[96,   40] loss 11.920337\n",
      "[96,   50] loss 10.041953\n",
      "[96,   60] loss 10.481562\n",
      "[96,   70] loss 9.859129\n",
      "[96,   80] loss 12.498808\n",
      "[96,   90] loss 10.066202\n",
      "[96,  100] loss 8.641535\n",
      "[96,  110] loss 10.290632\n",
      "[96,  120] loss 11.808352\n",
      "[96,  130] loss 10.299261\n",
      "[96,  140] loss 10.182855\n",
      "[96,  150] loss 14.555699\n",
      "[96,  160] loss 9.570909\n",
      "[97,   10] loss 10.753821\n",
      "[97,   20] loss 11.055314\n",
      "[97,   30] loss 11.586762\n",
      "[97,   40] loss 9.729275\n",
      "[97,   50] loss 9.618670\n",
      "[97,   60] loss 9.856901\n",
      "[97,   70] loss 9.718146\n",
      "[97,   80] loss 9.298386\n",
      "[97,   90] loss 10.623861\n",
      "[97,  100] loss 11.794878\n",
      "[97,  110] loss 10.903832\n",
      "[97,  120] loss 10.774703\n",
      "[97,  130] loss 11.941865\n",
      "[97,  140] loss 11.268709\n",
      "[97,  150] loss 9.329895\n",
      "[97,  160] loss 11.846778\n",
      "[98,   10] loss 12.962896\n",
      "[98,   20] loss 9.466669\n",
      "[98,   30] loss 12.233426\n",
      "[98,   40] loss 11.673656\n",
      "[98,   50] loss 10.567368\n",
      "[98,   60] loss 10.001357\n",
      "[98,   70] loss 10.400480\n",
      "[98,   80] loss 8.889406\n",
      "[98,   90] loss 12.803399\n",
      "[98,  100] loss 11.499260\n",
      "[98,  110] loss 10.996896\n",
      "[98,  120] loss 10.645389\n",
      "[98,  130] loss 12.073626\n",
      "[98,  140] loss 10.715469\n",
      "[98,  150] loss 8.932455\n",
      "[98,  160] loss 8.108162\n",
      "[99,   10] loss 12.437842\n",
      "[99,   20] loss 9.468918\n",
      "[99,   30] loss 11.957707\n",
      "[99,   40] loss 8.685991\n",
      "[99,   50] loss 11.766667\n",
      "[99,   60] loss 11.570560\n",
      "[99,   70] loss 10.934413\n",
      "[99,   80] loss 9.651681\n",
      "[99,   90] loss 10.896227\n",
      "[99,  100] loss 10.849418\n",
      "[99,  110] loss 8.956070\n",
      "[99,  120] loss 11.150940\n",
      "[99,  130] loss 10.676837\n",
      "[99,  140] loss 10.837107\n",
      "[99,  150] loss 12.227031\n",
      "[99,  160] loss 7.858517\n",
      "[100,   10] loss 10.684916\n",
      "[100,   20] loss 8.978997\n",
      "[100,   30] loss 12.379336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100,   40] loss 8.671435\n",
      "[100,   50] loss 11.041845\n",
      "[100,   60] loss 10.236699\n",
      "[100,   70] loss 10.400208\n",
      "[100,   80] loss 10.539080\n",
      "[100,   90] loss 10.531755\n",
      "[100,  100] loss 8.575422\n",
      "[100,  110] loss 9.922001\n",
      "[100,  120] loss 12.102042\n",
      "[100,  130] loss 10.188167\n",
      "[100,  140] loss 10.793706\n",
      "[100,  150] loss 11.024969\n",
      "[100,  160] loss 11.598377\n",
      "[101,   10] loss 10.011305\n",
      "[101,   20] loss 10.003129\n",
      "[101,   30] loss 10.300688\n",
      "[101,   40] loss 9.707080\n",
      "[101,   50] loss 11.018497\n",
      "[101,   60] loss 12.437722\n",
      "[101,   70] loss 10.212778\n",
      "[101,   80] loss 10.795215\n",
      "[101,   90] loss 11.325923\n",
      "[101,  100] loss 10.681365\n",
      "[101,  110] loss 9.372435\n",
      "[101,  120] loss 11.589939\n",
      "[101,  130] loss 9.282770\n",
      "[101,  140] loss 12.207607\n",
      "[101,  150] loss 9.437719\n",
      "[101,  160] loss 10.431557\n",
      "[102,   10] loss 11.457627\n",
      "[102,   20] loss 11.459150\n",
      "[102,   30] loss 10.444733\n",
      "[102,   40] loss 8.929128\n",
      "[102,   50] loss 9.381486\n",
      "[102,   60] loss 10.531157\n",
      "[102,   70] loss 9.370999\n",
      "[102,   80] loss 9.650943\n",
      "[102,   90] loss 11.885207\n",
      "[102,  100] loss 9.271118\n",
      "[102,  110] loss 12.624492\n",
      "[102,  120] loss 10.072171\n",
      "[102,  130] loss 10.784296\n",
      "[102,  140] loss 12.381587\n",
      "[102,  150] loss 10.643200\n",
      "[102,  160] loss 11.223735\n",
      "[103,   10] loss 10.323672\n",
      "[103,   20] loss 9.510737\n",
      "[103,   30] loss 11.453878\n",
      "[103,   40] loss 9.910414\n",
      "[103,   50] loss 9.611227\n",
      "[103,   60] loss 10.481363\n",
      "[103,   70] loss 8.667782\n",
      "[103,   80] loss 8.796912\n",
      "[103,   90] loss 12.595175\n",
      "[103,  100] loss 11.431567\n",
      "[103,  110] loss 11.663052\n",
      "[103,  120] loss 9.791775\n",
      "[103,  130] loss 11.727791\n",
      "[103,  140] loss 9.177526\n",
      "[103,  150] loss 10.549977\n",
      "[103,  160] loss 9.977353\n",
      "[104,   10] loss 9.423926\n",
      "[104,   20] loss 11.165384\n",
      "[104,   30] loss 8.646976\n",
      "[104,   40] loss 12.282015\n",
      "[104,   50] loss 10.995872\n",
      "[104,   60] loss 10.475400\n",
      "[104,   70] loss 10.586017\n",
      "[104,   80] loss 11.082321\n",
      "[104,   90] loss 12.271106\n",
      "[104,  100] loss 8.917572\n",
      "[104,  110] loss 8.792661\n",
      "[104,  120] loss 7.637166\n",
      "[104,  130] loss 11.431708\n",
      "[104,  140] loss 10.542061\n",
      "[104,  150] loss 9.875638\n",
      "[104,  160] loss 11.706605\n",
      "[105,   10] loss 10.360394\n",
      "[105,   20] loss 9.038962\n",
      "[105,   30] loss 9.976203\n",
      "[105,   40] loss 11.307494\n",
      "[105,   50] loss 9.906173\n",
      "[105,   60] loss 11.374442\n",
      "[105,   70] loss 11.721243\n",
      "[105,   80] loss 10.986386\n",
      "[105,   90] loss 9.886936\n",
      "[105,  100] loss 10.997233\n",
      "[105,  110] loss 10.232844\n",
      "[105,  120] loss 8.952929\n",
      "[105,  130] loss 9.943450\n",
      "[105,  140] loss 8.399911\n",
      "[105,  150] loss 10.665045\n",
      "[105,  160] loss 10.124762\n",
      "[106,   10] loss 12.444397\n",
      "[106,   20] loss 10.386170\n",
      "[106,   30] loss 10.418030\n",
      "[106,   40] loss 11.583148\n",
      "[106,   50] loss 10.305642\n",
      "[106,   60] loss 10.285684\n",
      "[106,   70] loss 11.176672\n",
      "[106,   80] loss 11.979136\n",
      "[106,   90] loss 11.147985\n",
      "[106,  100] loss 9.714041\n",
      "[106,  110] loss 10.598079\n",
      "[106,  120] loss 10.854471\n",
      "[106,  130] loss 8.927938\n",
      "[106,  140] loss 10.562275\n",
      "[106,  150] loss 9.503668\n",
      "[106,  160] loss 10.655660\n",
      "[107,   10] loss 10.486774\n",
      "[107,   20] loss 9.184321\n",
      "[107,   30] loss 10.573254\n",
      "[107,   40] loss 10.550821\n",
      "[107,   50] loss 9.810727\n",
      "[107,   60] loss 9.820307\n",
      "[107,   70] loss 11.699868\n",
      "[107,   80] loss 9.793835\n",
      "[107,   90] loss 9.949812\n",
      "[107,  100] loss 9.556827\n",
      "[107,  110] loss 10.699743\n",
      "[107,  120] loss 11.094679\n",
      "[107,  130] loss 10.055053\n",
      "[107,  140] loss 10.028404\n",
      "[107,  150] loss 9.934844\n",
      "[107,  160] loss 12.049006\n",
      "[108,   10] loss 9.410839\n",
      "[108,   20] loss 12.174532\n",
      "[108,   30] loss 10.455538\n",
      "[108,   40] loss 9.112496\n",
      "[108,   50] loss 9.569934\n",
      "[108,   60] loss 10.043341\n",
      "[108,   70] loss 10.187700\n",
      "[108,   80] loss 10.367246\n",
      "[108,   90] loss 10.533534\n",
      "[108,  100] loss 11.546577\n",
      "[108,  110] loss 11.074198\n",
      "[108,  120] loss 9.757991\n",
      "[108,  130] loss 11.103673\n",
      "[108,  140] loss 9.259730\n",
      "[108,  150] loss 11.084573\n",
      "[108,  160] loss 9.992011\n",
      "[109,   10] loss 10.740719\n",
      "[109,   20] loss 10.802952\n",
      "[109,   30] loss 9.091402\n",
      "[109,   40] loss 10.876143\n",
      "[109,   50] loss 9.727323\n",
      "[109,   60] loss 10.721127\n",
      "[109,   70] loss 10.061078\n",
      "[109,   80] loss 13.618123\n",
      "[109,   90] loss 11.401432\n",
      "[109,  100] loss 9.472076\n",
      "[109,  110] loss 8.728667\n",
      "[109,  120] loss 9.720020\n",
      "[109,  130] loss 9.984844\n",
      "[109,  140] loss 10.765881\n",
      "[109,  150] loss 11.102408\n",
      "[109,  160] loss 9.976774\n",
      "[110,   10] loss 10.640053\n",
      "[110,   20] loss 9.834295\n",
      "[110,   30] loss 10.879053\n",
      "[110,   40] loss 9.330923\n",
      "[110,   50] loss 9.860084\n",
      "[110,   60] loss 10.521310\n",
      "[110,   70] loss 9.569561\n",
      "[110,   80] loss 10.909564\n",
      "[110,   90] loss 9.603515\n",
      "[110,  100] loss 11.252020\n",
      "[110,  110] loss 9.768602\n",
      "[110,  120] loss 10.098872\n",
      "[110,  130] loss 9.296338\n",
      "[110,  140] loss 9.389369\n",
      "[110,  150] loss 11.225222\n",
      "[110,  160] loss 11.222661\n",
      "[111,   10] loss 8.104337\n",
      "[111,   20] loss 11.264386\n",
      "[111,   30] loss 11.353785\n",
      "[111,   40] loss 9.846750\n",
      "[111,   50] loss 9.937474\n",
      "[111,   60] loss 10.487606\n",
      "[111,   70] loss 10.442615\n",
      "[111,   80] loss 8.287351\n",
      "[111,   90] loss 11.029948\n",
      "[111,  100] loss 8.592467\n",
      "[111,  110] loss 10.921944\n",
      "[111,  120] loss 10.216394\n",
      "[111,  130] loss 10.006269\n",
      "[111,  140] loss 8.235927\n",
      "[111,  150] loss 10.711472\n",
      "[111,  160] loss 11.211758\n",
      "[112,   10] loss 9.127581\n",
      "[112,   20] loss 10.497834\n",
      "[112,   30] loss 7.724705\n",
      "[112,   40] loss 9.296356\n",
      "[112,   50] loss 11.933527\n",
      "[112,   60] loss 8.975502\n",
      "[112,   70] loss 10.357226\n",
      "[112,   80] loss 10.656586\n",
      "[112,   90] loss 10.129467\n",
      "[112,  100] loss 9.158677\n",
      "[112,  110] loss 11.819378\n",
      "[112,  120] loss 10.345039\n",
      "[112,  130] loss 11.573198\n",
      "[112,  140] loss 11.430724\n",
      "[112,  150] loss 9.724655\n",
      "[112,  160] loss 8.432229\n",
      "[113,   10] loss 8.534107\n",
      "[113,   20] loss 9.568934\n",
      "[113,   30] loss 10.753084\n",
      "[113,   40] loss 9.355319\n",
      "[113,   50] loss 9.929073\n",
      "[113,   60] loss 10.139268\n",
      "[113,   70] loss 9.130764\n",
      "[113,   80] loss 9.891989\n",
      "[113,   90] loss 10.601247\n",
      "[113,  100] loss 10.518644\n",
      "[113,  110] loss 9.672236\n",
      "[113,  120] loss 11.581316\n",
      "[113,  130] loss 10.171459\n",
      "[113,  140] loss 11.376736\n",
      "[113,  150] loss 10.787979\n",
      "[113,  160] loss 11.671989\n",
      "[114,   10] loss 11.902322\n",
      "[114,   20] loss 9.393525\n",
      "[114,   30] loss 9.804522\n",
      "[114,   40] loss 11.159535\n",
      "[114,   50] loss 8.191323\n",
      "[114,   60] loss 10.489707\n",
      "[114,   70] loss 12.492252\n",
      "[114,   80] loss 9.722328\n",
      "[114,   90] loss 8.870685\n",
      "[114,  100] loss 10.417285\n",
      "[114,  110] loss 10.526950\n",
      "[114,  120] loss 10.365170\n",
      "[114,  130] loss 11.260325\n",
      "[114,  140] loss 9.022032\n",
      "[114,  150] loss 10.050754\n",
      "[114,  160] loss 9.563455\n",
      "[115,   10] loss 10.015533\n",
      "[115,   20] loss 10.725365\n",
      "[115,   30] loss 10.017593\n",
      "[115,   40] loss 8.189596\n",
      "[115,   50] loss 8.303316\n",
      "[115,   60] loss 11.603867\n",
      "[115,   70] loss 11.360430\n",
      "[115,   80] loss 10.495969\n",
      "[115,   90] loss 10.954088\n",
      "[115,  100] loss 9.200263\n",
      "[115,  110] loss 11.512214\n",
      "[115,  120] loss 12.125278\n",
      "[115,  130] loss 9.149424\n",
      "[115,  140] loss 9.349949\n",
      "[115,  150] loss 10.694334\n",
      "[115,  160] loss 7.420281\n",
      "[116,   10] loss 10.456184\n",
      "[116,   20] loss 9.566280\n",
      "[116,   30] loss 10.589405\n",
      "[116,   40] loss 8.680080\n",
      "[116,   50] loss 9.652200\n",
      "[116,   60] loss 9.855120\n",
      "[116,   70] loss 12.000464\n",
      "[116,   80] loss 9.437309\n",
      "[116,   90] loss 10.702784\n",
      "[116,  100] loss 10.329007\n",
      "[116,  110] loss 8.122742\n",
      "[116,  120] loss 12.719165\n",
      "[116,  130] loss 10.733099\n",
      "[116,  140] loss 9.859951\n",
      "[116,  150] loss 9.164476\n",
      "[116,  160] loss 9.658104\n",
      "[117,   10] loss 9.661423\n",
      "[117,   20] loss 10.511445\n",
      "[117,   30] loss 9.918141\n",
      "[117,   40] loss 9.335953\n",
      "[117,   50] loss 8.468945\n",
      "[117,   60] loss 8.701105\n",
      "[117,   70] loss 11.720535\n",
      "[117,   80] loss 10.177221\n",
      "[117,   90] loss 9.341519\n",
      "[117,  100] loss 11.339507\n",
      "[117,  110] loss 9.105941\n",
      "[117,  120] loss 10.097890\n",
      "[117,  130] loss 9.235222\n",
      "[117,  140] loss 9.775571\n",
      "[117,  150] loss 12.962154\n",
      "[117,  160] loss 10.358254\n",
      "[118,   10] loss 10.964532\n",
      "[118,   20] loss 10.797478\n",
      "[118,   30] loss 9.696524\n",
      "[118,   40] loss 11.121414\n",
      "[118,   50] loss 10.506918\n",
      "[118,   60] loss 9.875796\n",
      "[118,   70] loss 9.014331\n",
      "[118,   80] loss 9.869783\n",
      "[118,   90] loss 11.288973\n",
      "[118,  100] loss 9.798562\n",
      "[118,  110] loss 9.008105\n",
      "[118,  120] loss 9.826056\n",
      "[118,  130] loss 11.342214\n",
      "[118,  140] loss 9.736816\n",
      "[118,  150] loss 11.037201\n",
      "[118,  160] loss 9.543729\n",
      "[119,   10] loss 10.185933\n",
      "[119,   20] loss 8.572851\n",
      "[119,   30] loss 11.589064\n",
      "[119,   40] loss 8.637672\n",
      "[119,   50] loss 9.802373\n",
      "[119,   60] loss 8.951118\n",
      "[119,   70] loss 9.391330\n",
      "[119,   80] loss 11.860774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119,   90] loss 10.576392\n",
      "[119,  100] loss 11.068437\n",
      "[119,  110] loss 8.934952\n",
      "[119,  120] loss 10.753922\n",
      "[119,  130] loss 9.938462\n",
      "[119,  140] loss 9.897179\n",
      "[119,  150] loss 10.553375\n",
      "[119,  160] loss 9.902129\n",
      "[120,   10] loss 10.651322\n",
      "[120,   20] loss 9.216577\n",
      "[120,   30] loss 10.186345\n",
      "[120,   40] loss 11.658162\n",
      "[120,   50] loss 9.317589\n",
      "[120,   60] loss 8.349532\n",
      "[120,   70] loss 11.373828\n",
      "[120,   80] loss 9.794421\n",
      "[120,   90] loss 8.797920\n",
      "[120,  100] loss 10.461372\n",
      "[120,  110] loss 8.749183\n",
      "[120,  120] loss 8.048030\n",
      "[120,  130] loss 10.871261\n",
      "[120,  140] loss 7.777464\n",
      "[120,  150] loss 11.137135\n",
      "[120,  160] loss 10.007461\n",
      "[121,   10] loss 10.277908\n",
      "[121,   20] loss 10.317077\n",
      "[121,   30] loss 10.777619\n",
      "[121,   40] loss 10.208557\n",
      "[121,   50] loss 9.383306\n",
      "[121,   60] loss 8.596388\n",
      "[121,   70] loss 12.327589\n",
      "[121,   80] loss 9.460010\n",
      "[121,   90] loss 9.948478\n",
      "[121,  100] loss 9.248854\n",
      "[121,  110] loss 10.019310\n",
      "[121,  120] loss 9.478699\n",
      "[121,  130] loss 8.913736\n",
      "[121,  140] loss 9.662841\n",
      "[121,  150] loss 10.066749\n",
      "[121,  160] loss 10.458152\n",
      "[122,   10] loss 10.844313\n",
      "[122,   20] loss 10.809186\n",
      "[122,   30] loss 12.708236\n",
      "[122,   40] loss 11.310689\n",
      "[122,   50] loss 10.573490\n",
      "[122,   60] loss 9.311264\n",
      "[122,   70] loss 10.312212\n",
      "[122,   80] loss 11.034806\n",
      "[122,   90] loss 9.955393\n",
      "[122,  100] loss 8.655400\n",
      "[122,  110] loss 10.661149\n",
      "[122,  120] loss 9.070721\n",
      "[122,  130] loss 10.593776\n",
      "[122,  140] loss 9.388012\n",
      "[122,  150] loss 10.039205\n",
      "[122,  160] loss 10.828620\n",
      "[123,   10] loss 9.950858\n",
      "[123,   20] loss 10.852150\n",
      "[123,   30] loss 10.404252\n",
      "[123,   40] loss 10.015862\n",
      "[123,   50] loss 9.074815\n",
      "[123,   60] loss 9.387103\n",
      "[123,   70] loss 11.104194\n",
      "[123,   80] loss 10.265078\n",
      "[123,   90] loss 9.489596\n",
      "[123,  100] loss 10.856511\n",
      "[123,  110] loss 9.362000\n",
      "[123,  120] loss 10.297694\n",
      "[123,  130] loss 8.836523\n",
      "[123,  140] loss 9.900579\n",
      "[123,  150] loss 9.038336\n",
      "[123,  160] loss 9.802749\n",
      "[124,   10] loss 12.191920\n",
      "[124,   20] loss 9.927535\n",
      "[124,   30] loss 11.188192\n",
      "[124,   40] loss 7.874646\n",
      "[124,   50] loss 9.783942\n",
      "[124,   60] loss 10.435568\n",
      "[124,   70] loss 9.557821\n",
      "[124,   80] loss 10.015924\n",
      "[124,   90] loss 11.846745\n",
      "[124,  100] loss 10.985552\n",
      "[124,  110] loss 8.850718\n",
      "[124,  120] loss 11.141101\n",
      "[124,  130] loss 8.879851\n",
      "[124,  140] loss 10.098742\n",
      "[124,  150] loss 9.611755\n",
      "[124,  160] loss 9.469946\n",
      "[125,   10] loss 9.514728\n",
      "[125,   20] loss 10.336282\n",
      "[125,   30] loss 8.441013\n",
      "[125,   40] loss 8.085571\n",
      "[125,   50] loss 9.160704\n",
      "[125,   60] loss 10.271509\n",
      "[125,   70] loss 12.077503\n",
      "[125,   80] loss 9.367448\n",
      "[125,   90] loss 9.524461\n",
      "[125,  100] loss 8.459900\n",
      "[125,  110] loss 12.149705\n",
      "[125,  120] loss 10.220795\n",
      "[125,  130] loss 10.064038\n",
      "[125,  140] loss 10.998124\n",
      "[125,  150] loss 11.011582\n",
      "[125,  160] loss 10.132821\n",
      "[126,   10] loss 8.757702\n",
      "[126,   20] loss 9.563244\n",
      "[126,   30] loss 10.516134\n",
      "[126,   40] loss 10.229888\n",
      "[126,   50] loss 9.456412\n",
      "[126,   60] loss 10.246051\n",
      "[126,   70] loss 9.659114\n",
      "[126,   80] loss 9.561251\n",
      "[126,   90] loss 10.486485\n",
      "[126,  100] loss 9.356300\n",
      "[126,  110] loss 12.276727\n",
      "[126,  120] loss 10.482739\n",
      "[126,  130] loss 9.913802\n",
      "[126,  140] loss 10.786634\n",
      "[126,  150] loss 9.992041\n",
      "[126,  160] loss 9.298332\n",
      "[127,   10] loss 7.453128\n",
      "[127,   20] loss 9.319093\n",
      "[127,   30] loss 10.779014\n",
      "[127,   40] loss 9.330964\n",
      "[127,   50] loss 9.273031\n",
      "[127,   60] loss 8.289706\n",
      "[127,   70] loss 10.274597\n",
      "[127,   80] loss 9.825754\n",
      "[127,   90] loss 9.766915\n",
      "[127,  100] loss 10.081780\n",
      "[127,  110] loss 8.419500\n",
      "[127,  120] loss 10.256686\n",
      "[127,  130] loss 11.841075\n",
      "[127,  140] loss 10.338257\n",
      "[127,  150] loss 9.838985\n",
      "[127,  160] loss 9.125026\n",
      "[128,   10] loss 8.654278\n",
      "[128,   20] loss 11.790627\n",
      "[128,   30] loss 10.747023\n",
      "[128,   40] loss 12.914190\n",
      "[128,   50] loss 8.746873\n",
      "[128,   60] loss 9.593132\n",
      "[128,   70] loss 9.285080\n",
      "[128,   80] loss 10.933143\n",
      "[128,   90] loss 9.061524\n",
      "[128,  100] loss 7.608870\n",
      "[128,  110] loss 11.136020\n",
      "[128,  120] loss 8.166005\n",
      "[128,  130] loss 10.525925\n",
      "[128,  140] loss 9.996224\n",
      "[128,  150] loss 9.481280\n",
      "[128,  160] loss 9.636456\n",
      "[129,   10] loss 9.806239\n",
      "[129,   20] loss 10.498026\n",
      "[129,   30] loss 9.292918\n",
      "[129,   40] loss 11.594425\n",
      "[129,   50] loss 10.447527\n",
      "[129,   60] loss 10.681131\n",
      "[129,   70] loss 9.638724\n",
      "[129,   80] loss 10.364800\n",
      "[129,   90] loss 11.181648\n",
      "[129,  100] loss 9.962503\n",
      "[129,  110] loss 7.369815\n",
      "[129,  120] loss 9.768773\n",
      "[129,  130] loss 8.446724\n",
      "[129,  140] loss 9.891628\n",
      "[129,  150] loss 9.883832\n",
      "[129,  160] loss 9.342535\n",
      "[130,   10] loss 9.189472\n",
      "[130,   20] loss 8.429664\n",
      "[130,   30] loss 9.704044\n",
      "[130,   40] loss 8.722356\n",
      "[130,   50] loss 12.141091\n",
      "[130,   60] loss 10.466052\n",
      "[130,   70] loss 8.349667\n",
      "[130,   80] loss 8.895381\n",
      "[130,   90] loss 7.934515\n",
      "[130,  100] loss 9.214328\n",
      "[130,  110] loss 9.335215\n",
      "[130,  120] loss 9.900511\n",
      "[130,  130] loss 9.943948\n",
      "[130,  140] loss 9.992798\n",
      "[130,  150] loss 9.761341\n",
      "[130,  160] loss 9.746314\n",
      "[131,   10] loss 10.651714\n",
      "[131,   20] loss 10.246261\n",
      "[131,   30] loss 9.437192\n",
      "[131,   40] loss 10.556211\n",
      "[131,   50] loss 10.145167\n",
      "[131,   60] loss 10.278805\n",
      "[131,   70] loss 11.144669\n",
      "[131,   80] loss 10.027441\n",
      "[131,   90] loss 8.081291\n",
      "[131,  100] loss 10.867722\n",
      "[131,  110] loss 8.571000\n",
      "[131,  120] loss 10.175746\n",
      "[131,  130] loss 11.080226\n",
      "[131,  140] loss 8.346758\n",
      "[131,  150] loss 8.897987\n",
      "[131,  160] loss 11.193571\n",
      "[132,   10] loss 9.636436\n",
      "[132,   20] loss 8.730152\n",
      "[132,   30] loss 8.217133\n",
      "[132,   40] loss 9.655587\n",
      "[132,   50] loss 10.447424\n",
      "[132,   60] loss 9.174899\n",
      "[132,   70] loss 10.920677\n",
      "[132,   80] loss 8.671084\n",
      "[132,   90] loss 9.783407\n",
      "[132,  100] loss 11.313985\n",
      "[132,  110] loss 11.994858\n",
      "[132,  120] loss 8.719087\n",
      "[132,  130] loss 8.425175\n",
      "[132,  140] loss 10.274338\n",
      "[132,  150] loss 8.766174\n",
      "[132,  160] loss 8.918737\n",
      "[133,   10] loss 9.175557\n",
      "[133,   20] loss 8.494110\n",
      "[133,   30] loss 10.713207\n",
      "[133,   40] loss 10.016596\n",
      "[133,   50] loss 8.967567\n",
      "[133,   60] loss 8.768822\n",
      "[133,   70] loss 10.635156\n",
      "[133,   80] loss 8.916470\n",
      "[133,   90] loss 9.923807\n",
      "[133,  100] loss 11.775967\n",
      "[133,  110] loss 10.126757\n",
      "[133,  120] loss 8.683208\n",
      "[133,  130] loss 9.213553\n",
      "[133,  140] loss 9.904625\n",
      "[133,  150] loss 8.837733\n",
      "[133,  160] loss 8.622256\n",
      "[134,   10] loss 8.987536\n",
      "[134,   20] loss 8.444110\n",
      "[134,   30] loss 9.522187\n",
      "[134,   40] loss 10.875393\n",
      "[134,   50] loss 8.922276\n",
      "[134,   60] loss 10.249266\n",
      "[134,   70] loss 9.348763\n",
      "[134,   80] loss 8.877982\n",
      "[134,   90] loss 10.272405\n",
      "[134,  100] loss 10.022555\n",
      "[134,  110] loss 8.712361\n",
      "[134,  120] loss 10.224360\n",
      "[134,  130] loss 9.237596\n",
      "[134,  140] loss 10.460496\n",
      "[134,  150] loss 9.575093\n",
      "[134,  160] loss 8.756846\n",
      "[135,   10] loss 8.773207\n",
      "[135,   20] loss 10.404946\n",
      "[135,   30] loss 9.214271\n",
      "[135,   40] loss 9.539998\n",
      "[135,   50] loss 9.357153\n",
      "[135,   60] loss 10.069400\n",
      "[135,   70] loss 10.816834\n",
      "[135,   80] loss 9.119391\n",
      "[135,   90] loss 8.629946\n",
      "[135,  100] loss 11.158524\n",
      "[135,  110] loss 9.464461\n",
      "[135,  120] loss 10.496439\n",
      "[135,  130] loss 9.266496\n",
      "[135,  140] loss 8.612851\n",
      "[135,  150] loss 10.558810\n",
      "[135,  160] loss 10.662162\n",
      "[136,   10] loss 10.452591\n",
      "[136,   20] loss 10.417154\n",
      "[136,   30] loss 8.165027\n",
      "[136,   40] loss 9.473631\n",
      "[136,   50] loss 11.717731\n",
      "[136,   60] loss 8.490686\n",
      "[136,   70] loss 11.222943\n",
      "[136,   80] loss 9.744450\n",
      "[136,   90] loss 9.311249\n",
      "[136,  100] loss 9.788923\n",
      "[136,  110] loss 10.734045\n",
      "[136,  120] loss 7.269172\n",
      "[136,  130] loss 10.146595\n",
      "[136,  140] loss 10.315753\n",
      "[136,  150] loss 10.282147\n",
      "[136,  160] loss 8.828163\n",
      "[137,   10] loss 9.746320\n",
      "[137,   20] loss 7.849013\n",
      "[137,   30] loss 10.306718\n",
      "[137,   40] loss 9.060869\n",
      "[137,   50] loss 10.100922\n",
      "[137,   60] loss 9.676383\n",
      "[137,   70] loss 8.442968\n",
      "[137,   80] loss 10.646207\n",
      "[137,   90] loss 11.571689\n",
      "[137,  100] loss 11.102458\n",
      "[137,  110] loss 8.372997\n",
      "[137,  120] loss 8.551140\n",
      "[137,  130] loss 8.957889\n",
      "[137,  140] loss 9.691026\n",
      "[137,  150] loss 10.241277\n",
      "[137,  160] loss 10.936346\n",
      "[138,   10] loss 7.646968\n",
      "[138,   20] loss 10.904743\n",
      "[138,   30] loss 8.399961\n",
      "[138,   40] loss 10.095419\n",
      "[138,   50] loss 10.192468\n",
      "[138,   60] loss 8.960305\n",
      "[138,   70] loss 8.674947\n",
      "[138,   80] loss 8.795636\n",
      "[138,   90] loss 10.229421\n",
      "[138,  100] loss 8.984036\n",
      "[138,  110] loss 10.085433\n",
      "[138,  120] loss 9.907116\n",
      "[138,  130] loss 10.295357\n",
      "[138,  140] loss 10.195891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138,  150] loss 10.002051\n",
      "[138,  160] loss 8.905482\n",
      "[139,   10] loss 9.142079\n",
      "[139,   20] loss 9.222300\n",
      "[139,   30] loss 10.305054\n",
      "[139,   40] loss 9.922074\n",
      "[139,   50] loss 8.545458\n",
      "[139,   60] loss 8.451187\n",
      "[139,   70] loss 12.443355\n",
      "[139,   80] loss 8.938810\n",
      "[139,   90] loss 8.783118\n",
      "[139,  100] loss 8.972540\n",
      "[139,  110] loss 9.072387\n",
      "[139,  120] loss 8.513771\n",
      "[139,  130] loss 8.235881\n",
      "[139,  140] loss 11.129411\n",
      "[139,  150] loss 8.995359\n",
      "[139,  160] loss 9.807585\n",
      "[140,   10] loss 6.728956\n",
      "[140,   20] loss 8.664526\n",
      "[140,   30] loss 11.167829\n",
      "[140,   40] loss 9.645916\n",
      "[140,   50] loss 10.823834\n",
      "[140,   60] loss 8.929448\n",
      "[140,   70] loss 10.641204\n",
      "[140,   80] loss 9.715105\n",
      "[140,   90] loss 10.427501\n",
      "[140,  100] loss 9.220746\n",
      "[140,  110] loss 10.016709\n",
      "[140,  120] loss 7.609062\n",
      "[140,  130] loss 9.013094\n",
      "[140,  140] loss 9.875535\n",
      "[140,  150] loss 8.944752\n",
      "[140,  160] loss 9.996567\n",
      "[141,   10] loss 8.940572\n",
      "[141,   20] loss 11.242865\n",
      "[141,   30] loss 10.315170\n",
      "[141,   40] loss 8.807459\n",
      "[141,   50] loss 9.474471\n",
      "[141,   60] loss 10.020092\n",
      "[141,   70] loss 9.119920\n",
      "[141,   80] loss 10.574648\n",
      "[141,   90] loss 10.029846\n",
      "[141,  100] loss 9.412760\n",
      "[141,  110] loss 8.283177\n",
      "[141,  120] loss 9.382456\n",
      "[141,  130] loss 8.655803\n",
      "[141,  140] loss 8.558440\n",
      "[141,  150] loss 8.461295\n",
      "[141,  160] loss 8.803050\n",
      "[142,   10] loss 9.497059\n",
      "[142,   20] loss 8.899985\n",
      "[142,   30] loss 9.550997\n",
      "[142,   40] loss 9.716066\n",
      "[142,   50] loss 9.562625\n",
      "[142,   60] loss 10.029441\n",
      "[142,   70] loss 8.675352\n",
      "[142,   80] loss 8.474280\n",
      "[142,   90] loss 8.928731\n",
      "[142,  100] loss 7.710842\n",
      "[142,  110] loss 9.590354\n",
      "[142,  120] loss 10.511054\n",
      "[142,  130] loss 10.950843\n",
      "[142,  140] loss 8.734888\n",
      "[142,  150] loss 10.956607\n",
      "[142,  160] loss 10.620237\n",
      "[143,   10] loss 10.785693\n",
      "[143,   20] loss 10.137810\n",
      "[143,   30] loss 10.360041\n",
      "[143,   40] loss 10.191709\n",
      "[143,   50] loss 9.763162\n",
      "[143,   60] loss 10.394087\n",
      "[143,   70] loss 10.171099\n",
      "[143,   80] loss 8.812187\n",
      "[143,   90] loss 11.145214\n",
      "[143,  100] loss 7.907965\n",
      "[143,  110] loss 8.438355\n",
      "[143,  120] loss 9.340362\n",
      "[143,  130] loss 9.640874\n",
      "[143,  140] loss 11.486868\n",
      "[143,  150] loss 8.609723\n",
      "[143,  160] loss 9.899413\n",
      "[144,   10] loss 8.931614\n",
      "[144,   20] loss 7.951011\n",
      "[144,   30] loss 9.444787\n",
      "[144,   40] loss 8.545857\n",
      "[144,   50] loss 10.307061\n",
      "[144,   60] loss 9.451979\n",
      "[144,   70] loss 8.953169\n",
      "[144,   80] loss 11.047471\n",
      "[144,   90] loss 9.189049\n",
      "[144,  100] loss 9.241413\n",
      "[144,  110] loss 9.890685\n",
      "[144,  120] loss 8.652340\n",
      "[144,  130] loss 7.865191\n",
      "[144,  140] loss 11.605944\n",
      "[144,  150] loss 9.627141\n",
      "[144,  160] loss 9.884436\n",
      "[145,   10] loss 9.231078\n",
      "[145,   20] loss 10.662970\n",
      "[145,   30] loss 8.642535\n",
      "[145,   40] loss 8.420352\n",
      "[145,   50] loss 8.176664\n",
      "[145,   60] loss 8.509811\n",
      "[145,   70] loss 9.966609\n",
      "[145,   80] loss 8.177235\n",
      "[145,   90] loss 9.543295\n",
      "[145,  100] loss 9.563676\n",
      "[145,  110] loss 11.682957\n",
      "[145,  120] loss 8.759381\n",
      "[145,  130] loss 9.698637\n",
      "[145,  140] loss 11.588142\n",
      "[145,  150] loss 9.762800\n",
      "[145,  160] loss 10.523783\n",
      "[146,   10] loss 10.031260\n",
      "[146,   20] loss 7.743114\n",
      "[146,   30] loss 9.161685\n",
      "[146,   40] loss 7.747390\n",
      "[146,   50] loss 8.911818\n",
      "[146,   60] loss 10.759274\n",
      "[146,   70] loss 9.942348\n",
      "[146,   80] loss 10.989153\n",
      "[146,   90] loss 8.661741\n",
      "[146,  100] loss 8.709650\n",
      "[146,  110] loss 10.194308\n",
      "[146,  120] loss 10.717980\n",
      "[146,  130] loss 8.192817\n",
      "[146,  140] loss 7.692116\n",
      "[146,  150] loss 9.701600\n",
      "[146,  160] loss 11.519392\n",
      "[147,   10] loss 10.016747\n",
      "[147,   20] loss 8.192586\n",
      "[147,   30] loss 7.440599\n",
      "[147,   40] loss 11.798385\n",
      "[147,   50] loss 8.923973\n",
      "[147,   60] loss 9.897257\n",
      "[147,   70] loss 10.298200\n",
      "[147,   80] loss 8.789172\n",
      "[147,   90] loss 8.565571\n",
      "[147,  100] loss 10.698372\n",
      "[147,  110] loss 8.459038\n",
      "[147,  120] loss 9.141008\n",
      "[147,  130] loss 9.925815\n",
      "[147,  140] loss 10.361235\n",
      "[147,  150] loss 9.721160\n",
      "[147,  160] loss 8.675382\n",
      "[148,   10] loss 8.317914\n",
      "[148,   20] loss 9.181013\n",
      "[148,   30] loss 9.801947\n",
      "[148,   40] loss 9.065440\n",
      "[148,   50] loss 10.219114\n",
      "[148,   60] loss 7.893545\n",
      "[148,   70] loss 9.755000\n",
      "[148,   80] loss 10.336192\n",
      "[148,   90] loss 8.661403\n",
      "[148,  100] loss 8.761375\n",
      "[148,  110] loss 8.755321\n",
      "[148,  120] loss 9.273844\n",
      "[148,  130] loss 8.961237\n",
      "[148,  140] loss 8.998558\n",
      "[148,  150] loss 8.701608\n",
      "[148,  160] loss 9.919121\n",
      "[149,   10] loss 10.269087\n",
      "[149,   20] loss 7.535912\n",
      "[149,   30] loss 7.637444\n",
      "[149,   40] loss 9.287769\n",
      "[149,   50] loss 9.748504\n",
      "[149,   60] loss 10.089478\n",
      "[149,   70] loss 10.679752\n",
      "[149,   80] loss 9.529230\n",
      "[149,   90] loss 9.868816\n",
      "[149,  100] loss 9.148254\n",
      "[149,  110] loss 9.657366\n",
      "[149,  120] loss 10.122421\n",
      "[149,  130] loss 9.116903\n",
      "[149,  140] loss 9.893518\n",
      "[149,  150] loss 9.579941\n",
      "[149,  160] loss 7.883015\n",
      "[150,   10] loss 9.048774\n",
      "[150,   20] loss 10.675819\n",
      "[150,   30] loss 10.468161\n",
      "[150,   40] loss 8.914226\n",
      "[150,   50] loss 10.055429\n",
      "[150,   60] loss 9.876606\n",
      "[150,   70] loss 7.287617\n",
      "[150,   80] loss 9.209166\n",
      "[150,   90] loss 10.027652\n",
      "[150,  100] loss 9.103137\n",
      "[150,  110] loss 8.280024\n",
      "[150,  120] loss 11.098334\n",
      "[150,  130] loss 7.185112\n",
      "[150,  140] loss 6.558976\n",
      "[150,  150] loss 10.122792\n",
      "[150,  160] loss 7.745757\n",
      "[151,   10] loss 8.782912\n",
      "[151,   20] loss 7.441738\n",
      "[151,   30] loss 10.120255\n",
      "[151,   40] loss 10.428679\n",
      "[151,   50] loss 10.183942\n",
      "[151,   60] loss 8.408534\n",
      "[151,   70] loss 8.663501\n",
      "[151,   80] loss 9.478656\n",
      "[151,   90] loss 9.689791\n",
      "[151,  100] loss 9.458380\n",
      "[151,  110] loss 9.863186\n",
      "[151,  120] loss 9.472328\n",
      "[151,  130] loss 10.049122\n",
      "[151,  140] loss 8.951062\n",
      "[151,  150] loss 7.509497\n",
      "[151,  160] loss 9.496917\n",
      "[152,   10] loss 9.718755\n",
      "[152,   20] loss 10.181100\n",
      "[152,   30] loss 7.232933\n",
      "[152,   40] loss 9.947223\n",
      "[152,   50] loss 9.114400\n",
      "[152,   60] loss 11.441812\n",
      "[152,   70] loss 8.942960\n",
      "[152,   80] loss 7.004109\n",
      "[152,   90] loss 8.826827\n",
      "[152,  100] loss 11.658265\n",
      "[152,  110] loss 7.431541\n",
      "[152,  120] loss 9.237372\n",
      "[152,  130] loss 6.798198\n",
      "[152,  140] loss 8.552166\n",
      "[152,  150] loss 10.655380\n",
      "[152,  160] loss 8.257654\n",
      "[153,   10] loss 11.656517\n",
      "[153,   20] loss 9.479837\n",
      "[153,   30] loss 8.160349\n",
      "[153,   40] loss 7.409607\n",
      "[153,   50] loss 9.035082\n",
      "[153,   60] loss 8.342623\n",
      "[153,   70] loss 10.293129\n",
      "[153,   80] loss 10.918750\n",
      "[153,   90] loss 9.833427\n",
      "[153,  100] loss 9.910101\n",
      "[153,  110] loss 9.187658\n",
      "[153,  120] loss 8.455514\n",
      "[153,  130] loss 8.782592\n",
      "[153,  140] loss 8.985618\n",
      "[153,  150] loss 9.196272\n",
      "[153,  160] loss 9.038827\n",
      "[154,   10] loss 9.393758\n",
      "[154,   20] loss 9.275757\n",
      "[154,   30] loss 10.043657\n",
      "[154,   40] loss 9.735745\n",
      "[154,   50] loss 9.455751\n",
      "[154,   60] loss 7.287606\n",
      "[154,   70] loss 9.146192\n",
      "[154,   80] loss 9.742015\n",
      "[154,   90] loss 9.849386\n",
      "[154,  100] loss 8.262508\n",
      "[154,  110] loss 9.125176\n",
      "[154,  120] loss 7.354013\n",
      "[154,  130] loss 9.887002\n",
      "[154,  140] loss 8.322365\n",
      "[154,  150] loss 8.565684\n",
      "[154,  160] loss 10.340482\n",
      "[155,   10] loss 8.792168\n",
      "[155,   20] loss 9.619411\n",
      "[155,   30] loss 8.060902\n",
      "[155,   40] loss 11.324820\n",
      "[155,   50] loss 8.368031\n",
      "[155,   60] loss 10.664065\n",
      "[155,   70] loss 8.422183\n",
      "[155,   80] loss 8.964976\n",
      "[155,   90] loss 9.565727\n",
      "[155,  100] loss 8.100504\n",
      "[155,  110] loss 10.012240\n",
      "[155,  120] loss 7.757627\n",
      "[155,  130] loss 9.835882\n",
      "[155,  140] loss 8.667991\n",
      "[155,  150] loss 10.376562\n",
      "[155,  160] loss 9.469725\n",
      "[156,   10] loss 7.421302\n",
      "[156,   20] loss 9.679754\n",
      "[156,   30] loss 9.730902\n",
      "[156,   40] loss 10.914893\n",
      "[156,   50] loss 8.479047\n",
      "[156,   60] loss 8.683093\n",
      "[156,   70] loss 9.019448\n",
      "[156,   80] loss 9.260295\n",
      "[156,   90] loss 8.358708\n",
      "[156,  100] loss 8.299048\n",
      "[156,  110] loss 10.422080\n",
      "[156,  120] loss 8.897980\n",
      "[156,  130] loss 8.994009\n",
      "[156,  140] loss 8.139850\n",
      "[156,  150] loss 8.079824\n",
      "[156,  160] loss 10.491843\n",
      "[157,   10] loss 10.156187\n",
      "[157,   20] loss 9.457549\n",
      "[157,   30] loss 8.176953\n",
      "[157,   40] loss 8.166794\n",
      "[157,   50] loss 8.818636\n",
      "[157,   60] loss 9.224769\n",
      "[157,   70] loss 11.583773\n",
      "[157,   80] loss 8.744314\n",
      "[157,   90] loss 8.418527\n",
      "[157,  100] loss 10.952480\n",
      "[157,  110] loss 8.716257\n",
      "[157,  120] loss 9.218318\n",
      "[157,  130] loss 8.362703\n",
      "[157,  140] loss 8.491301\n",
      "[157,  150] loss 9.277775\n",
      "[157,  160] loss 8.346347\n",
      "[158,   10] loss 11.264110\n",
      "[158,   20] loss 8.770090\n",
      "[158,   30] loss 9.707817\n",
      "[158,   40] loss 9.325253\n",
      "[158,   50] loss 11.210071\n",
      "[158,   60] loss 8.545003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158,   70] loss 8.324016\n",
      "[158,   80] loss 9.498942\n",
      "[158,   90] loss 9.858298\n",
      "[158,  100] loss 8.591129\n",
      "[158,  110] loss 8.216933\n",
      "[158,  120] loss 8.344152\n",
      "[158,  130] loss 9.273717\n",
      "[158,  140] loss 7.749646\n",
      "[158,  150] loss 9.487582\n",
      "[158,  160] loss 9.168487\n",
      "[159,   10] loss 9.371869\n",
      "[159,   20] loss 9.346002\n",
      "[159,   30] loss 8.764380\n",
      "[159,   40] loss 8.278126\n",
      "[159,   50] loss 9.602832\n",
      "[159,   60] loss 7.823578\n",
      "[159,   70] loss 9.522020\n",
      "[159,   80] loss 9.240379\n",
      "[159,   90] loss 8.719040\n",
      "[159,  100] loss 8.796643\n",
      "[159,  110] loss 8.908282\n",
      "[159,  120] loss 9.235514\n",
      "[159,  130] loss 10.563823\n",
      "[159,  140] loss 9.454334\n",
      "[159,  150] loss 10.171915\n",
      "[159,  160] loss 9.827206\n",
      "[160,   10] loss 9.011468\n",
      "[160,   20] loss 9.554752\n",
      "[160,   30] loss 7.529510\n",
      "[160,   40] loss 10.924004\n",
      "[160,   50] loss 7.946727\n",
      "[160,   60] loss 9.690063\n",
      "[160,   70] loss 10.564862\n",
      "[160,   80] loss 8.953710\n",
      "[160,   90] loss 10.347947\n",
      "[160,  100] loss 9.149051\n",
      "[160,  110] loss 8.080402\n",
      "[160,  120] loss 7.286139\n",
      "[160,  130] loss 8.009742\n",
      "[160,  140] loss 9.397266\n",
      "[160,  150] loss 7.932113\n",
      "[160,  160] loss 8.208479\n",
      "[161,   10] loss 8.083338\n",
      "[161,   20] loss 9.426414\n",
      "[161,   30] loss 7.313743\n",
      "[161,   40] loss 9.941438\n",
      "[161,   50] loss 11.188995\n",
      "[161,   60] loss 11.643164\n",
      "[161,   70] loss 9.677484\n",
      "[161,   80] loss 9.023088\n",
      "[161,   90] loss 8.327441\n",
      "[161,  100] loss 7.913767\n",
      "[161,  110] loss 10.289124\n",
      "[161,  120] loss 7.939990\n",
      "[161,  130] loss 8.007964\n",
      "[161,  140] loss 9.230628\n",
      "[161,  150] loss 7.209572\n",
      "[161,  160] loss 8.224820\n",
      "[162,   10] loss 9.012772\n",
      "[162,   20] loss 10.970577\n",
      "[162,   30] loss 9.477830\n",
      "[162,   40] loss 9.332830\n",
      "[162,   50] loss 8.300720\n",
      "[162,   60] loss 9.022252\n",
      "[162,   70] loss 8.826126\n",
      "[162,   80] loss 8.605800\n",
      "[162,   90] loss 9.240463\n",
      "[162,  100] loss 10.494249\n",
      "[162,  110] loss 8.930589\n",
      "[162,  120] loss 7.484099\n",
      "[162,  130] loss 9.068427\n",
      "[162,  140] loss 7.732649\n",
      "[162,  150] loss 10.325501\n",
      "[162,  160] loss 7.864856\n",
      "[163,   10] loss 9.817654\n",
      "[163,   20] loss 8.393572\n",
      "[163,   30] loss 9.787105\n",
      "[163,   40] loss 11.124610\n",
      "[163,   50] loss 9.510775\n",
      "[163,   60] loss 8.766207\n",
      "[163,   70] loss 9.509524\n",
      "[163,   80] loss 8.357071\n",
      "[163,   90] loss 7.754079\n",
      "[163,  100] loss 8.953760\n",
      "[163,  110] loss 8.311706\n",
      "[163,  120] loss 9.363905\n",
      "[163,  130] loss 9.216006\n",
      "[163,  140] loss 7.658315\n",
      "[163,  150] loss 8.415541\n",
      "[163,  160] loss 7.739040\n",
      "[164,   10] loss 8.427988\n",
      "[164,   20] loss 11.487508\n",
      "[164,   30] loss 8.239162\n",
      "[164,   40] loss 9.095666\n",
      "[164,   50] loss 9.104977\n",
      "[164,   60] loss 10.285008\n",
      "[164,   70] loss 8.396514\n",
      "[164,   80] loss 8.231245\n",
      "[164,   90] loss 10.494435\n",
      "[164,  100] loss 6.998610\n",
      "[164,  110] loss 10.447168\n",
      "[164,  120] loss 8.793523\n",
      "[164,  130] loss 8.524926\n",
      "[164,  140] loss 6.666777\n",
      "[164,  150] loss 9.024233\n",
      "[164,  160] loss 8.684827\n",
      "[165,   10] loss 12.383421\n",
      "[165,   20] loss 9.699189\n",
      "[165,   30] loss 9.013539\n",
      "[165,   40] loss 8.534117\n",
      "[165,   50] loss 8.818186\n",
      "[165,   60] loss 9.079269\n",
      "[165,   70] loss 9.094483\n",
      "[165,   80] loss 9.277050\n",
      "[165,   90] loss 8.715674\n",
      "[165,  100] loss 8.805864\n",
      "[165,  110] loss 9.109150\n",
      "[165,  120] loss 7.978250\n",
      "[165,  130] loss 8.176978\n",
      "[165,  140] loss 7.610822\n",
      "[165,  150] loss 7.301375\n",
      "[165,  160] loss 8.950386\n",
      "[166,   10] loss 10.057651\n",
      "[166,   20] loss 9.496002\n",
      "[166,   30] loss 8.560270\n",
      "[166,   40] loss 8.061166\n",
      "[166,   50] loss 9.469516\n",
      "[166,   60] loss 8.284352\n",
      "[166,   70] loss 8.838479\n",
      "[166,   80] loss 9.400697\n",
      "[166,   90] loss 7.984968\n",
      "[166,  100] loss 8.924281\n",
      "[166,  110] loss 7.468587\n",
      "[166,  120] loss 8.728939\n",
      "[166,  130] loss 9.695098\n",
      "[166,  140] loss 9.963643\n",
      "[166,  150] loss 8.549465\n",
      "[166,  160] loss 9.208568\n",
      "[167,   10] loss 7.999321\n",
      "[167,   20] loss 7.783387\n",
      "[167,   30] loss 10.068966\n",
      "[167,   40] loss 10.142238\n",
      "[167,   50] loss 9.637982\n",
      "[167,   60] loss 8.118195\n",
      "[167,   70] loss 8.255741\n",
      "[167,   80] loss 10.315924\n",
      "[167,   90] loss 9.890271\n",
      "[167,  100] loss 9.773355\n",
      "[167,  110] loss 8.489577\n",
      "[167,  120] loss 10.095044\n",
      "[167,  130] loss 8.078602\n",
      "[167,  140] loss 6.809760\n",
      "[167,  150] loss 8.265588\n",
      "[167,  160] loss 6.499353\n",
      "[168,   10] loss 8.728342\n",
      "[168,   20] loss 11.186634\n",
      "[168,   30] loss 8.707508\n",
      "[168,   40] loss 8.072295\n",
      "[168,   50] loss 7.627014\n",
      "[168,   60] loss 9.356805\n",
      "[168,   70] loss 9.522895\n",
      "[168,   80] loss 10.117381\n",
      "[168,   90] loss 11.124945\n",
      "[168,  100] loss 9.671152\n",
      "[168,  110] loss 8.773340\n",
      "[168,  120] loss 8.170946\n",
      "[168,  130] loss 8.561478\n",
      "[168,  140] loss 8.732908\n",
      "[168,  150] loss 7.669214\n",
      "[168,  160] loss 7.705764\n",
      "[169,   10] loss 9.264858\n",
      "[169,   20] loss 9.752593\n",
      "[169,   30] loss 7.953786\n",
      "[169,   40] loss 8.353903\n",
      "[169,   50] loss 8.871393\n",
      "[169,   60] loss 7.933582\n",
      "[169,   70] loss 9.318124\n",
      "[169,   80] loss 8.709811\n",
      "[169,   90] loss 9.022586\n",
      "[169,  100] loss 8.906068\n",
      "[169,  110] loss 8.336303\n",
      "[169,  120] loss 10.512888\n",
      "[169,  130] loss 7.561504\n",
      "[169,  140] loss 10.183032\n",
      "[169,  150] loss 8.753171\n",
      "[169,  160] loss 8.035732\n",
      "[170,   10] loss 9.855243\n",
      "[170,   20] loss 7.768273\n",
      "[170,   30] loss 9.910071\n",
      "[170,   40] loss 9.773440\n",
      "[170,   50] loss 7.690990\n",
      "[170,   60] loss 9.492227\n",
      "[170,   70] loss 9.607559\n",
      "[170,   80] loss 8.685474\n",
      "[170,   90] loss 9.228475\n",
      "[170,  100] loss 9.199952\n",
      "[170,  110] loss 8.178192\n",
      "[170,  120] loss 8.885668\n",
      "[170,  130] loss 8.890492\n",
      "[170,  140] loss 7.760888\n",
      "[170,  150] loss 7.914843\n",
      "[170,  160] loss 9.112706\n",
      "[171,   10] loss 7.590149\n",
      "[171,   20] loss 9.334650\n",
      "[171,   30] loss 9.890906\n",
      "[171,   40] loss 8.228015\n",
      "[171,   50] loss 8.203911\n",
      "[171,   60] loss 8.993083\n",
      "[171,   70] loss 8.149461\n",
      "[171,   80] loss 8.116617\n",
      "[171,   90] loss 8.673971\n",
      "[171,  100] loss 9.710585\n",
      "[171,  110] loss 9.719508\n",
      "[171,  120] loss 9.185880\n",
      "[171,  130] loss 9.211275\n",
      "[171,  140] loss 8.524555\n",
      "[171,  150] loss 8.835106\n",
      "[171,  160] loss 9.467591\n",
      "[172,   10] loss 8.818545\n",
      "[172,   20] loss 8.465995\n",
      "[172,   30] loss 7.620895\n",
      "[172,   40] loss 9.045484\n",
      "[172,   50] loss 8.558595\n",
      "[172,   60] loss 9.520936\n",
      "[172,   70] loss 8.326743\n",
      "[172,   80] loss 7.804734\n",
      "[172,   90] loss 9.657850\n",
      "[172,  100] loss 9.340661\n",
      "[172,  110] loss 9.924118\n",
      "[172,  120] loss 8.827798\n",
      "[172,  130] loss 9.488477\n",
      "[172,  140] loss 8.359406\n",
      "[172,  150] loss 10.555270\n",
      "[172,  160] loss 8.100570\n",
      "[173,   10] loss 8.644391\n",
      "[173,   20] loss 8.637861\n",
      "[173,   30] loss 7.798694\n",
      "[173,   40] loss 8.691324\n",
      "[173,   50] loss 9.349300\n",
      "[173,   60] loss 7.744978\n",
      "[173,   70] loss 9.276575\n",
      "[173,   80] loss 9.776560\n",
      "[173,   90] loss 8.518194\n",
      "[173,  100] loss 8.052487\n",
      "[173,  110] loss 11.518659\n",
      "[173,  120] loss 6.922209\n",
      "[173,  130] loss 9.021538\n",
      "[173,  140] loss 8.720557\n",
      "[173,  150] loss 8.474922\n",
      "[173,  160] loss 7.558627\n",
      "[174,   10] loss 9.457095\n",
      "[174,   20] loss 8.137660\n",
      "[174,   30] loss 8.548696\n",
      "[174,   40] loss 8.701686\n",
      "[174,   50] loss 8.037216\n",
      "[174,   60] loss 9.818973\n",
      "[174,   70] loss 11.307572\n",
      "[174,   80] loss 7.765449\n",
      "[174,   90] loss 8.986263\n",
      "[174,  100] loss 8.542960\n",
      "[174,  110] loss 7.611982\n",
      "[174,  120] loss 7.906361\n",
      "[174,  130] loss 9.403968\n",
      "[174,  140] loss 7.155149\n",
      "[174,  150] loss 9.352899\n",
      "[174,  160] loss 8.927725\n",
      "[175,   10] loss 8.421647\n",
      "[175,   20] loss 7.780974\n",
      "[175,   30] loss 8.355714\n",
      "[175,   40] loss 7.216530\n",
      "[175,   50] loss 8.558557\n",
      "[175,   60] loss 7.451457\n",
      "[175,   70] loss 9.234830\n",
      "[175,   80] loss 9.162710\n",
      "[175,   90] loss 9.486569\n",
      "[175,  100] loss 8.138268\n",
      "[175,  110] loss 9.386384\n",
      "[175,  120] loss 9.276368\n",
      "[175,  130] loss 7.961864\n",
      "[175,  140] loss 9.636909\n",
      "[175,  150] loss 10.883356\n",
      "[175,  160] loss 7.549711\n",
      "[176,   10] loss 9.365059\n",
      "[176,   20] loss 9.171619\n",
      "[176,   30] loss 8.860642\n",
      "[176,   40] loss 9.876965\n",
      "[176,   50] loss 7.941241\n",
      "[176,   60] loss 8.113705\n",
      "[176,   70] loss 7.698379\n",
      "[176,   80] loss 8.629702\n",
      "[176,   90] loss 10.996895\n",
      "[176,  100] loss 8.013849\n",
      "[176,  110] loss 7.967576\n",
      "[176,  120] loss 7.909792\n",
      "[176,  130] loss 8.298169\n",
      "[176,  140] loss 9.348995\n",
      "[176,  150] loss 8.649157\n",
      "[176,  160] loss 8.536652\n",
      "[177,   10] loss 8.512911\n",
      "[177,   20] loss 7.113898\n",
      "[177,   30] loss 7.119994\n",
      "[177,   40] loss 8.721953\n",
      "[177,   50] loss 9.779392\n",
      "[177,   60] loss 7.358979\n",
      "[177,   70] loss 10.582288\n",
      "[177,   80] loss 7.935067\n",
      "[177,   90] loss 9.272204\n",
      "[177,  100] loss 10.192337\n",
      "[177,  110] loss 9.114632\n",
      "[177,  120] loss 9.267802\n",
      "[177,  130] loss 7.189850\n",
      "[177,  140] loss 9.474189\n",
      "[177,  150] loss 8.521573\n",
      "[177,  160] loss 9.969632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178,   10] loss 9.436863\n",
      "[178,   20] loss 8.266971\n",
      "[178,   30] loss 7.635916\n",
      "[178,   40] loss 8.726009\n",
      "[178,   50] loss 8.551650\n",
      "[178,   60] loss 7.901505\n",
      "[178,   70] loss 9.719926\n",
      "[178,   80] loss 8.977957\n",
      "[178,   90] loss 8.480833\n",
      "[178,  100] loss 7.354341\n",
      "[178,  110] loss 9.357048\n",
      "[178,  120] loss 8.884423\n",
      "[178,  130] loss 8.904060\n",
      "[178,  140] loss 8.305448\n",
      "[178,  150] loss 9.614554\n",
      "[178,  160] loss 7.112962\n",
      "[179,   10] loss 9.669504\n",
      "[179,   20] loss 8.622600\n",
      "[179,   30] loss 9.760108\n",
      "[179,   40] loss 7.933162\n",
      "[179,   50] loss 11.059308\n",
      "[179,   60] loss 8.607331\n",
      "[179,   70] loss 7.192022\n",
      "[179,   80] loss 9.297703\n",
      "[179,   90] loss 9.746862\n",
      "[179,  100] loss 8.912885\n",
      "[179,  110] loss 7.972771\n",
      "[179,  120] loss 7.740001\n",
      "[179,  130] loss 6.271955\n",
      "[179,  140] loss 7.975037\n",
      "[179,  150] loss 7.406565\n",
      "[179,  160] loss 8.494843\n",
      "[180,   10] loss 9.074404\n",
      "[180,   20] loss 9.885044\n",
      "[180,   30] loss 7.945162\n",
      "[180,   40] loss 8.282041\n",
      "[180,   50] loss 7.226470\n",
      "[180,   60] loss 10.012128\n",
      "[180,   70] loss 7.217355\n",
      "[180,   80] loss 8.644333\n",
      "[180,   90] loss 8.691056\n",
      "[180,  100] loss 8.318967\n",
      "[180,  110] loss 8.467236\n",
      "[180,  120] loss 8.151375\n",
      "[180,  130] loss 7.547377\n",
      "[180,  140] loss 9.370253\n",
      "[180,  150] loss 8.035420\n",
      "[180,  160] loss 8.563483\n",
      "[181,   10] loss 8.660530\n",
      "[181,   20] loss 8.357179\n",
      "[181,   30] loss 7.521090\n",
      "[181,   40] loss 9.617665\n",
      "[181,   50] loss 8.281888\n",
      "[181,   60] loss 7.906568\n",
      "[181,   70] loss 10.140734\n",
      "[181,   80] loss 7.932879\n",
      "[181,   90] loss 7.779650\n",
      "[181,  100] loss 7.822139\n",
      "[181,  110] loss 7.976752\n",
      "[181,  120] loss 10.260146\n",
      "[181,  130] loss 10.340361\n",
      "[181,  140] loss 9.859689\n",
      "[181,  150] loss 9.412250\n",
      "[181,  160] loss 8.741237\n",
      "[182,   10] loss 8.523879\n",
      "[182,   20] loss 7.587044\n",
      "[182,   30] loss 8.058569\n",
      "[182,   40] loss 8.097303\n",
      "[182,   50] loss 11.550717\n",
      "[182,   60] loss 8.030365\n",
      "[182,   70] loss 8.250295\n",
      "[182,   80] loss 9.016527\n",
      "[182,   90] loss 7.448778\n",
      "[182,  100] loss 8.673017\n",
      "[182,  110] loss 7.682272\n",
      "[182,  120] loss 6.445571\n",
      "[182,  130] loss 9.740318\n",
      "[182,  140] loss 9.493385\n",
      "[182,  150] loss 8.922944\n",
      "[182,  160] loss 9.410707\n",
      "[183,   10] loss 8.745504\n",
      "[183,   20] loss 8.548086\n",
      "[183,   30] loss 9.732438\n",
      "[183,   40] loss 7.768455\n",
      "[183,   50] loss 7.001911\n",
      "[183,   60] loss 7.484190\n",
      "[183,   70] loss 8.350498\n",
      "[183,   80] loss 10.063315\n",
      "[183,   90] loss 8.362369\n",
      "[183,  100] loss 8.535376\n",
      "[183,  110] loss 9.088443\n",
      "[183,  120] loss 9.643621\n",
      "[183,  130] loss 8.586931\n",
      "[183,  140] loss 8.051645\n",
      "[183,  150] loss 8.130783\n",
      "[183,  160] loss 8.807716\n",
      "[184,   10] loss 7.914927\n",
      "[184,   20] loss 9.455755\n",
      "[184,   30] loss 8.869462\n",
      "[184,   40] loss 9.433799\n",
      "[184,   50] loss 8.003536\n",
      "[184,   60] loss 7.725870\n",
      "[184,   70] loss 9.095046\n",
      "[184,   80] loss 8.657288\n",
      "[184,   90] loss 8.216882\n",
      "[184,  100] loss 9.205027\n",
      "[184,  110] loss 8.455354\n",
      "[184,  120] loss 8.569763\n",
      "[184,  130] loss 9.912597\n",
      "[184,  140] loss 7.880201\n",
      "[184,  150] loss 7.629176\n",
      "[184,  160] loss 6.975194\n",
      "[185,   10] loss 8.021265\n",
      "[185,   20] loss 7.721667\n",
      "[185,   30] loss 7.755455\n",
      "[185,   40] loss 10.560235\n",
      "[185,   50] loss 8.276302\n",
      "[185,   60] loss 8.284165\n",
      "[185,   70] loss 9.624116\n",
      "[185,   80] loss 9.114452\n",
      "[185,   90] loss 10.093376\n",
      "[185,  100] loss 9.719206\n",
      "[185,  110] loss 8.853083\n",
      "[185,  120] loss 8.521050\n",
      "[185,  130] loss 7.296730\n",
      "[185,  140] loss 8.043931\n",
      "[185,  150] loss 8.198098\n",
      "[185,  160] loss 8.995325\n",
      "[186,   10] loss 7.380890\n",
      "[186,   20] loss 7.566175\n",
      "[186,   30] loss 9.442487\n",
      "[186,   40] loss 8.564967\n",
      "[186,   50] loss 8.268233\n",
      "[186,   60] loss 8.160016\n",
      "[186,   70] loss 8.048375\n",
      "[186,   80] loss 8.365863\n",
      "[186,   90] loss 8.430364\n",
      "[186,  100] loss 6.182136\n",
      "[186,  110] loss 7.657760\n",
      "[186,  120] loss 9.561503\n",
      "[186,  130] loss 8.789683\n",
      "[186,  140] loss 8.093838\n",
      "[186,  150] loss 8.876692\n",
      "[186,  160] loss 9.004844\n",
      "[187,   10] loss 9.218403\n",
      "[187,   20] loss 8.111108\n",
      "[187,   30] loss 8.527870\n",
      "[187,   40] loss 7.701266\n",
      "[187,   50] loss 9.999616\n",
      "[187,   60] loss 8.182261\n",
      "[187,   70] loss 9.329376\n",
      "[187,   80] loss 8.111445\n",
      "[187,   90] loss 9.888923\n",
      "[187,  100] loss 7.186995\n",
      "[187,  110] loss 9.706675\n",
      "[187,  120] loss 7.972308\n",
      "[187,  130] loss 7.654932\n",
      "[187,  140] loss 7.937001\n",
      "[187,  150] loss 8.299759\n",
      "[187,  160] loss 8.206425\n",
      "[188,   10] loss 7.989532\n",
      "[188,   20] loss 8.271752\n",
      "[188,   30] loss 8.849809\n",
      "[188,   40] loss 10.188437\n",
      "[188,   50] loss 7.187328\n",
      "[188,   60] loss 8.009474\n",
      "[188,   70] loss 9.046612\n",
      "[188,   80] loss 8.611085\n",
      "[188,   90] loss 8.603430\n",
      "[188,  100] loss 6.484672\n",
      "[188,  110] loss 7.174204\n",
      "[188,  120] loss 8.347824\n",
      "[188,  130] loss 7.795857\n",
      "[188,  140] loss 7.032640\n",
      "[188,  150] loss 8.140415\n",
      "[188,  160] loss 8.612768\n",
      "[189,   10] loss 9.411412\n",
      "[189,   20] loss 8.742709\n",
      "[189,   30] loss 9.430032\n",
      "[189,   40] loss 7.828245\n",
      "[189,   50] loss 7.104385\n",
      "[189,   60] loss 8.870010\n",
      "[189,   70] loss 8.431858\n",
      "[189,   80] loss 8.505313\n",
      "[189,   90] loss 7.107035\n",
      "[189,  100] loss 8.934767\n",
      "[189,  110] loss 7.106835\n",
      "[189,  120] loss 9.325787\n",
      "[189,  130] loss 9.107412\n",
      "[189,  140] loss 7.740176\n",
      "[189,  150] loss 8.725816\n",
      "[189,  160] loss 7.552797\n",
      "[190,   10] loss 9.802320\n",
      "[190,   20] loss 8.116422\n",
      "[190,   30] loss 8.311144\n",
      "[190,   40] loss 9.364359\n",
      "[190,   50] loss 8.293324\n",
      "[190,   60] loss 8.599860\n",
      "[190,   70] loss 9.508484\n",
      "[190,   80] loss 8.267424\n",
      "[190,   90] loss 9.297760\n",
      "[190,  100] loss 7.811149\n",
      "[190,  110] loss 7.154475\n",
      "[190,  120] loss 7.131982\n",
      "[190,  130] loss 6.274931\n",
      "[190,  140] loss 7.366937\n",
      "[190,  150] loss 8.290702\n",
      "[190,  160] loss 8.669410\n",
      "[191,   10] loss 5.686493\n",
      "[191,   20] loss 8.934591\n",
      "[191,   30] loss 9.469975\n",
      "[191,   40] loss 8.244116\n",
      "[191,   50] loss 8.064271\n",
      "[191,   60] loss 7.208551\n",
      "[191,   70] loss 9.283205\n",
      "[191,   80] loss 8.480603\n",
      "[191,   90] loss 8.842734\n",
      "[191,  100] loss 7.576702\n",
      "[191,  110] loss 8.315333\n",
      "[191,  120] loss 9.625801\n",
      "[191,  130] loss 6.256488\n",
      "[191,  140] loss 7.515154\n",
      "[191,  150] loss 9.182154\n",
      "[191,  160] loss 10.035845\n",
      "[192,   10] loss 9.445654\n",
      "[192,   20] loss 7.913227\n",
      "[192,   30] loss 6.709890\n",
      "[192,   40] loss 9.934821\n",
      "[192,   50] loss 8.685584\n",
      "[192,   60] loss 6.746645\n",
      "[192,   70] loss 7.936649\n",
      "[192,   80] loss 8.515884\n",
      "[192,   90] loss 8.902382\n",
      "[192,  100] loss 8.749131\n",
      "[192,  110] loss 8.939281\n",
      "[192,  120] loss 8.045509\n",
      "[192,  130] loss 9.383462\n",
      "[192,  140] loss 7.705284\n",
      "[192,  150] loss 6.754558\n",
      "[192,  160] loss 9.619262\n",
      "[193,   10] loss 8.201730\n",
      "[193,   20] loss 7.280315\n",
      "[193,   30] loss 8.410631\n",
      "[193,   40] loss 7.714687\n",
      "[193,   50] loss 7.934983\n",
      "[193,   60] loss 7.319494\n",
      "[193,   70] loss 8.558269\n",
      "[193,   80] loss 7.782811\n",
      "[193,   90] loss 7.717961\n",
      "[193,  100] loss 7.784986\n",
      "[193,  110] loss 7.782087\n",
      "[193,  120] loss 10.326423\n",
      "[193,  130] loss 9.089440\n",
      "[193,  140] loss 7.370945\n",
      "[193,  150] loss 9.199872\n",
      "[193,  160] loss 8.055016\n",
      "[194,   10] loss 8.203783\n",
      "[194,   20] loss 8.459056\n",
      "[194,   30] loss 7.897152\n",
      "[194,   40] loss 7.628962\n",
      "[194,   50] loss 8.280476\n",
      "[194,   60] loss 8.251827\n",
      "[194,   70] loss 9.236974\n",
      "[194,   80] loss 7.928188\n",
      "[194,   90] loss 10.439004\n",
      "[194,  100] loss 6.783701\n",
      "[194,  110] loss 8.519802\n",
      "[194,  120] loss 7.560582\n",
      "[194,  130] loss 8.912615\n",
      "[194,  140] loss 8.203587\n",
      "[194,  150] loss 9.017148\n",
      "[194,  160] loss 6.408033\n",
      "[195,   10] loss 6.678849\n",
      "[195,   20] loss 8.151711\n",
      "[195,   30] loss 9.495036\n",
      "[195,   40] loss 8.007617\n",
      "[195,   50] loss 7.761667\n",
      "[195,   60] loss 8.088893\n",
      "[195,   70] loss 9.216014\n",
      "[195,   80] loss 9.015917\n",
      "[195,   90] loss 7.455297\n",
      "[195,  100] loss 8.165452\n",
      "[195,  110] loss 7.959959\n",
      "[195,  120] loss 9.547124\n",
      "[195,  130] loss 8.213590\n",
      "[195,  140] loss 7.292543\n",
      "[195,  150] loss 7.439064\n",
      "[195,  160] loss 8.281621\n",
      "[196,   10] loss 7.117702\n",
      "[196,   20] loss 9.271004\n",
      "[196,   30] loss 7.377129\n",
      "[196,   40] loss 8.508900\n",
      "[196,   50] loss 8.472093\n",
      "[196,   60] loss 8.366860\n",
      "[196,   70] loss 7.721271\n",
      "[196,   80] loss 7.602594\n",
      "[196,   90] loss 8.189057\n",
      "[196,  100] loss 6.846790\n",
      "[196,  110] loss 8.519772\n",
      "[196,  120] loss 7.857343\n",
      "[196,  130] loss 9.427032\n",
      "[196,  140] loss 9.120524\n",
      "[196,  150] loss 10.276771\n",
      "[196,  160] loss 7.118589\n",
      "[197,   10] loss 8.304455\n",
      "[197,   20] loss 8.695042\n",
      "[197,   30] loss 7.704331\n",
      "[197,   40] loss 8.085754\n",
      "[197,   50] loss 9.534338\n",
      "[197,   60] loss 7.929605\n",
      "[197,   70] loss 7.481915\n",
      "[197,   80] loss 8.553219\n",
      "[197,   90] loss 7.420818\n",
      "[197,  100] loss 7.782243\n",
      "[197,  110] loss 9.448136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197,  120] loss 6.582457\n",
      "[197,  130] loss 7.855511\n",
      "[197,  140] loss 8.905802\n",
      "[197,  150] loss 8.791404\n",
      "[197,  160] loss 10.044801\n",
      "[198,   10] loss 8.313948\n",
      "[198,   20] loss 9.601603\n",
      "[198,   30] loss 7.645288\n",
      "[198,   40] loss 9.277590\n",
      "[198,   50] loss 8.305311\n",
      "[198,   60] loss 7.034502\n",
      "[198,   70] loss 6.582784\n",
      "[198,   80] loss 7.428387\n",
      "[198,   90] loss 9.714935\n",
      "[198,  100] loss 8.899665\n",
      "[198,  110] loss 8.294057\n",
      "[198,  120] loss 7.265180\n",
      "[198,  130] loss 8.886294\n",
      "[198,  140] loss 7.673786\n",
      "[198,  150] loss 7.509605\n",
      "[198,  160] loss 9.794825\n",
      "[199,   10] loss 8.029254\n",
      "[199,   20] loss 8.401737\n",
      "[199,   30] loss 8.079035\n",
      "[199,   40] loss 9.560243\n",
      "[199,   50] loss 8.840394\n",
      "[199,   60] loss 7.428243\n",
      "[199,   70] loss 6.892544\n",
      "[199,   80] loss 7.415276\n",
      "[199,   90] loss 8.904389\n",
      "[199,  100] loss 9.062617\n",
      "[199,  110] loss 6.973261\n",
      "[199,  120] loss 6.220614\n",
      "[199,  130] loss 9.227940\n",
      "[199,  140] loss 8.192176\n",
      "[199,  150] loss 7.423880\n",
      "[199,  160] loss 8.726039\n",
      "[200,   10] loss 8.860398\n",
      "[200,   20] loss 8.001382\n",
      "[200,   30] loss 7.810631\n",
      "[200,   40] loss 9.042875\n",
      "[200,   50] loss 7.686338\n",
      "[200,   60] loss 7.610435\n",
      "[200,   70] loss 8.180103\n",
      "[200,   80] loss 7.118132\n",
      "[200,   90] loss 7.366954\n",
      "[200,  100] loss 9.260476\n",
      "[200,  110] loss 8.240696\n",
      "[200,  120] loss 7.179404\n",
      "[200,  130] loss 7.286635\n",
      "[200,  140] loss 8.425800\n",
      "[200,  150] loss 9.657433\n",
      "[200,  160] loss 6.928290\n",
      "finish training\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#  lr=1e-5\n",
    "##################\n",
    "epochnum=0\n",
    "iteration=0\n",
    "criterion=nn.L1Loss()\n",
    "Net=model.double()\n",
    "for epoch in np.arange(200):\n",
    "    epochnum+=1\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(dloador,0):\n",
    "#         ipdb.set_trace()\n",
    "        iteration+=1\n",
    "        sample,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        sample.to(device)\n",
    "        labels.to(device)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "\n",
    "#         print(labels)\n",
    "        optimizer.zero_grad()\n",
    "#         sample=sample.double()\n",
    "        outputs=Net(sample.double())\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        if i%10==9:\n",
    "            print('[%d,%5d] loss %f' %(epoch+1,i+1,running_loss/10))\n",
    "            running_loss=0.0\n",
    "print('finish training')\n",
    "t.save(Net.state_dict(),'/home/wcj/ReferenceProject/PPGnet/checkpoints/0430Net_epoch%d-iteration%d.pth'%(epochnum,iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    1] loss 18.075403\n",
      "[1,    2] loss 19.515793\n",
      "[1,    3] loss 17.558434\n",
      "[1,    4] loss 16.412480\n",
      "[1,    5] loss 18.079704\n",
      "[1,    6] loss 19.425593\n",
      "[1,    7] loss 20.072812\n",
      "[1,    8] loss 18.760013\n",
      "[1,    9] loss 18.147868\n",
      "[1,   10] loss 18.618706\n",
      "[1,   11] loss 17.294487\n",
      "[1,   12] loss 19.384800\n",
      "[1,   13] loss 17.303515\n",
      "[1,   14] loss 18.040458\n",
      "[1,   15] loss 18.887307\n",
      "[1,   16] loss 16.524797\n",
      "[1,   17] loss 13.260293\n",
      "[2,    1] loss 17.287551\n",
      "[2,    2] loss 17.788845\n",
      "[2,    3] loss 18.117845\n",
      "[2,    4] loss 20.128804\n",
      "[2,    5] loss 17.015121\n",
      "[2,    6] loss 17.332191\n",
      "[2,    7] loss 17.894420\n",
      "[2,    8] loss 14.809302\n",
      "[2,    9] loss 18.031544\n",
      "[2,   10] loss 18.067026\n",
      "[2,   11] loss 17.439267\n",
      "[2,   12] loss 19.546238\n",
      "[2,   13] loss 17.064752\n",
      "[2,   14] loss 18.226314\n",
      "[2,   15] loss 18.054190\n",
      "[2,   16] loss 18.959108\n",
      "[2,   17] loss 11.341774\n",
      "[3,    1] loss 19.969725\n",
      "[3,    2] loss 17.759180\n",
      "[3,    3] loss 17.005852\n",
      "[3,    4] loss 18.858227\n",
      "[3,    5] loss 18.909532\n",
      "[3,    6] loss 16.703894\n",
      "[3,    7] loss 19.656682\n",
      "[3,    8] loss 16.005751\n",
      "[3,    9] loss 19.664005\n",
      "[3,   10] loss 18.489792\n",
      "[3,   11] loss 14.054113\n",
      "[3,   12] loss 18.017741\n",
      "[3,   13] loss 17.257299\n",
      "[3,   14] loss 19.010705\n",
      "[3,   15] loss 18.475257\n",
      "[3,   16] loss 19.111609\n",
      "[3,   17] loss 7.961122\n",
      "[4,    1] loss 19.850323\n",
      "[4,    2] loss 18.594574\n",
      "[4,    3] loss 17.947023\n",
      "[4,    4] loss 18.967082\n",
      "[4,    5] loss 17.140776\n",
      "[4,    6] loss 15.742223\n",
      "[4,    7] loss 16.789354\n",
      "[4,    8] loss 18.591699\n",
      "[4,    9] loss 18.137166\n",
      "[4,   10] loss 17.441244\n",
      "[4,   11] loss 19.129158\n",
      "[4,   12] loss 15.928334\n",
      "[4,   13] loss 18.739361\n",
      "[4,   14] loss 16.691649\n",
      "[4,   15] loss 20.593309\n",
      "[4,   16] loss 16.507891\n",
      "[4,   17] loss 11.646076\n",
      "[5,    1] loss 18.214019\n",
      "[5,    2] loss 15.675824\n",
      "[5,    3] loss 16.895130\n",
      "[5,    4] loss 16.187352\n",
      "[5,    5] loss 17.790089\n",
      "[5,    6] loss 16.882596\n",
      "[5,    7] loss 19.581497\n",
      "[5,    8] loss 18.690170\n",
      "[5,    9] loss 19.042037\n",
      "[5,   10] loss 20.157389\n",
      "[5,   11] loss 19.560005\n",
      "[5,   12] loss 15.974621\n",
      "[5,   13] loss 16.098847\n",
      "[5,   14] loss 19.861083\n",
      "[5,   15] loss 18.401515\n",
      "[5,   16] loss 19.186207\n",
      "[5,   17] loss 12.114905\n",
      "[6,    1] loss 19.558829\n",
      "[6,    2] loss 16.762099\n",
      "[6,    3] loss 17.727752\n",
      "[6,    4] loss 18.361458\n",
      "[6,    5] loss 17.878764\n",
      "[6,    6] loss 18.406030\n",
      "[6,    7] loss 18.964011\n",
      "[6,    8] loss 16.928183\n",
      "[6,    9] loss 16.114887\n",
      "[6,   10] loss 16.324096\n",
      "[6,   11] loss 18.685180\n",
      "[6,   12] loss 17.048184\n",
      "[6,   13] loss 19.548936\n",
      "[6,   14] loss 16.856599\n",
      "[6,   15] loss 17.920989\n",
      "[6,   16] loss 15.241090\n",
      "[6,   17] loss 14.749790\n",
      "[7,    1] loss 20.547601\n",
      "[7,    2] loss 17.225033\n",
      "[7,    3] loss 16.459902\n",
      "[7,    4] loss 19.042707\n",
      "[7,    5] loss 17.825162\n",
      "[7,    6] loss 18.444107\n",
      "[7,    7] loss 17.536707\n",
      "[7,    8] loss 16.202264\n",
      "[7,    9] loss 17.203038\n",
      "[7,   10] loss 16.366634\n",
      "[7,   11] loss 15.982360\n",
      "[7,   12] loss 21.425027\n",
      "[7,   13] loss 18.334926\n",
      "[7,   14] loss 16.273651\n",
      "[7,   15] loss 16.747138\n",
      "[7,   16] loss 21.482990\n",
      "[7,   17] loss 10.268861\n",
      "[8,    1] loss 17.454501\n",
      "[8,    2] loss 17.793060\n",
      "[8,    3] loss 15.469389\n",
      "[8,    4] loss 18.567115\n",
      "[8,    5] loss 17.416130\n",
      "[8,    6] loss 19.656373\n",
      "[8,    7] loss 17.249455\n",
      "[8,    8] loss 21.200582\n",
      "[8,    9] loss 19.239284\n",
      "[8,   10] loss 18.831526\n",
      "[8,   11] loss 17.320339\n",
      "[8,   12] loss 16.806309\n",
      "[8,   13] loss 17.767452\n",
      "[8,   14] loss 18.113114\n",
      "[8,   15] loss 17.052696\n",
      "[8,   16] loss 15.823522\n",
      "[8,   17] loss 7.899303\n",
      "[9,    1] loss 16.507910\n",
      "[9,    2] loss 18.362872\n",
      "[9,    3] loss 17.861466\n",
      "[9,    4] loss 18.501416\n",
      "[9,    5] loss 18.941149\n",
      "[9,    6] loss 17.826611\n",
      "[9,    7] loss 18.202107\n",
      "[9,    8] loss 17.578312\n",
      "[9,    9] loss 19.046707\n",
      "[9,   10] loss 18.205307\n",
      "[9,   11] loss 20.398711\n",
      "[9,   12] loss 15.151612\n",
      "[9,   13] loss 17.218385\n",
      "[9,   14] loss 16.804776\n",
      "[9,   15] loss 18.018447\n",
      "[9,   16] loss 17.341506\n",
      "[9,   17] loss 9.024308\n",
      "[10,    1] loss 19.532458\n",
      "[10,    2] loss 18.699301\n",
      "[10,    3] loss 15.867797\n",
      "[10,    4] loss 19.156369\n",
      "[10,    5] loss 18.116272\n",
      "[10,    6] loss 14.681506\n",
      "[10,    7] loss 17.847551\n",
      "[10,    8] loss 18.228150\n",
      "[10,    9] loss 16.990759\n",
      "[10,   10] loss 17.826301\n",
      "[10,   11] loss 17.789443\n",
      "[10,   12] loss 20.058015\n",
      "[10,   13] loss 19.920893\n",
      "[10,   14] loss 17.324444\n",
      "[10,   15] loss 17.824602\n",
      "[10,   16] loss 17.760554\n",
      "[10,   17] loss 8.085126\n",
      "[11,    1] loss 15.379424\n",
      "[11,    2] loss 18.732728\n",
      "[11,    3] loss 15.612608\n",
      "[11,    4] loss 19.273528\n",
      "[11,    5] loss 17.852608\n",
      "[11,    6] loss 19.085608\n",
      "[11,    7] loss 18.491964\n",
      "[11,    8] loss 16.100793\n",
      "[11,    9] loss 17.635150\n",
      "[11,   10] loss 17.199706\n",
      "[11,   11] loss 16.927396\n",
      "[11,   12] loss 16.356653\n",
      "[11,   13] loss 18.669291\n",
      "[11,   14] loss 19.398410\n",
      "[11,   15] loss 20.707546\n",
      "[11,   16] loss 18.140750\n",
      "[11,   17] loss 12.317174\n",
      "[12,    1] loss 16.501880\n",
      "[12,    2] loss 19.856626\n",
      "[12,    3] loss 16.612239\n",
      "[12,    4] loss 17.917388\n",
      "[12,    5] loss 17.370443\n",
      "[12,    6] loss 17.263319\n",
      "[12,    7] loss 18.204431\n",
      "[12,    8] loss 19.805616\n",
      "[12,    9] loss 17.622800\n",
      "[12,   10] loss 16.641071\n",
      "[12,   11] loss 18.848227\n",
      "[12,   12] loss 18.841962\n",
      "[12,   13] loss 15.627415\n",
      "[12,   14] loss 18.882840\n",
      "[12,   15] loss 15.184860\n",
      "[12,   16] loss 17.677684\n",
      "[12,   17] loss 12.191688\n",
      "[13,    1] loss 17.944517\n",
      "[13,    2] loss 21.157452\n",
      "[13,    3] loss 17.631889\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-714551855385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#         sample=sample.double()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-235e0ce2fd3f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m         hidden=(torch.zeros([2, concat.size(0), self.hidden_dim],dtype=torch.double),\n\u001b[1;32m    119\u001b[0m                 torch.zeros([2, concat.size(0), self.hidden_dim],dtype=torch.double))\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mLstmOutp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;31m#         ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLstmOutp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLstmOutp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# 再训练20epoch 加大batcsize\n",
    "#####################\n",
    "optimizer=optim.Adam(model.parameters(),lr=1e-7)\n",
    "Net=model.double()\n",
    "Net.load_state_dict(t.load('/home/wcj/ReferenceProject/PPGnet/checkpoints/0430Net_epoch200-iteration32600.pth'))\n",
    "batch_size=100\n",
    "epochnum=100\n",
    "iteration=32600\n",
    "dloador=DataLoader(dataloader,batch_size=batch_size,shuffle=True,num_workers=8,pin_memory=CUDA)\n",
    "criterion=nn.L1Loss()\n",
    "Net=model.double()\n",
    "for epoch in np.arange(20):\n",
    "    epochnum+=1\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(dloador,0):\n",
    "#         ipdb.set_trace()\n",
    "        iteration+=1\n",
    "        sample,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        sample.to(device)\n",
    "        labels.to(device)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "\n",
    "#         print(labels)\n",
    "        optimizer.zero_grad()\n",
    "#         sample=sample.double()\n",
    "        outputs=Net(sample.double())\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "#         if i%10==9:\n",
    "        print('[%d,%5d] loss %f' %(epoch+1,i+1,running_loss))\n",
    "        running_loss=0.0\n",
    "print('finish training')\n",
    "t.save(Net.state_dict(),'/home/wcj/ReferenceProject/PPGnet/checkpoints/0430Net_epoch%d-iteration%d_batchsize%d.pth'%(epochnum,iteration,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1622个样本计算MAE: 25.352355\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 模型实时测试 在12组数据(训练)上\n",
    "###############################\n",
    "testfilepath='/home/wcj/ReferenceProject/PPGnet/ppghr'\n",
    "Testdataloader=PPGData(testfilepath)\n",
    "testloader=DataLoader(\n",
    "Testdataloader,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# for i in np.arange(7):\n",
    "sub=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        samples,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "        outputs=model(samples)\n",
    "    #         _,predicted=t.max(outputs,1)# 在矩阵的第一个维度上\n",
    "        total+=labels.size(0)\n",
    "#         ipdb.set_trace()\n",
    "        sub+=abs(outputs-labels).sum()\n",
    "    #         print('++++')\n",
    "    print(' %d个样本计算MAE: %.6f'%(total,sub/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1328个样本计算MAE: 28.195536\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 模型实时测试 在10组数据上\n",
    "###############################\n",
    "\n",
    "testfilepath='/home/wcj/ReferenceProject/PPGnet/testdata'\n",
    "Testdataloader=PPGData(testfilepath)\n",
    "testloader=DataLoader(\n",
    "Testdataloader,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# for i in np.arange(7):\n",
    "sub=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        samples,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "        outputs=model(samples)\n",
    "    #         _,predicted=t.max(outputs,1)# 在矩阵的第一个维度上\n",
    "        total+=labels.size(0)\n",
    "#         ipdb.set_trace()\n",
    "        sub+=abs(outputs-labels).sum()\n",
    "    #         print('++++')\n",
    "    print(' %d个样本计算MAE: %.6f'%(total,sub/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 146个样本计算MAE: 24.093812\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# leave one\n",
    "########################\n",
    "\n",
    "Net=PPGNet(kernels,125,125,125).eval()\n",
    "Net.double()\n",
    "Net.load_state_dict(t.load('/home/wcj/ReferenceProject/PPGnet/checkpoints/0430Net_epoch200-iteration32600.pth'))\n",
    "\n",
    "testfilepath='/home/wcj/ReferenceProject/PPGnet/12thData'\n",
    "Testdataloader=PPGData(testfilepath)\n",
    "testloader=DataLoader(\n",
    "Testdataloader,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# for i in np.arange(7):\n",
    "sub=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        samples,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "        outputs=Net(samples)\n",
    "    #         _,predicted=t.max(outputs,1)# 在矩阵的第一个维度上\n",
    "        total+=labels.size(0)\n",
    "#         ipdb.set_trace()\n",
    "        sub+=abs(outputs-labels).sum()\n",
    "    #         print('++++')\n",
    "    print(' %d个样本计算MAE: %.6f'%(total,sub/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1328个样本计算MAE: 38.290428\n"
     ]
    }
   ],
   "source": [
    "Net=PPGNet(kernels,125,125,125).eval()\n",
    "Net.double()\n",
    "Net.load_state_dict(t.load('/home/wcj/ReferenceProject/PPGnet/checkpoints/0429Net_epoch20_lr1e-05_weight_decay.pth'))\n",
    "\n",
    "testfilepath='/home/wcj/ReferenceProject/PPGnet/testdata'\n",
    "Testdataloader=PPGData(testfilepath)\n",
    "testloader=DataLoader(\n",
    "Testdataloader,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# for i in np.arange(7):\n",
    "sub=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        samples,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "        outputs=Net(samples)\n",
    "    #         _,predicted=t.max(outputs,1)# 在矩阵的第一个维度上\n",
    "        total+=labels.size(0)\n",
    "#         ipdb.set_trace()\n",
    "        sub+=abs(outputs-labels).sum()\n",
    "    #         print('++++')\n",
    "    print(' %d个样本计算MAE: %.6f'%(total,sub/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1622个样本计算MAE: 17.072523\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 模型测试 在12组数据(训练)上\n",
    "###############################\n",
    "Net=PPGNet(kernels,125,125,125).eval()\n",
    "Net.double()\n",
    "Net.load_state_dict(t.load('/home/wcj/ReferenceProject/PPGnet/checkpoints/0429Net_epoch100_lr1e-05_weight_decay_1.pth'))\n",
    "testfilepath='/home/wcj/ReferenceProject/PPGnet/ppghr'\n",
    "Testdataloader=PPGData(testfilepath)\n",
    "testloader=DataLoader(\n",
    "Testdataloader,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# for i in np.arange(7):\n",
    "sub=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        samples,labels=data\n",
    "        labels=np.asarray(labels,dtype=float)\n",
    "        labels=t.from_numpy(labels)\n",
    "        labels=labels.view(labels.size(0),-1)\n",
    "        outputs=Net(samples)\n",
    "    #         _,predicted=t.max(outputs,1)# 在矩阵的第一个维度上\n",
    "        total+=labels.size(0)\n",
    "#         ipdb.set_trace()\n",
    "        sub+=abs(outputs-labels).sum()\n",
    "    #         print('++++')\n",
    "    print(' %d个样本计算MAE: %.6f'%(total,sub/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
